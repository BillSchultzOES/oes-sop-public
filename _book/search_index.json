[
["index.html", "OES Standard Operating Procedures for The Design and Statistical Analysis of Experiments. Overview Purposes of this document Nature and limitations of this document Help us improve our work! About this document", " OES Standard Operating Procedures for The Design and Statistical Analysis of Experiments. Jake Bowers, Ryan T. Moore, Lula Chen, Paul Testa, Nate Higgins, Oliver McClellan, Miles Williams, Tyler Simko, Bill Schultz August 23, 2023 Overview This document explains how our team, the Office of Evaluation Sciences in the General Services Administration (the OES), tends to do statistical analysis and it also explains why we do what we do.1 The research integrity processes OES follows is already documented on our Evaluation Resources Web Page. For example, on that page we provide templates for our research design and analysis pre-registration process. Here, we get into the nitty gritty of our statistical work. Purposes of this document First, this document educates new team members about the decisions past team members have made regarding the design and analysis of the studies fielded so far. It also serves as a place to record decisions for our own future selves, and helps us harness the power that arises from our disciplinary diversity. That is, we have decided, as a team, how to do certain statistical analyses, and these decisions may differ from those that are common in any given academic discipline. This document, thus, helps explain why we have landed on those decisions (for now), and how to implement those practices. Second, this document records decisions that we have made in the absence of pre-analysis plans, or in the context of circumstances unforeseen by our pre-analysis planning. It should help guide our future experimental design and analysis. Third, this document should help us write better analysis plans and speed our practice of re-analysis. (Our team insists on a blind re-analysis of every study as a quality control for our results before they are reported to our agency partners.) Fourth, this document should help other teams working to learn about the causal impacts of policy interventions. We hope this contributes to the federal government&#x2019;s own work pursuing evidence-based public policy, but also helps other teams in other places doing work that is similar to our own. Nature and limitations of this document We (mostly) focus on randomized field experiments. This document focuses on design and analysis of randomized field experiments. Although we include some discussion about non-randomized studies, often known as observational studies, until now, our team has focused primarily on randomized field experiments. We plan to include more discussion of observational studies over time. We present examples using R As public servants and social and behavioral scientists, we use the R statistical analysis language for this document because it is (a) one of the two industry standards in the field of data science (along with Python), (b) free, open source, and multiplatform, and (c) the standard for advanced methodological work in the statistical sciences as applied to the social and behavioral sciences (the latest new statistical techniques for social and behavioral scientists tend to be developed in R). Many members of our team use Stata or SAS or SPSS or Python. We welcome additions to this document using those languages as well. Structure of the document Each section of this document will include, if applicable: A description of our approach. A description of how we implement our approach, including functions in R, key arguments that must be entered into the function, and key values that are outputs from the function. A general example using simulated data (perhaps including some evaluation of the tool as compared to other possible choices). A discussion of a specific example from OES (if applicable) in which we implemented the given procedure. Throughout the document, we include links to the Glossary and Appendix, to clarify terms or explain tools and procedures in more depth. Help us improve our work! Since we hope to improve our analytic workflow with every project, this document should be seen as provisional &#x2014; as a record and a guide for our continuous learning and improvement. We invite comments in the Issues and pull requests for direct contributions. About this document This book was written in bookdown. The complete source is available from GitHub. This version of the book was built with R version 4.2.1 (2022-06-23 ucrt) and the following packages. package version source bfe 2.0 Github (gibbonscharlie/bfe@4eaebc00d12bc427a9c75aec3280c43a0034b416) blockTools 0.6.4 CRAN (R 4.2.3) bookdown 0.7 CRAN (R 4.2.1) coin 1.4-2 CRAN (R 4.2.1) DeclareDesign 1.0.0 CRAN (R 4.2.1) devtools 2.4.4 CRAN (R 4.2.1) estimatr 1.0.0 CRAN (R 4.2.1) future 1.28.0 CRAN (R 4.2.1) future.apply 1.9.1 CRAN (R 4.2.1) here 1.0.1 CRAN (R 4.2.2) ICC 2.4.0 CRAN (R 4.2.0) kableExtra 1.3.4 CRAN (R 4.2.1) katex 1.4.1 https://ropensci.r-universe.dev (R 4.2.3) lmtest 0.9-40 CRAN (R 4.2.1) multcomp 1.4-20 CRAN (R 4.2.1) nbpMatching 1.5.1 CRAN (R 4.2.3) quickblock 0.2.0 CRAN (R 4.2.1) randomizr 0.22.0 CRAN (R 4.2.1) remotes 2.4.2 CRAN (R 4.2.1) ri2 0.4.0 CRAN (R 4.2.2) RItools 0.3-3 CRAN (R 4.2.3) sandwich 3.0-2 CRAN (R 4.2.2) tidyverse 1.3.2 CRAN (R 4.2.2) V8 4.3.3 CRAN (R 4.2.3) withr 2.5.0 CRAN (R 4.2.1) "],
["statistical-and-causal-inference-for-policy-change.html", "Chapter 1 Statistical and causal inference for policy change", " Chapter 1 Statistical and causal inference for policy change Most of this this document dives into the details of our statistical decision making and assumes that the reader has heard of a hypothesis test and a statistical estimator. However, here we explain in very broad terms how tests and estimators help us do our job in helping the US federal government improve public policy. Recall that &#x201C;evidence-based public policy&#x201D; can refer to both &#x201C;evidence-as-insight&#x201D; (the use of previous scientific literature as input to the design of new policies) and &#x201C;evidence-as-evaluation&#x201D; (the careful design of studies to learn about how and whether a new policy worked) (Bowers and Testa 2019). Our team aims to help government agencies design new policies and also to learn about how those new ideas work. This document focuses on the learning part of our work. How would we know whether and how a new policy worked? In an ideal and unrealistic case, we would know that a new policy improved the life of a single person, Jake, if we could compare Jake&#x2019;s decisions both under the new policy and under the status quo at the same moment in time. If we saw that Jake&#x2019;s decisions were better under the new policy than under the status quo, we would say that the new policy caused Jake to make better decisions. Since no one can observe Jake in both situations &#x2014; say, making health decisions both with and without a new procedure for visiting the doctor &#x2014; researchers try to find at least one other person (if not more) who represents how Jake would have acted if he had not been exposed to the new policy. Holland (1986) calls this the &#x201C;fundamental problem of causal inference&#x201D; and explains more formally when we might believe that other people are a good approximations for how Jake would have acted without the new policy. For example, if access to the new policy is randomized, we can claim that that the two groups are good counterfactuals for each other. That is, our team tends to think about the causal effects of a policy in counterfactual terms. What do statistics have to do with learning about the causal effect of a new policy idea? We use randomized experiments to create groups of people who represent behavior under both the new policy and the status quo. In medical experiments to assess the effectiveness of new treatments, these two groups tend to be called the &#x201C;treatment group&#x201D; and the &#x201C;control group.&#x201D; Social scientists often use that same language even if we are not really providing a new treatment, but are, instead, offering a new communication or structure for a decision. If we pilot the new policy with people chosen at random, we can claim that the people chosen and the people not chosen represent each other. In a randomized study, we can use what we see from one group to learn about what would have happened had the other group instead received the treatment or new policy intervention. Now, in any given sample we won&#x2019;t know exactly how the treatment group would have behaved if they had been assigned to the control group instead. For example, if we pulled 500 out of 1000 names from a hat and assigned those 500 people to receive treatment, that&#x2019;s just one of many possible sets of 500 people we could have drawn. If we were to do the experiment again, and pulled another 500 names at random, this second experiment will also have a randomly selected treatment group, but the second 500 people will be different from the first 500 people. A single experiment offers us some information about the effect of treatment, but we need to ask other questions too, like, &#x201C;How much could our result differ just due to pulling a different 500 people from the hat?&#x201D; We also need to answer questions like, &#x201C;What do you mean by &#x2018;result&#x2019;? How do I know that this really is a good result rather than a bad result?&#x201D; Our team uses statistical theory to produce estimates of the causal effect of a new policy. We also use statistical theory to answer questions like &#x201C;Could the effect really have been zero?&#x201D; or &#x201C;How many people do we need to observe in order to distinguish a positive effect from a zero effect?&#x201D; The rest of this document presents decisions we have made about the particulars of estimators and tests, as well as other tricky decisions that we have had to confront &#x2014; like what to do when some of data are missing. For more on the basics of how statistics helps us answer questions about causal effects, we recommend chapters 1&#x2013;3 of Gerber and Green (2012) (which focuses on randomized experiments) and the first part of Rosenbaum (2017) (which focuses on both experiments and research designs without randomization). Another good treatment comes from the opening chapters of Angrist and Pischke (2009). References "],
["basics-of-experimental-design-and-analysis.html", "Chapter 2 Basics of Experimental Design and Analysis 2.1 Statistical Power: Designing Studies that effectively distinguish signal from noise 2.2 Error Rates of Tests 2.3 Bias in Estimators", " Chapter 2 Basics of Experimental Design and Analysis Here we very briefly define and describe some of the general characteristics of statistical procedures that guide our decision making in the rest of this guide. Briefly, we want to create research designs that have enough statistical power to tell us something meaningful about the new policy interventions that we are piloting, and we want to use statistical tests that will rarely mislead us &#x2014; will rarely give a false positive result, and we want to use estimators without systematic error. These operating characteristics of our procedures depend on both the design of the study and the choices of computational procedures that we use. So, we descibe them more in-depth in the Power Analysis section that comes after both our sections on randomization and the design of experiments and the section on analysis choices. 2.1 Statistical Power: Designing Studies that effectively distinguish signal from noise The research designs we use in the OES aim to enhance our ability to distinguish signal from noise: studies with very few observations cannot tell us much about the treatment effect, while studies with very many observations provide a lot of information about the treatment effect. A study which effectively distinguishes signal from noise has excellent &#x201C;statistical power&#x201D; and a study which cannot do this has low statistical power. The Evidence in Governance and Politics (EGAP) Methods Guide 10 Things You Need to Know about Statistical Power describes more about what statistical power is and how to assess it. Before we field a research design, we assess its statistical power. If we anticipate that the intervention will only make a small change in peoples&#x2019; behavior, then we will need a relatively large number of people in the study: too few people will result in a report saying something like, &#x201C;The new policy might have improved the lives of the people in the study, but we can&#x2019;t argue strongly that this is so because the study was too small.&#x201D; 2.2 Error Rates of Tests A good statistical test rarely rejects a true hypothesis and often rejects false hypotheses. The EGAP Methods Guide 10 Things to Know about Hypothesis Testing describes the basics of hypothesis tests and explains more about how one might know that a given ppp-value arises from a test with good properties in a given research design. Our team tries to follow these practices of choosing testing procedures that are not likely to mislead analysts, when we make our analysis plans and complete our analyses and re-analyses. 2.3 Bias in Estimators A good estimator is not systematically different from the truth, and an even better estimator tends to produce estimates that are close to the truth across different experiments. Because the difference of means between treatment and control groups is well known as an unbiased estimator of the average treatment effect within a given experimental pool, this is a primary quantity of interest to report by our team. Similarly, since we know that the coefficient in a logistic regression of a binary outcome on a treatment indicator and a covariate is a biased estimator of the underlying causal difference in log-odds we use other approaches when we want to talk about the causal effect of a treatment on log-odds (Freedman 2008b). References "],
["design-based-principles-of-statistical-inference.html", "Chapter 3 Design-Based Principles of Statistical Inference 3.1 An example using simulated data 3.2 Summary: What does a design based approach mean for policy evaluation?", " Chapter 3 Design-Based Principles of Statistical Inference Most policy evaluations using administrative data or surveys report the results of their studies using estimators and tests. Although we can never know the true causal effect of a new policy on our beneficiaries, we can provide a best guess (&#x201C;The average amount saved for retirement by people in the treatment group was $1000 more than the average amount in the control group: our estimate of the average treatment effect is $1000.&#x201D;) and we can provide a test of a hunch or hypothesis (&#x201C;We can reject the null hypothesis at the 5% significance level with p=.02p=.02p=.02.&#x201D;). Confidence intervals, by the way, summarize hypothesis tests, so we think of them as tests rather than estimators. Now, when we are asked why we used this or that method for calculating an average treatment effect or a ppp-value or a confidence interval, our team has tended to say that our statistical analyses depend on the design of our studies. When applied to randomized experiments, this principle can be written simply as: analyze as you randomize. We provide an example of this principle in practice below. This idea, often known as &#x201C;randomization based&#x201D; or &#x201C;design based&#x201D; inference, was proposed by two of the founders of modern statistics. Jerzy Neyman&#x2019;s 1923 paper showed how to use randomization to learn about what we would currently call &#x201C;average treatment effects&#x201D; (Neyman (1923)) and Ronald A. Fisher&#x2019;s 1935 book showed how to use randomization to test hypotheses about what we would currently call &#x201C;treatment effects&#x201D; (Fisher (1935)). We mention this commitment here because it guides our choices of statistical tools in general. We use a design based approach because we often know how a study was designed &#x2014; after all, we and our agency collaborators tend to be the ones deciding on the sample size, the experimental arms, and the outcome data to be extracted from administrative databases. There are other ways to justify statistical procedures, and we do not exclude any reasonable approach in our work &#x2014; such as approaches based on theoretical probability models. However, referring to what we know we did in a given study has served us well so far, and it thus forms the basis of our decisions in general. 3.1 An example using simulated data Imagine we have a simple randomized experiment where the relationship between outcomes and treatment is shown in Figure ??. Notice that, in this simulated experiment, the treatment changes the variability in the outcome in the treated group &#x2014; this is a common pattern when the control group is status quo. ## Read in data for the fake experiment dat1 &lt;- read.csv(&quot;dat1.csv&quot;) ## y0 and y1 are the true underlying potential outcomes. with(dat1,{boxplot(list(y0,y1),names=c(&quot;Control&quot;,&quot;Treatment&quot;),ylab=&quot;Outcomes&quot;) stripchart(list(y0,y1),add=TRUE,vertical=TRUE) stripchart(list(mean(y0),mean(y1)),add=TRUE,vertical=TRUE,pch=19,cex=2)}) In this simulated data, we know the true average treatment effect (ATE) because we know both of the underlying true potential outcomes (written in code as y0 for yi,Zi=0y_{i,Z_i=0}yi,Zi&#x200B;=0&#x200B; or &#x201C;the response person iii would provide if he/she were in the status quo or control group&#x201D; and y1 for yi,Zi=1y_{i,Z_i = 1}yi,Zi&#x200B;=1&#x200B; for &#x201C;the response person iii would provide if he/she were in the new policy or treatment group&#x201D;. We use ZiZ_iZi&#x200B; to refer to the experimental arm. In this case Zi=0Z_i=0Zi&#x200B;=0 for people in the status quo and Zi=1Z_i=1Zi&#x200B;=1 for people in the new policy. (You can click to SHOW the code.) trueATE &lt;- with(dat1, mean(y1) - mean(y0)) trueATE [1] 5.453 Now, we have one experiment (defined by randomly assigning half of the people to treatment and half to control). We know that the observed difference of means of the outcome, YYY, between treated and control groups is an unbiased estimator of the true ATE. And we can calculate this number in a few ways: Notice that we can just calculate the difference of means or we can use the fact that the ordinary least squares linear model also calculates the same number if we have a binary treatment on the right hand side. ## Y is the observed outcome. estATE1 &lt;- with(dat1, mean(Y[Z==1]) - mean(Y[Z==0])) estATE2 &lt;- lm(Y~Z,data=dat1)$coef[[&quot;Z&quot;]] c(estimatedATEv1=estATE1,estimatedATEv2=estATE2) estimatedATEv1 estimatedATEv2 4.637 4.637 stopifnot(all.equal(estATE1,estATE2)) The design-based perspective causes differences in our approach when we think about how to calculate standard errors (and thus ppp-values and confidence intervals). 3.1.1 How do we calculate randomization-based standard errors? How would an estimate of the average treatment effect vary if we repeated the experiment on the same group of people? The standard error of an estimate of the average treatment effect is one answer to this question. Below, we simulate a simple, individual-level experiment to develop intuition about what a standard error is.2 simEstAte &lt;- function(Z,y1,y0){ ## A function to re-assign treatment and recalculate the difference of means ## Treatment was assigned without blocking or other structure, so we ## just permute or shuffle the existing treatment assignment vector Znew &lt;- sample(Z) Y &lt;- Znew * y1 + (1-Znew) * y0 estate &lt;- mean(Y[Znew == 1]) - mean(Y[Znew == 0]) return(estate) } sims &lt;- 10000 set.seed(12345) simpleResults &lt;- with(dat1, replicate(sims, simEstAte(Z = Z, y1 = y1, y0 = y0))) ## The standard error of the estimate of the ATE. seEstATEsim &lt;- sd(simpleResults) seEstATEsim [1] 0.9256 Although this preceding standard error is intuitive (it is merely the standard deviation of the distribution arising from repeating the experiment), more statistics-savvy readers will recognize closed-form expressions for the standard error like the following (See Gerber and Green (2012) and Dunning (2012) for easy to read explanations and derivations of the design-based standard error of the simple estimator of the average treatment effect.) If we write TTT as the set of all mmm treated units and CCC as the set of all n&#x2212;mn-mn&#x2212;m non-treated units, we might write where s2(x)s^2(x)s2(x) is the sample variance such that s2(x)=(1/(n&#x2212;1))&#x2211;i=1n(xi&#x2212;x&#x2C9;)2s^2(x) = (1/(n-1))\\sum^n_{i = 1}(x_i-\\bar{x})^2s2(x)=(1/(n&#x2212;1))&#x2211;i=1n&#x200B;(xi&#x200B;&#x2212;x&#x2C9;)2. Here we compare the results of the simulation to this most common standard error as well as to the true version: We know what the true variance of the estimated ATE would be because we know the algebra of variances and covariances and because, in this example, we know the actual underlying counterfactual outcomes. We show this here to show that &#x201C;standard deviation of the estimated ATE after repeating the experiment&#x201D; is the same as what textbooks teach. ## True SE (Dunning Chap 6, Gerber and Green Chap 3 and Freedman, Pisani and Purves A-32) ## including the covariance between the potential outcomes N &lt;- nrow(dat1) V &lt;- var(cbind(dat1$y0,dat1$y1)) varc &lt;- V[1,1] vart &lt;- V[2,2] covtc &lt;- V[1,2] nt &lt;- sum(dat1$Z) nc &lt;- N-nt varestATE &lt;- ((N-nt)/(N-1)) * (vart/(N-nt)) + ((N-nc)/(N-1)) * (varc/nc) + (2/(N-1)) * covtc seEstATETrue &lt;- sqrt(varestATE) And the finite sample feasible version (where we do not observe all the potential outcomes) and so we do not have the covariance. This is what we calculate &#x2014; notice that it is not what OLS calculates. varYc &lt;- with(dat1,var(Y[Z == 0])) varYt &lt;- with(dat1,var(Y[Z == 1])) fvarestATE &lt;- (N/(N-1)) * ( (varYt/nt) + (varYc/nc) ) estSEEstATE &lt;- sqrt(fvarestATE) Here we use the HC2 standard error &#x2014; which (Lin (2013)) shows is the randomization-justified SE for OLS. Following our design-based approach, we use this standard error. And below we compare the true standard error, the feasible standard error, the HC2 SE (which is the same as the feasible standard error), the standard error arising from direct repetition of the experiment, and the OLS standard error. lm1 &lt;- lm(Y~Z,data=dat1) ## The OLS SE iidSE &lt;- sqrt(diag(vcov(lm1)))[[&quot;Z&quot;]] ## Worth noting that if we had covariates in the model we would want this one ## (which is identical to the previous one without covariates). NeymanSE &lt;- sqrt(diag(vcovHC(lm1,type = &quot;HC2&quot;)))[[&quot;Z&quot;]] compareSEs &lt;- c(simSE = seEstATEsim, feasibleSE = estSEEstATE, trueSE = seEstATETrue, olsIIDSE = iidSE, NeymanDesignSE = NeymanSE) sort(compareSEs) trueSE olsIIDSE simSE NeymanDesignSE feasibleSE 0.6760 0.8930 0.9256 1.0387 1.0439 The Neyman SE is supposed to be conservative relative to the true SE. Below, we show this to be the case. In this particular case, the SE of the OLS estimator is larger than both of the other SEs &#x2014; recall that our design involves different variance between the outcomes in the treated group and the control group &#x2014; so we expect that what we are calling the &#x201C;iid&#x201D; SE should be biased but not necessarily guaranteed to be overly conservative or liberal in all cases. sePerfFn &lt;- function(Z,y1,y0){ Znew &lt;- sample(Z) Ynew &lt;- Znew * y1 + (1-Znew) * y0 lm1 &lt;- lm(Ynew~Znew) iidSE &lt;- sqrt(diag(vcov(lm1)))[[&quot;Znew&quot;]] NeymanSE &lt;- sqrt(diag(vcovHC(lm1,type = &quot;HC2&quot;)))[[&quot;Znew&quot;]] return(c(estATE=coef(lm1)[[&quot;Znew&quot;]], estSEiid=iidSE, estSENeyman=NeymanSE)) } set.seed(12345) sePerformance &lt;- with(dat1, replicate(sims, sePerfFn(Z = Z, y1 = y1, y0 = y0))) apply(sePerformance[c(&quot;estSEiid&quot;, &quot;estSENeyman&quot;), ], 1, summary) estSEiid estSENeyman Min. 0.7004 0.6373 1st Qu. 0.8301 0.8949 Median 0.8511 0.9579 Mean 0.8511 0.9574 3rd Qu. 0.8720 1.0205 Max. 0.9600 1.2640 ExpectedSEs &lt;- apply(sePerformance[c(&quot;estSEiid&quot;, &quot;estSENeyman&quot;), ], 1, mean) c(ExpectedSEs, trueSE = seEstATETrue, simSE = sd(sePerformance[&quot;estATE&quot;, ])) estSEiid estSENeyman trueSE simSE 0.8511 0.9574 0.6760 0.9256 When we have a two arm trial, we can estimate the ATE and calculate design-based standard errors and use them to create large-sample justified confidence intervals in relatively large experiments using either of the following approaches: ## the difference_in_means function comes from the estimatr package estAndSE1 &lt;- difference_in_means(Y ~ Z, data = dat1) ## coeftest and coefci come from the lmtest package est2 &lt;- lm(Y ~ Z, data = dat1) estAndSE2 &lt;- coeftest(est2, vcov.=vcovHC(est2, type = &quot;HC2&quot;)) estAndCI2&lt;- coefci(est2, vcov.=vcovHC(est2, type = &quot;HC2&quot;), parm = &quot;Z&quot;) estAndSE1 Design: Standard Estimate Std. Error t value Pr(&gt;|t|) CI Lower CI Upper DF Z 4.637 1.039 4.465 8.792e-05 2.524 6.75 33.12 estAndSE2[&quot;Z&quot;, , drop=FALSE] Estimate Std. Error t value Pr(&gt;|t|) Z 4.637 1.039 4.465 2.147e-05 estAndCI2 2.5 % 97.5 % Z 2.576 6.699 3.2 Summary: What does a design based approach mean for policy evaluation? Hypothesis tests produce ppp-values telling us how much information we have against null hypotheses. Estimators produce estimates &#x2014; guesses about some causal effect like the average treatment effect. Standard errors summarize how our estimates might vary from experiment to experiment. Confidence intervals tell us which ranges of null hypotheses are more versus less consistent with our data. Recall that ppp-values and standard errors refer to probability distributions of test statistics under a null hypothesis and the distributions of estimators across repeated experiments, respectively. In the frequentist approach to probability, these probability distributions arise from some process of repetition. Statistics textbooks often encourage us to imagine that this process of repetition involves repeated sampling from a population, or even a hypothetical super-population. But, most of the work of the OES involves a pool of people who are not a well-definied population, nor do we tend to have a strong probability model of how these people entered our sample. Instead, we have a known process of random assignment to an experimental intervention. And this makes a randomization-based inference approach natural for our work, and helps our work be easiest to explain and interepret for our policy partners. References "],
["randomization-and-design.html", "Chapter 4 Randomization and Design 4.1 Coin flipping randomization versus Urn-drawing randomization 4.2 Urn-Drawing or Complete Randomization into 2 or more groups 4.3 Factorial Designs 4.4 Block Random Assignment 4.5 Cluster random assignment 4.6 Other designs 4.7 Randomization assessment", " Chapter 4 Randomization and Design After working together with our agency partners to translate insights from the social and behavioral sciences into policy recommendations following our process, our combined OES and agency teams assess these new ideas by observing differences and changes in real world outcomes (usually measured using existing administrative data). Nearly always, we design a randomized control trial (an RCT) to ensure that the differences and changes we observe arise from the policy intervention and not from some other pre-existing difference or change. Here we show examples of the ways that we create the random numbers that form the core of our different types of RCTs that we use to build evidence about the effectiveness of the new policy. 4.1 Coin flipping randomization versus Urn-drawing randomization Many discussions of RCTs begin by talking about the intervention being assigned to units (people, schools, offices, districts) &#x201C;by the flip of a coin&#x201D;. We rarely use this method directly even though it is a useful way to introduce the idea that RCTs guarantee fair access to the new policy. The following code contrasts coin-flipping style randomization (where the number of units in each condition is not guaranteed) with drawing-from-an-urn style, or complete, randomization (where the number of units in each condition is guaranteed). We try to avoid the coin-flipping style of randomization because of this lack of control over the number of units in each condition &#x2014; coin-flipping based experiments are still valid and tell us about the underlying counterfactuals, but they can have less statistical power. Notice that the simple randomization implemented in the code below results in more observations in the treatment group (group 1) than in the control group (group 0). The complete randomization will always assign 5 units to the treatment, 5 to the control. ## Start with a small experiment with only 10 units n &lt;- 10 ## Set a seed for the pseudo-random number generator so that we always get the same results set.seed(12345) ## Coin flipping does not guarantee half and half treated and control ## This next bit of code shows the base R version of coin flipping randomization ## trt_coinflip &lt;- rbinom(10, size = 1, prob = .5) trt_coinflip &lt;- simple_ra(n) ## Drawing from an urn or shuffling cards, guarantees half treated and control ##trt_urn &lt;- sample(rep(c(1, 0), n / 2)) trt_urn &lt;- complete_ra(n) table(trt_coinflip) trt_coinflip 0 1 3 7 table(trt_urn) trt_urn 0 1 5 5 4.2 Urn-Drawing or Complete Randomization into 2 or more groups We tend to use the randomizr R package (Coppock 2022a) for simple designs rather than the base R sample function because thee randomizr does some quality control checks. Notice that we implement a check on our code below with the stopifnot command: the code will stop and issue a warning if we didn&#x2019;t actually assign 1/4 of the observations to the treatment condition. Here, we assign the units first to 2 arms with equal probability (Z2armEqual) and then, to show how the code works, to 2 arms where one arm has only 1/4 the probability of receiving treatment (imagining a design with an expensive intervention) (Z2armUnequalA and Z2armUnequalB), and then to a design with 4 arms, each with equal probability. (We often use ZZZ to refer to the variable recording our intervention arms.) N &lt;- nrow(dat1) ## Two equal arms dat1$Z2armEqual &lt;- complete_ra(N) ## Two unequal arms: .25 chance of treatment set.seed(12345) dat1$Z2armUnequalA &lt;- complete_ra(N,prob=.25) stopifnot(sum(dat1$Z2armUnequalA)==N/4) dat1$Z2armUnequalB &lt;- complete_ra(N,m=N/4) ## Four equal arms dat1$Z4arms &lt;- complete_ra(N,m_each=rep(N/4,4)) table(Z2armEqual=dat1$Z2armEqual) Z2armEqual 0 1 50 50 table(Z2armUnequalA=dat1$Z2armUnequalA) Z2armUnequalA 0 1 75 25 table(Z2armUnequalB=dat1$Z2armUnequalB) Z2armUnequalB 0 1 75 25 table(Z4arms=dat1$Z4arms) Z4arms T1 T2 T3 T4 25 25 25 25 4.3 Factorial Designs One can often test the effects of more than one intervention without losing much statistical power by randomly assigning more than one treatment independently of each other. The simplest design that we use for this purpose is the 2&#xD7;22 \\times 22&#xD7;2 factorial design. For example, in the next table we see that we have assigned 50 observations to each arm of both of two interventions. Since the randomization of treatment1 is independent of treatment2, we can assess the effects of each treatment separately and pay little power penalty unless one of the treatments dramatically increases the variance of the outcome compared to the experiment with only one treatment assigned. ## Two equal arms, second cross treatment dat1$Z2armEqual2 &lt;- complete_ra(N) table(treatment1=dat1$Z2armEqual,treatment2=dat1$Z2armEqual2) treatment2 treatment1 0 1 0 22 28 1 28 22 Although factorial designs allow us to test more than one intervention at the same time, they tend to provide little statistical power for testing hypotheses about the interaction of the two treatments. If we want to learn about how two different interventions work together, then the sample size requirements will be much larger than if we want to learn about only each treatment separately.3 4.4 Block Random Assignment Statistical power depends not only on the size of the experiment and the strength of the treatment effect, but also on the amount of noise, or non-treatment-related variability, in outcomes. Block-randomized designs can help reduce the noise in outcomes, while simultaneously minimizing estimation error &#x2013; the amount that our particular experiment&#x2019;s estimate differs from the truth. In a block-randomized, or stratified, experiment, we randomly assign units to the policy interventions within groups. Suppose we are evaluating whether dedicated navigators can increase the percentage of students who live in public housing who complete federal financial aid applications (FAFSA). Our experiment will send navigators to two of four eligible buildings, two of which are large and two of which are small. Though we can never know the outcome in all buildings both with and without navigators (the &#x201C;fundamental problem of causal inference&#x201D;), if we could, we might have the data below. In this case, the true average treatment effect for this sample is the average under treatment minus the average under control: ATE=50&#x2212;25=25\\text{ATE} = 50-25=25ATE=50&#x2212;25=25 percent more applications if navigators are deployed everywhere. If we randomly allocate two buildings to treatment and two to control, we might treat the first two buildings and observe yielding an estimate of the treatment effect of 65&#x2212;25=4065-25 = 4065&#x2212;25=40 percent more applications &#x2013; an estimate larger than the true value of 25. Or, we might observe yielding an estimate of the treatment effect of 45&#x2212;30=1545-30 = 1545&#x2212;30=15 percent fewer applications &#x2013; an estimate smaller than the true value of 25. All the possible equiprobable assignments, and their estimated treatment effects are below. These possible estimates have mean equal to the true value of 25, showing that the difference in means is an unbiased estimator. However, some of these estimates are far from the truth, and they have a lot of variability. To design our experiment to best estimate the true value, and to do so with more statistical power, we can block the experiment. Here, this means restricting our randomization to those three possible assignments that balance the large and small buildings across the treatment groups. With the blocked design, we will get an estimate no more than 10 percentage points from the truth. Further, our estimates will have less variability (an SD of 8.16 rather than 11.4), improving the power of our design. For a more realistic example, suppose our experiment has two hospitals. We might randomly assign people to treatment and control within each hospital. We might assign half of the people in hospital &#x201C;A&#x201D; to treatment and half to control and likewise in hospital &#x201C;B.&#x201D;4 Below, we have 50 units in hospital &#x201C;A&#x201D; and 50 in hospital &#x201C;B&#x201D;. dat1$blockID &lt;- gl(n = 2, k = N/2, labels = c(&quot;A&quot;, &quot;B&quot;)) with(dat1,table(blockID=dat1$blockID)) blockID A B 50 50 We assign half of the units in each hospital to each treatment condition: dat1$Z2armBlocked &lt;- block_ra(blocks=dat1$blockID) with(dat1,table(blockID,Z2armBlocked)) Z2armBlocked blockID 0 1 A 25 25 B 25 25 If, say, there were fewer people eligible for treatment in hospital &#x201C;A&#x201D; &#x2014; or perhaps the intervention was more expensive in that block &#x2014; we might assign treatment with different probability within each block. The code below shows half of the hospital &#x201C;A&#x201D; units assigned to treatment, but only a quarter of those from hospital &#x201C;B&#x201D;. Again, we check that this code worked by including a test. This approach is an informal version of one of the best practices in writing code in general, called &#x201C;unit testing&#x201D;. See the EGAP Guide to Workflow for more examples. dat1$Z2armBlockedUneqProb &lt;- block_ra(blocks=dat1$blockID,block_prob = c(.5,.25)) with(dat1,table(blockID,Z2armBlockedUneqProb)) Z2armBlockedUneqProb blockID 0 1 A 25 25 B 38 12 stopifnot(sum(dat1$Z2armBlockedUneqProb==1&amp;dat1$blockID==&quot;B&quot;)==ceiling(sum(dat1$blockID==&quot;B&quot;)/4)| sum(dat1$Z2armBlockedUneqProb==1&amp;dat1$blockID==&quot;B&quot;)==floor(sum(dat1$blockID==&quot;B&quot;)/4)) Our team tries to implement block-randomized assignment whenever possible in order to increase the statistical power of our experiments. We also often find it useful in cases where different administrative units are implementing the treatment or where we expect different groups of people to have different reactions to the treatment. 4.4.1 Using only a few covariates to create blocks If we have background information on a few covariates, we can great blocks by hand using the kind of process demonstrated here: ## For example, make three groups from the cov2 variable dat1$cov2cat &lt;- with(dat1,cut(cov2,breaks=3)) table(dat1$cov2cat,exclude=c()) (-7.32,-2.6] (-2.6,2.1] (2.1,6.82] 11 68 21 with(dat1,tapply(cov2,cov2cat,summary)) $`(-7.32,-2.6]` Min. 1st Qu. Median Mean 3rd Qu. Max. -7.30 -5.03 -3.66 -4.14 -3.01 -2.77 $`(-2.6,2.1]` Min. 1st Qu. Median Mean 3rd Qu. Max. -2.3916 -0.7630 -0.0194 -0.0218 0.7592 2.0864 $`(2.1,6.82]` Min. 1st Qu. Median Mean 3rd Qu. Max. 2.13 2.46 2.95 3.24 3.68 6.80 ## And we can make blocks that are the same on two covariates dat1$cov1bin &lt;- as.numeric( dat1$cov1 &gt; median(dat1$cov1) ) dat1$blockV2 &lt;- droplevels(with(dat1,interaction(cov1bin,cov2cat))) table(dat1$blockV2,exclude=c()) 0.(-7.32,-2.6] 1.(-7.32,-2.6] 0.(-2.6,2.1] 1.(-2.6,2.1] 0.(2.1,6.82] 1.(2.1,6.82] 7 4 38 30 5 16 ## And then assign within these blocks set.seed(12345) dat1$ZblockV2 &lt;- block_ra(blocks = dat1$blockV2) with(dat1,table(blockV2,ZblockV2,exclude=c())) ZblockV2 blockV2 0 1 0.(-7.32,-2.6] 4 3 1.(-7.32,-2.6] 2 2 0.(-2.6,2.1] 19 19 1.(-2.6,2.1] 15 15 0.(2.1,6.82] 2 3 1.(2.1,6.82] 8 8 4.4.2 Multivariate blocking using many covariates If we have many background variables, we can increase precision by thinking about the problem of blocking as a problem of matching or creating sets of units which are as similar as possible in terms of the collection of those covariates (Moore 2012; Moore and Schnakenberg 2016). Here we show two approaches. Creating pairs: ## using the blockTools package mvblocks &lt;- block(dat1,id.vars=&quot;id&quot;,block.vars=c(&quot;cov1&quot;,&quot;cov2&quot;),algorithm=&quot;optimal&quot;) dat1$blocksV3 &lt;- createBlockIDs(mvblocks,data=dat1,id.var = &quot;id&quot;) dat1$ZblockV3 &lt;- block_ra(blocks = dat1$blocksV3) ## just show the first ten pairs with(dat1,table(blocksV3,ZblockV3,exclude=c()))[1:10,] ZblockV3 blocksV3 0 1 1 1 1 2 1 1 3 1 1 4 1 1 5 1 1 6 1 1 7 1 1 8 1 1 9 1 1 10 1 1 Creating larger blocks: ## using the quickblock package distmat &lt;- distances(dat1, dist_variables = c(&quot;cov1&quot;, &quot;cov2&quot;),id_variable = &quot;id&quot;, normalize=&quot;mahalanobiz&quot;) distmat[1:5,1:5] 1 2 3 4 5 1 0.0000 1.4766 0.3313 0.2835 0.6736 2 1.4766 0.0000 1.6919 1.5891 0.9681 3 0.3313 1.6919 0.0000 0.5477 0.7729 4 0.2835 1.5891 0.5477 0.0000 0.9089 5 0.6736 0.9681 0.7729 0.9089 0.0000 quantile(as.vector(distmat),seq(0,1,.1)) 0% 10% 20% 30% 40% 50% 60% 70% 80% 90% 100% -3.13279 -1.18748 -0.79019 -0.38455 -0.14051 0.08898 0.26701 0.53818 0.89242 1.34321 2.91687 ## The caliper argument is supposed to prevent the inclusion of ill-matched ## points. mvbigblock &lt;- quickblock(distmat, size_constraint = 6L) #,caliper=2.5) ## Look for missing points table(mvbigblock,exclude=c()) mvbigblock 0 1 2 3 4 5 6 7 8 9 10 11 6 6 6 6 8 11 14 7 7 10 7 12 dat1$blocksV4 &lt;- mvbigblock dat1$ZblockV4 &lt;- block_ra(blocks = dat1$blocksV4) with(dat1,table(blocksV4,ZblockV3,exclude=c()))[1:10,] ZblockV3 blocksV4 0 1 0 3 3 1 2 4 2 4 2 3 3 3 4 5 3 5 6 5 6 6 8 7 4 3 8 4 3 9 4 6 Here we produce some description of the differences within block: the proportion of people in category &#x201C;1&#x201D; on the binary covariate (notice that the sets are homogeneous on this covariate) and the difference between the largest and smallest value on the continuous covariate. blockingDescEval &lt;- dat1 %&gt;% group_by(blocksV4) %&gt;% summarize(cov2diff = max(abs(cov2)) - min(abs(cov2)), cov1 = mean(cov1)) blockingDescEval # A tibble: 12 &#xD7; 3 blocksV4 cov2diff cov1 &lt;qb_blckn&gt; &lt;dbl&gt; &lt;dbl&gt; 1 0 3.62 0.555 2 1 2.92 4.21 3 2 1.91 -3.89 4 3 3.64 -1.38 5 4 2.78 3.60 6 5 1.28 1.51 7 6 3.44 0.577 8 7 1.91 -0.0277 9 8 1.46 1.56 10 9 2.61 -2.41 11 10 2.68 -1.35 12 11 1.16 -0.770 4.5 Cluster random assignment We often implement a new policy intervention at the level of some group of people &#x2014; like a doctor&#x2019;s practice, or a building, or some other administrative unit. Notice that, even though we have 100 units in our example data, imagine that they are grouped into 10 buildings, and the policy intervention is at the building level. Below, we assign 50 of those units to treatment and 50 to control. Everyone in each building has the same treatment assignment. ndat1 &lt;- nrow(dat1) ## Make an indicator of cluster membership dat1$buildingID &lt;- rep(1:(ndat1/10),length=ndat1) set.seed(12345) dat1$Zcluster &lt;- cluster_ra(cluster=dat1$buildingID) with(dat1,table(Zcluster,buildingID)) buildingID Zcluster 1 2 3 4 5 6 7 8 9 10 0 10 0 10 10 0 0 0 0 10 10 1 0 10 0 0 10 10 10 10 0 0 Cluster randomized designs raise new questions about estimation and testing and thus statistical power. We describe our approaches to analysis and power analysis of cluster randomized designs in the section on the analysis of Cluster Randomized Trials. 4.6 Other designs Our team has also designed stepped-wedge style designs, saturation designs aimed to discover whether the effects of the experimental intervention are communicated across people (via some spillover or network mechanism), and designs where we try to isolate certain experimental units (like buildings) from each other so that we can focus our learning about the effects of the intervention rather than on the effects of communication of the intervention across people. In later versions of this document we will include simple descriptions and code for those other, less common designs. 4.7 Randomization assessment If we have covariates, we can assess the performance of the randomization procedure by testing the hypothesis that the treatment-vs-control differences, or differences across treatment arms, in covariates are consistent with our claimed mechanism of randomization. In the absence of covariates, we assess whether the number of units assigned to each arm (conditional on other design features, such as blocking or stratification) are consistent with the claimed random assignment. Here is an example with a binary treatment and a continuous outcome and 10 covariates. In this case we use the d2d^2d2 omnibus balance test function xBalance() in the package RItools (see Hansen and Bowers 2008 ; Bowers, Fredrickson, and Hansen 2016). The &#x201C;overall&#x201D; ppp-value below shows us that we have little evidence against the idea that treatment (ZZZ) was assigned at random &#x2014; at least in terms of the relationships between the two covariates and the treatment assignment. The test statistics here are mean differences. This overall or omnibus test is the key here &#x2014; if we had many covariates, it would easy to discover one or a few covariates with means that differ detectably from zero just by chance. That is, in a well-operating experiment, we would expect some baseline imbalances (as measured using randomization based tests) &#x2013; roughly 5 in 100. Thus the value of the omnibus test is that we will not be misled by chance false positives. randAssessV1 &lt;- balanceTest(Z~cov1+cov2,data=dat1) randAssessV1$overall[,] chisquare df p.value -- 1.998 2 0.3683 Here is an example of randomization assessment with block-randomized assignment: randAssessV3 &lt;- balanceTest(ZblockV3~cov1+cov2+strata(blocksV3),data=dat1) randAssessV3$overall[,] chisquare df p.value blocksV3 2.8342 2 0.2424 -- 0.1365 2 0.9340 One can also assess the randomization given both block and cluster random assignment using a formula like so: Z ~ cov1 + cov2 + strata(blockID) + cluster(clusterID). This approach, using the RItools package, works well for experiments that are not overly small. In a very small experiment, say, an experiment with 5 clusters assigned to treatment and 5 clusters assigned to control, we would do the same test, but would not use the &#x3C7;2\\chi^2&#x3C7;2 distribution. Instead, we would do a permutation-based test. We do not use FFF-tests or Likelihood Ratio tests to assess randomization. See Hansen and Bowers (2008) for some evidence that a test based on randomization-inference (like the d2d^2d2 test developed in that article) maintains false positive rates better than the sampling- or likelihood-justified FFF and likelihood ratio tests. 4.7.1 What to do with &#x201C;failed&#x201D; randomization assessments? A ppp-value less than .05 on a test of randomization mechanism ought to triggers extra scrutiny of how the randomization was conducted and how the data were recorded by the agency. For example, we might contact our agency partner to learn more deetails about how the random numbers themselves were generated, or we may ask for the SQL or SAS code or other code that might have been used to randomize. In most cases, we will learn that randomization worked well but that our understanding of the design and the data were incorrect. In some cases, we will learn that the randomization occurred as we had initially understood. In such cases, we tend to assume that rejecting the null in our randomization assessment is a false positive from our testing procedure (we assume we would see about 5 such errors in every 100 experiments). If the rejection of the null appears to be driven by one or more particularly substantively relevant covariates, say, the variable age looks very imbalanced between treated and control groups in a health study, then we will present both the unadjusted results but also adjust for that covariate via stratification and/or covariance adjustment as we describe later in this document. A chance rejection of the null that the experiment was randomized as it should not cause us to the use the adjusted estimate as our primary causal effect &#x2014; after all, it will be biased whereas the unadjusted estimate will not be biased undere chance departures from the null. But, large differences between the two estimates can inform the qualitative task of substantive interpretation of the study: looking at different effects by age groups, for example, might tell us something about the particular context of the study and, in turn, help us think about what we have learned. 4.7.2 How to minimize large chance departures of randomization? Our approach to block-randomization helps us avoid these problems. We can also restrict our randomization in ways that are more flexible than requiring blocks, but which, in turn should minimize chance. We tend not to use re-randomization, among methods of restricted randomization, only because we often can use block-randomization and thus can minimize the complexity of later analysis. However, since we pursue a randomization-based approach to the analysis of experiments, we can easily (in concept) estimate causal effects and test hypotheses about causal effects as well after such modes of randomization. References "],
["analysis-choices.html", "Chapter 5 Analysis Choices 5.1 Completely or Urn-Draw Randomized Trials 5.2 Covariance Adjustment (the use of background information to increase precision) 5.3 How to choose covariates for covariance adjustment? 5.4 Block-randomized trials 5.5 Cluster-randomized trials", " Chapter 5 Analysis Choices We organize our discussion of analysis tactics by design of the study. Different study designs require different analyses. But there are a few general tactics that we use to pursue the strategy of transparent, valid, and statistically precise statements about the results of our experiments. The nature of the data that we expect to see from a given experiment also informs our analysis plans. For example, we may make some specific choices depending on the nature of the measured outcome &#x2014; a binary outcome, a symmetrically distributed continuous outcome, and a heavily skewed continuous outcome each might require some different approaches within the blocked/not blocked and clustered/not clustered designs. We tend to ask three questions of each of our studies that we answer with a different statistical procedure. Can we detect an effect of our experiment? (We use hypothesis tests to answer this question). What is our best guess about the size of the effect of the experiment? (We estimate the average treatment effect of our interventions to answer this question.) How precise is our guess? (We report confidence intervals and standard errors.) Each procedure below describes testing a hypothesis of no effect, estimating an average treatment effect, and finding standard errors and confidence intervals for different categories of experiments. In our Analysis Plans, we try to anticipate many of the common decisions involved in data analysis &#x2014; including how we treat missing data, how we rescale, recode, and combine columns of raw data, etc. We touch on some of these topics below in general terms. 5.1 Completely or Urn-Draw Randomized Trials 5.1.1 Two arms 5.1.1.1 Continuous outcomes In a completely randomized trial where outcomes take on many levels (units like times, counts of events, dollars, percentages, etc.), we assess the weak null hypothesis of no average effects, we estimate an average treatment effect, and we often assess the sharp null hypothesis of no effects using some test statistic other than a difference of means. We perform this last assessment often as a check on whether our choice to focus on means matters for our substantive interpretation of the results of the study. 5.1.1.1.1 Estimating the average treatment effect and testing the weak null of no average effects We show the kind of code we use for these purposes here. Below, Y is the outcome variable and Z is an indicator of the assignment to treatment. estAndSE1 &lt;- difference_in_means(Y ~ Z,data = dat1) print(estAndSE1) Design: Standard Estimate Std. Error t value Pr(&gt;|t|) CI Lower CI Upper DF Z 4.637 1.039 4.465 8.792e-05 2.524 6.75 33.12 Notice that the standard errors that we use are not the same as the result from OLS itself: estAndSE1OLS &lt;- lm(Y~Z,data=dat1) summary(estAndSE1OLS)$coef Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 2.132 0.4465 4.775 6.283e-06 Z 4.637 0.8930 5.193 1.123e-06 The standard errors we use by default reflect repeated randomization from a fixed experimental pool. This is known as the HC2 standard error. Lin (2013) and Samii and Aronow (2012) show this is equivalent to the design-based standard error for standard unbiased estimators of the average treatment effect. These correct SEs are produced by default from the estimatr package&#x2019;s function difference_in_means() and the lmtest package&#x2019;s functions coeftest() and coefci(). 5.1.1.1.2 Testing the sharp null of no effects We also tend to assess the sharp null of no effects via direct permutation as a check on the assumptions underlying the statistical inferences above. We tend to use a ttt-statistic as the test statistic in this case to parallel the above test, and also use a rank-based test if we are concerned about long-tails reducing power in the above test. Below, we show how to perform these tests using three different R packages: RItools, coin, and ri2. First the RItools package (Bowers, Fredrickson, and Hansen 2023): ## Currently RItest is only in the randomization-distribution development branch ## of RItools. This code would work if that branch were installed. set.seed(12345) test1T &lt;- RItest(y = dat1$Y, z = dat1$Z, test.stat = t.mean.difference, samples = 1000) print(test1T) test1R &lt;- RItest(y = dat1$rankY, z = dat1$Z, test.stat = t.mean.difference, samples = 1000) print(test1R) The coin package (Hothorn et al. 2021): test1coinT &lt;- oneway_test(Y~factor(Z),data=dat1,distribution=approximate(nresample=1000)) test1coinT Approximative Two-Sample Fisher-Pitman Permutation Test data: Y by factor(Z) (0, 1) Z = -4.6, p-value &lt;0.001 alternative hypothesis: true mu is not equal to 0 test1coinR&lt;- oneway_test(rankY~factor(Z),data=dat1,distribution=approximate(nresample=1000)) test1coinR Approximative Two-Sample Fisher-Pitman Permutation Test data: rankY by factor(Z) (0, 1) Z = -4.9, p-value &lt;0.001 alternative hypothesis: true mu is not equal to 0 test1coinWR &lt;- wilcox_test(Y~factor(Z),data=dat1,distribution=approximate(nresample=1000)) test1coinWR Approximative Wilcoxon-Mann-Whitney Test data: Y by factor(Z) (0, 1) Z = -4.9, p-value &lt;0.001 alternative hypothesis: true mu is not equal to 0 The ri2 package (Coppock 2022b): ## using the ri2 package thedesign1 &lt;- randomizr:::declare_ra(N=ndat1,m=sum(dat1$Z)) thedesign1 test1riT &lt;- conduct_ri( Y ~ Z, declaration = thedesign1, sharp_hypothesis = 0, data = dat1, sims = 1000) summary(test1riT) test1riR &lt;- conduct_ri( rankY ~ Z, declaration = thedesign1, sharp_hypothesis = 0, data = dat1, sims = 1000) summary(test1riR) 5.1.1.2 Binary outcomes We tend to focus on differences in proportions when we are working with binary outcomes. A statement such as &#x201C;The effect was 5 percentage points.&#x201D; has tended to make communication with our policy partners easier than a discussion in terms of log odds or odds ratios or probabilities. Our tests of the hypothesis of no difference tend to change in the case of binary outcomes, however, in order to increase statistical power. In addition to the problem of interpretation and communication, we also avoid logistic regression coefficients because of the bias problem noticed by Freedman (2008b) in the case of covariance adjustment or more complicated research designs. See our memo on binary_outcomes.Rmd for some simulation studies showing the code and explaining more of the reasoning behind Freedman&#x2019;s results. 5.1.1.2.1 Estimating the average treatment effect and testing the weak null of no average effects In our standard practice, we can estimate effects and produce standard errors for differences of proportions using the same process as above. The estimate is an estimate of the difference in proportions of positive responses between the treatment conditions. The standard error is valid because it is based on the design of the study and not the distribution of the outcomes. ## Make some binary outcomes dat1$u &lt;- runif(ndat1) dat1$v &lt;- runif(ndat1) dat1$y0bin &lt;- ifelse(dat1$u&gt;.5, 1, 0) #potential outcomes dat1$y1bin &lt;- ifelse((dat1$u+dat1$v) &gt;.75, 1, 0) #potential outcomes dat1$Ybin &lt;- with(dat1, Z*y1bin + (1-Z)*y0bin) truePropDiff &lt;- mean(dat1$y1bin) - mean(dat1$y0bin) estAndSE2 &lt;- difference_in_means(Ybin~Z,data=dat1) print(estAndSE2) Design: Standard Estimate Std. Error t value Pr(&gt;|t|) CI Lower CI Upper DF Z 0.1733 0.1084 1.599 0.1168 -0.04503 0.3917 44.65 When we have an experiment that includes treatment and control with binary outcomes and we are estimating the ATE, the standard errors in the difference in proportions test are the same as the standard errors in a regular OLS regression, which are also the same as the Neyman standard errors. Difference of proportions standard errors are estimated with the following equation: where we can think of n1n_1n1&#x200B; as the size of the group assigned treatment, n2n_2n2&#x200B; as the size of the group assigned control, p1p_1p1&#x200B; as the proportion of &#x201C;successes&#x201D; in the group assigned treatment, and p2p_2p2&#x200B; as the proportion of &#x201C;successes&#x201D; in the group assigned control. We can compare this with the Neyman standard errors equation, see Lin (2013): where YcY_cYc&#x200B; is outcomes under control and YtY_tYt&#x200B; is outcomes under treatment; we use the variance of each population to find the Neyman standard error. We can also compare both difference in proportions and Neyman standard errors to OLS standard errors, written in matrix form: SE^OLS=VAR(ATE^)(X&#x2032;X)&#x2212;1\\widehat{SE}_{OLS} = \\sqrt{VAR(\\widehat{ATE})(X&apos;X)^{-1}}SEOLS&#x200B;=VAR(ATE)(X&#x2032;X)&#x2212;1&#x200B; where VAR(ATE^)VAR(\\widehat{ATE})VAR(ATE) is the variance of the estimated ATE coefficient and (X&#x2032;X)&#x2212;1(X&apos;X)^{-1}(X&#x2032;X)&#x2212;1 is a scalar since X is a vector. When no additional covariates and only binary outcomes are in the model, all three versions produce the same standard errors, as depicted in the code below. nt &lt;- sum(dat1$Z) nc &lt;- sum(1-dat1$Z) ## 2. Find SE for difference of proportions. p1 &lt;- mean(dat1$Ybin[dat1$Z==1]) p0 &lt;- mean(dat1$Ybin[dat1$Z==0]) se1 &lt;- (p1*(1-p1))/nt se0 &lt;- (p0*(1-p0))/nc se_prop &lt;- round(sqrt(se1 + se0), 4) ## 3. Find Neyman SE varc_s &lt;- var(dat1$Ybin[dat1$Z == 0]) vart_s &lt;- var(dat1$Ybin[dat1$Z == 1]) se_neyman &lt;- round(sqrt((vart_s/nt) + (varc_s/nc)), 4) ## 4. Find OLS SE simpOLS &lt;- lm(Ybin~Z,dat1) se_ols &lt;- round(coef(summary(simpOLS))[&quot;Z&quot;, &quot;Std. Error&quot;], 2) ## 5. Find Neyman SE (which are the HC2 SEs) se_neyman2 &lt;- coeftest(simpOLS,vcov = vcovHC(simpOLS,type=&quot;HC2&quot;))[2,2] se_neyman3 &lt;- estAndSE2$std.error ## 5. Show SEs se_compare &lt;- as.data.frame(cbind(se_prop, se_neyman, se_neyman2, se_neyman3, se_ols)) rownames(se_compare) &lt;- &quot;SE(ATE)&quot; colnames(se_compare) &lt;- c(&quot;diff in prop&quot;, &quot;neyman&quot;,&quot;neyman&quot;,&quot;neyman&quot;, &quot;ols&quot;) print(se_compare) diff in prop neyman neyman neyman ols SE(ATE) 0.1066 0.1084 0.1084 0.1084 0.11 5.1.1.2.2 Testing the sharp null of no effects In this case, with a binary treatment and a binary outcome, we can also do a simple test of the hypothesis that outcomes are independent of treatment assignment using Fisher&#x2019;s exact test and we can also use the approaches above to produce results that do not rely on asymptotic assumptions. Below we show how the Fisher&#x2019;s exact test, the Exact Cochran-Mantel-Haenszel test, and the Exact &#x3C7;\\chi&#x3C7;-squared test produce the same answers. test2fisher &lt;- fisher.test(x=dat1$Z,y=dat1$Ybin) print(test2fisher) Fisher&apos;s Exact Test for Count Data data: dat1$Z and dat1$Ybin p-value = 0.2 alternative hypothesis: true odds ratio is not equal to 1 95 percent confidence interval: 0.7349 6.7377 sample estimates: odds ratio 2.117 test2chisq &lt;- chisq_test(factor(Ybin)~factor(Z),data=dat1,distribution=exact()) print(test2chisq) Exact Pearson Chi-Squared Test data: factor(Ybin) by factor(Z) (0, 1) chi-squared = 2.3, p-value = 0.2 test2cmh &lt;- cmh_test(factor(Ybin)~factor(Z),data=dat1,distribution=exact()) print(test2cmh) Exact Generalized Cochran-Mantel-Haenszel Test data: factor(Ybin) by factor(Z) (0, 1) chi-squared = 2.3, p-value = 0.2 Notice that a difference of proportions test can be done directly rather than through least squares where the null hypothesis is tested using a binomial distribution rather than a Normal distribution &#x2014; both approximate the underlying randomization distribution well. mat &lt;- with(dat1,table(Z,Ybin)) matpt&lt;-prop.test(mat[,2:1]) matpt 2-sample test for equality of proportions with continuity correction data: mat[, 2:1] X-squared = 1.7, df = 1, p-value = 0.2 alternative hypothesis: two.sided 95 percent confidence interval: -0.40898 0.06231 sample estimates: prop 1 prop 2 0.5467 0.7200 5.1.2 Multiple arms Multiple treatment arms can be analyzed as above, except that we tend to have more than one comparison between a treated group and a control group and so such studies raise both substantive and statistical questions about multiple testing or multiple comparisons. For example, the difference_in_means function asks which average treatment effect it should estimate &#x2014; and it only presents one comparison at a time: here we compare the treatment T2 with the baseline outcome of T1. We can compare arms 2&#x2013;3 with arm 1, as in the second set of results (lm_robust implements the same standard errors as difference_in_means but allows for more flexibility). estAndSE3 &lt;- difference_in_means(Y~Z4arms,data=dat1,condition1=&quot;T1&quot;,condition2=&quot;T2&quot;) print(estAndSE3) Design: Standard Estimate Std. Error t value Pr(&gt;|t|) CI Lower CI Upper DF Z4armsT2 0.2457 1.174 0.2092 0.8352 -2.118 2.609 46.35 estAndSE3multarms &lt;- lm_robust(Y~Z4arms,data=dat1) print(estAndSE3multarms) Estimate Std. Error t value Pr(&gt;|t|) CI Lower CI Upper DF (Intercept) 3.0158 0.748 4.0316 0.0001111 1.531 4.501 96 Z4armsT2 0.2457 1.174 0.2092 0.8347052 -2.086 2.577 96 Z4armsT3 0.1602 1.108 0.1446 0.8853288 -2.039 2.359 96 Z4armsT4 0.6972 1.271 0.5485 0.5846318 -1.826 3.220 96 In this case, we could have ((4&#xD7;4)&#x2212;4)/2)=6((4 \\times 4)-4)/2)=6((4&#xD7;4)&#x2212;4)/2)=6 tests. And if there were really no effects, and we rejected the null at &#x3B1;=.05\\alpha=.05&#x3B1;=.05, we would claim that there was at least one effect out of six tests much more than 5% of the time: 1&#x2212;(1&#x2212;.05)6=.271 - ( 1 - .05)^6 = .271&#x2212;(1&#x2212;.05)6=.27 or 27% of the time we would make a false positive error, claiming an effect existed when it did not. In general, our analysis of studies with multiple arms should reflect the fact that we are making multiple comparisons for two reasons. First, the family-wise error rate of these tests will differ from the individual error rate of single test. In short, testing more than one hypothesis increases the chance of making a Type I error (i.e., incorrectly rejecting a true null hypothesis). Suppose instead of testing a single hypothesis at a conventional significance level of &#x3B1;=0.05\\alpha = 0.05&#x3B1;=0.05 we tested two hypothesis at &#x3B1;=0.05\\alpha = 0.05&#x3B1;=0.05. The probability of retaining both hypotheses is (1&#x2212;&#x3B1;)2=.9025(1-\\alpha)^2 = .9025(1&#x2212;&#x3B1;)2=.9025 and the probability of rejecting at least one of these hypotheses is 1&#x2212;(1&#x2212;&#x3B1;)2=.09751-(1-\\alpha)^2 = .09751&#x2212;(1&#x2212;&#x3B1;)2=.0975 &#x2014; almost double our stated significance threshold of &#x3B1;=0.05\\alpha = 0.05&#x3B1;=0.05. Second, multiple tests will often be correlated and our tests should recognize these relationships (which will penalize the multiple testing less).5 So, our standard practice in multi-arm trials is the following: First, decide on a focal comparison: say, control/status quo versus receiving any version of the treatment. This test has a lot of statistical power and would have a controlled false positive rate. Next, either do the rest of the comparisons as exploration for future studies &#x2014; to give hints about where we might be seeing more or less of an effect OR do a second series of comparisons but adjusting for the collective false positive rate (i.e.&#xA0;adjusting the Family Wise Error Rate &#x2014; see more later on when we might use FDR versus FWER). We will often use the Tukey HSD procedure for pairwise comparisons in this case or test the hypotheses in a particular order to preserve statistical power (Rosenbaum (2008)). 5.1.2.1 Adjusting p-values and confidence intervals for multiple comparisons using Tukey HSD in R Here is an illustration of how we might adjust for multiple comparisons. To reflect that fact that we are making multiple comparisons, we can adjust ppp-values from our tests to control the familywise error rate at &#x3B1;\\alpha&#x3B1; through either a single step (e.g.&#xA0;Bonferroni correction) or stepwise procedure (such as the Holm correction or the Benjamini-Hochberg correction). Our standard practice is to adjust FWER using the Holm adjustment. For more on such adjustments and multiple comparisons see EGAP&#x2019;s 10 Things you need to know about multiple comparisons. Explain here about what constitutes a family and how we choose. Also why Holm. And when FDR. # Get p-values but exclude intercept pvals &lt;- summary(estAndSE3multarms)$coef[2:4,4] round(p.adjust(pvals, &quot;none&quot;), 3) Z4armsT2 Z4armsT3 Z4armsT4 0.835 0.885 0.585 round(p.adjust(pvals, &quot;bonferroni&quot;), 3) Z4armsT2 Z4armsT3 Z4armsT4 1 1 1 round(p.adjust(pvals, &quot;holm&quot;), 3) Z4armsT2 Z4armsT3 Z4armsT4 1 1 1 round(p.adjust(pvals, &quot;hochberg&quot;), 3) ## The FDR Correction Z4armsT2 Z4armsT3 Z4armsT4 0.885 0.885 0.885 Simply adjusting ppp-values from this linear model, however, ignores the fact that we are likely interested in other pairwise comparisons, such as the difference between simply receiving an email and receiving an email with framing A (or framing B). It also ignores potential correlations in the distribution of test statistics. Below we demonstrate how to implement a Tukey Honest Signficant Differences (HSD) test. The Tukey HSD test (sometimes called a Tukey range test or just a Tuke test) calculates adjusted ppp-values and simultaneous confidence intervals from a studentized range distribution for all pairwise comparisons in a model, taking into account the correlation of test statistics. The test statistic for any comparison between group iii and jjj: tij=yi&#x2C9;&#x2212;yj&#x2C9;s2nt_{ij} = \\frac{\\bar{y_i}-\\bar{y_j}}{s\\sqrt{\\frac{2}{n}}}tij&#x200B;=sn2&#x200B;&#x200B;yi&#x200B;&#x2C9;&#x200B;&#x2212;yj&#x200B;&#x2C9;&#x200B;&#x200B; Where, yi&#x2C9;\\bar{y_i}yi&#x200B;&#x2C9;&#x200B; and yj&#x2C9;\\bar{y_j}yj&#x200B;&#x2C9;&#x200B; are the means in groups iii and jjj, respectively, sss is the pooled standard deviation and nnn is the common sample size. The confidence interval for any difference is simply: [yi&#x2C9;&#x2212;yj&#x2C9;&#x2212;u1&#x2212;&#x3B1;s2n;yi&#x2C9;&#x2212;yj&#x2C9;+u1&#x2212;&#x3B1;s2n]\\left[ \\bar{y_i}-\\bar{y_j}-u_{1-\\alpha}s\\sqrt{\\frac{2}{n}};\\bar{y_i}-\\bar{y_j}+u_{1-\\alpha}s\\sqrt{\\frac{2}{n}}\\right][yi&#x200B;&#x2C9;&#x200B;&#x2212;yj&#x200B;&#x2C9;&#x200B;&#x2212;u1&#x2212;&#x3B1;&#x200B;sn2&#x200B;&#x200B;;yi&#x200B;&#x2C9;&#x200B;&#x2212;yj&#x200B;&#x2C9;&#x200B;+u1&#x2212;&#x3B1;&#x200B;sn2&#x200B;&#x200B;] Where u1&#x2212;&#x3B1;u_{1-\\alpha}u1&#x2212;&#x3B1;&#x200B; denotes the (1&#x2212;&#x3B1;)(1-\\alpha)(1&#x2212;&#x3B1;)-quantile of the multivariate ttt-distribution. We present an implementation of the Tukey HSD test using the glht() function from the multcomp package which offers more flexiblity than the TukeyHSD in the base stats package at the price of a slightly more complicated syntax. ## We can use aov() or lm() ## aovmod &lt;- aov(Y~Z4arms, dat1) lmmod &lt;- lm(Y~Z4arms, dat1) Using the glht() function&#x2019;s linfcnt argument, we tell the function to conduct a Tukey test of all pairwise comparisons for our treatment indicator, ZZZ. tukey_mc &lt;- glht(lmmod, linfct = mcp(Z4arms = &quot;Tukey&quot;)) summary(tukey_mc) Simultaneous Tests for General Linear Hypotheses Multiple Comparisons of Means: Tukey Contrasts Fit: lm(formula = Y ~ Z4arms, data = dat1) Linear Hypotheses: Estimate Std. Error t value Pr(&gt;|t|) T2 - T1 == 0 0.2457 1.2456 0.20 1.00 T3 - T1 == 0 0.1602 1.2456 0.13 1.00 T4 - T1 == 0 0.6972 1.2456 0.56 0.94 T3 - T2 == 0 -0.0856 1.2456 -0.07 1.00 T4 - T2 == 0 0.4515 1.2456 0.36 0.98 T4 - T3 == 0 0.5370 1.2456 0.43 0.97 (Adjusted p values reported -- single-step method) We can plot the 95-percent family wise confidence intervals from these comparisons # Save dfault ploting parameters op &lt;- par() # Add space to lefthand outer margin par(oma = c(1, 3, 0, 0)) plot(tukey_mc) And also obtain simultaneous confidence intervals at other levels of statistical significance using confint() function tukey_mc_90ci &lt;- confint(tukey_mc, level = .90) plot(tukey_mc_90ci) # Restore plotting defaults ## par(op) See also: pairwise.prop.test for binary outcomes. 5.1.3 Multiple Outcomes Some times our studies involve more than one outcome. Assessing the effect of even a simple two-arm treatment on 10 different outcomes raises the same kinds of questions that come up in the context of multi-arm trials. 5.2 Covariance Adjustment (the use of background information to increase precision) When we have background or baseline information about experimental units, we can use this to increase the precision with which we estimate our treatment effects (or, equivalently, increase the statistical power of our tests). We prefer to use this information during the design phase to create block randomized designs, but we sometimes have access to such background information after the study has been fielded, and so we will pre-specify use of this information to increase our statistical power. We tend to avoid the practice of adjusting for the covariates in a linear and additive fashion because of this estimator of the average treatment effect is biased (Freedman 2008a) whereas a version of the estimator that we call the &#x201C;Lin estimator&#x201D; is not (Lin 2013). Note that the bias in the commonly used linear covariance adjustment estimator tends to be quite small, and especially small when sample sizes are large (Lin 2013). Yet, because it is basically costless to use the Lin estimator, this is our standard practice (see also https://declaredesign.org/blog/2018-09-11-controlling-pretreatment-covariates.html) 5.2.1 Intuition about bias in the least squares estimator of the ATE with covariates When we estimate the average treatment effect by using a least squares we tend to say that we &#x201C;regress&#x201D; some outcome for each unit iii, YiY_iYi&#x200B;, on (often binary) treatment assignment, ZiZ_iZi&#x200B;, where Zi=1Z_i=1Zi&#x200B;=1 if a unit is assigned to treatment and 0 if assigned to control. And we write a linear model relating ZZZ and YYY as below, where &#x3B2;1\\beta_1&#x3B2;1&#x200B; represents the difference in means of YYY between units with Z=1Z=1Z=1 and Z=0Z=0Z=0: This is a common practice because, we know that the formula to estimate &#x3B2;1\\beta_1&#x3B2;1&#x200B; in equation (??) is the same as the difference of means in YYY between treatment and control groups: This last term, expressed with covariances and variances, is the expression for the least squares coefficient in a bivariate linear least squares model. And we also know that this estimator of the average treatment effect has no systematic error (i.e.&#xA0;is unbiased), so we can write ER(&#x3B2;^1)=&#x3B2;1&#x2261;ATEE_R(\\hat{\\beta}_1)=\\beta_1 \\equiv \\text{ATE}ER&#x200B;(&#x3B2;^&#x200B;1&#x200B;)=&#x3B2;1&#x200B;&#x2261;ATE where we take the expectation across randomizations consistent with the experimental design. Now, sometimes we have a covariate XiX_iXi&#x200B; and we use it as would be common in the analysis of non-experimental data: What is &#x3B2;1\\beta_1&#x3B2;1&#x200B; in this case? We know the matrix representation here (XTX)&#x2212;1XTy(\\mathbf{X}^{T}\\mathbf{X})^{-1}\\mathbf{X}^{T}\\mathbf{y}(XTX)&#x2212;1XTy, but here is the scalar formula for this particular case in (??): &#x3B2;^1=Var(X)Cov(Z,Y)&#x2212;Cov(X,Z)Cov(X,Y)Var(Z)Var(X)&#x2212;Cov(Z,X)2\\hat{\\beta}_1 = \\frac{\\mathrm{Var}(X)\\mathrm{Cov}(Z,Y) - \\mathrm{Cov}(X,Z)\\mathrm{Cov}(X,Y)}{\\mathrm{Var}(Z)\\mathrm{Var}(X) - \\mathrm{Cov}(Z,X)^2}&#x3B2;^&#x200B;1&#x200B;=Var(Z)Var(X)&#x2212;Cov(Z,X)2Var(X)Cov(Z,Y)&#x2212;Cov(X,Z)Cov(X,Y)&#x200B; In very large experiments Cov(X,Z)&#x2248;0\\mathrm{Cov}(X,Z) \\approx 0Cov(X,Z)&#x2248;0 &#x2014; because ZZZ is randomly assigned and is thus independent of background variables like XXX &#x2014; however in any given finite sized experiment cov(X,Z)&#x2260;0cov(X,Z) \\ne 0cov(X,Z)&#xE020;=0 so this does not reduce to the unbiased estimator of the bivariate case. Thus, Freedman (2008a) showed that there is a small amount of bias in using equation (??) to estimate the average treatment effect. As a way to engage with this problem, Lin (2013) suggested using the following least squares approach &#x2014; regressing the outcome on binary treatment assignment ZiZ_iZi&#x200B; and its interaction with mean-centered covariates. See the Green-Lin-Coppock SOP for more examples of this approach to covariance adjustment. 5.2.2 The Lin Approach to Covariance Adjustment Here we show how covariance adjustment can create bias in estimation of the average treatment effect &#x2014; and how to reduce this bias while using the Lin procedure as well as by increasing the size of the experiment. In this case, we compare an experiment with 20 units to an experiement with 100 units, in each case with half of the units assigned to treatment by complete random assignment. We use the DeclareDesign package for R to make this process of assessing bias easier. So, much of the code that follows provides input to the diagnose_design command which repeats the design of the experiment many times, each time estimating an average treatment effect, and comparing the mean of those estimate to the truth (labeled &#x201C;Mean Estimand&#x201D; below). The true potential outcomes (y1 and y0) are generated using one covariate, called cov2. In what follows we compare the performance of the simple estimator using OLS, to estimators that use Lin&#x2019;s procedure involving just the correct covariate, to estimators that use incorrect covariates (since we rarely know exactly the covariates that help generate any given behavioral outcome). ##summary(lm(y0~cov2,data=dat1))$r.squared ##summary(lm(y1~cov2,data=dat1))$r.squared wrkdat1 &lt;- dat1 %&gt;% dplyr::select(id,y1,y0,contains(&quot;cov&quot;)) popbigdat1 &lt;- declare_population(wrkdat1) ## Make a small dataset to represent a small experiment or a cluster randomized experiment with few clusters set.seed(12345) smalldat1 &lt;- dat1 %&gt;% dplyr::select(id,y1,y0,contains(&quot;cov&quot;)) %&gt;% sample_n(20) ## The relevant covariate is a reasonably strong predictor of the outcome summary(lm(y0~cov2,data=smalldat1))$r.squared ### Now declare the differeent inputs for DeclareDesign popsmalldat1 &lt;- declare_population(smalldat1) assignsmalldat1 &lt;- declare_assignment(Znew=complete_ra(N,m=10)) assignbigdat1 &lt;- declare_assignment(Znew=complete_ra(N,m=50)) ## No additional treatment effects other than those created when we made y0 and y1 earlier po_functionNull &lt;- function(data){ data$Y_Znew_0 &lt;- data$y0 data$Y_Znew_1 &lt;- data$y1 data } ysdat1 &lt;- declare_potential_outcomes(handler = po_functionNull) theestimanddat1 &lt;- declare_inquiry(ATE = mean(Y_Znew_1 - Y_Znew_0)) theobsidentdat1 &lt;- declare_reveal(Y, Znew) thedesignsmalldat1 &lt;- popsmalldat1 + assignsmalldat1 + ysdat1 + theestimanddat1 + theobsidentdat1 thedesignbigdat1 &lt;- popbigdat1 + assignbigdat1 + ysdat1 + theestimanddat1 + theobsidentdat1 estCov0 &lt;- declare_estimator(Y~Znew, inquiry=theestimanddat1, model=lm_robust, label=&quot;CovAdj0: Lm, No covariates&quot;) estCov1 &lt;- declare_estimator(Y~Znew+cov2, inquiry=theestimanddat1, model=lm_robust, label=&quot;CovAdj1: Lm,Correct Covariate&quot;) estCov2 &lt;- declare_estimator(Y~Znew+cov1+cov2+cov3+cov4+cov5+cov6+cov7+cov8, inquiry=theestimanddat1, model=lm_robust, label=&quot;CovAdj2: Lm, Mixed Covariates&quot;) estCov3 &lt;- declare_estimator(Y~Znew+cov1+cov3+cov4+cov5+cov6, inquiry=theestimanddat1, model=lm_robust, label=&quot;CovAdj3: Lm, Wrong Covariates&quot;) estCov4 &lt;- declare_estimator(Y~Znew,covariates=~cov1+cov2+cov3+cov4+cov5+cov6+cov7+cov8, inquiry=theestimanddat1, model=lm_lin, label=&quot;CovAdj4: Lin, Mixed Covariates&quot;) estCov5 &lt;- declare_estimator(Y~Znew,covariates=~cov2, inquiry=theestimanddat1, model=lm_lin, label=&quot;CovAdj5: Lin, Correct Covariate&quot;) thedesignsmalldat1PlusEstimators &lt;- thedesignsmalldat1 + estCov0 + estCov1 + estCov2 + estCov3 + estCov4 + estCov5 thedesignbigdat1PlusEstimators &lt;- thedesignbigdat1 + estCov0 + estCov1 + estCov2 + estCov3 + estCov4 + estCov5 sims &lt;- 200 set.seed(12345) thediagnosisCovAdj1 &lt;- diagnose_design(thedesignsmalldat1PlusEstimators, sims = sims, bootstrap_sims = 0) thediagnosisCovAdj1 set.seed(12345) thediagnosisCovAdj2 &lt;- diagnose_design(thedesignbigdat1PlusEstimators, sims = sims, bootstrap_sims = 0) thediagnosisCovAdj2 After 1000 simulations using the small experiment (N=20) we can see that the &#x201C;CovAdj1: Lm, Correct Covariate&#x201D; lines show fairly large bias compared to the estimator using no covariates at all. The Lin approach using only the known to be correct covariate reduces the bias, but does not erase it. However, the unadjusted estimator has fairly low power where as the Lin approach with the correct covariate &#x201C;CovAdj5: Lin, Correct Covariate&#x201D; has excellent power to detect the 1 SD effect built into this experiment. One interesting result here is that the Lin approach is worst (in power and even false positive rate (called &#x201C;Coverage&#x201D; below) when a mixture or correct and incorrect covariates are added to the linear model following the interaction-and-centering based approach. diagcols &lt;- c(3,5,6,7,8,9,10,11) ## See https://haozhu233.github.io/kableExtra/awesome_table_in_html.html kable(reshape_diagnosis(thediagnosisCovAdj1)[,diagcols] ) # %&gt;% kable_styling() %&gt;% scroll_box(width = &quot;100%&quot;, height = &quot;400px&quot;) Estimator Term Mean Estimand Mean Estimate Bias SD Estimate RMSE Power CovAdj0: Lm, No covariates Znew 5.03 5.08 0.05 1.78 1.78 0.65 CovAdj1: Lm,Correct Covariate Znew 5.03 5.32 0.29 1.08 1.11 0.98 CovAdj2: Lm, Mixed Covariates Znew 5.03 5.12 0.09 1.39 1.39 0.89 CovAdj3: Lm, Wrong Covariates Znew 5.03 5.21 0.18 1.64 1.65 0.78 CovAdj4: Lin, Mixed Covariates Znew 5.03 4.58 -0.45 8.07 8.06 0.35 CovAdj5: Lin, Correct Covariate Znew 5.03 5.13 0.10 1.05 1.05 1.00 The experiment with N=100N=100N=100 shows much smaller bias than the small experiment above. Since all estimators allow us to detect the 1 SD effect (Power=1), the RMSE (Root Mean Squared Error) column or &#x201C;Mean Se&#x201D; columns tell us about the precision of the estimators. Again, the unadjusted approach has low bias, but has the largest standard error. While the Lin approach with a mixture of correct and incorrect covariates has low bias, it shows slightly worse coverage (or false positive error rate) even it has most precision. ## See https://haozhu233.github.io/kableExtra/awesome_table_in_html.html kable(reshape_diagnosis(thediagnosisCovAdj2)[,diagcols] ) # %&gt;% kable_styling() %&gt;% scroll_box(width = &quot;100%&quot;, height = &quot;400px&quot;) Estimator Term Mean Estimand Mean Estimate Bias SD Estimate RMSE Power CovAdj0: Lm, No covariates Znew 5.45 5.45 -0.01 0.76 0.76 1.00 CovAdj1: Lm,Correct Covariate Znew 5.45 5.47 0.02 0.50 0.50 1.00 CovAdj2: Lm, Mixed Covariates Znew 5.45 5.48 0.03 0.53 0.53 1.00 CovAdj3: Lm, Wrong Covariates Znew 5.45 5.44 -0.01 0.63 0.63 1.00 CovAdj4: Lin, Mixed Covariates Znew 5.45 5.47 0.02 0.53 0.53 1.00 CovAdj5: Lin, Correct Covariate Znew 5.45 5.47 0.01 0.50 0.50 1.00 simdesignsCovAdj1 &lt;- get_simulations(thediagnosisCovAdj1) trueATE1covadj &lt;- with(dat1,mean(y1-y0)) ## simdesigns &lt;- simulate_design(thedesign,sims=sims) simmeansCovAdj1 &lt;- simdesignsCovAdj1 %&gt;% group_by(estimator) %&gt;% summarize(expest=mean(estimate)) Although the Lin approach works well when covariates are few and sample sizes are large, these simulations show where the approach is weak: when covariates are many. In this case the estimator involving both correct and irrelevant covariates used 18 terms. Fitting a model with 18 terms with N=20 allows nearly any observation to exert undue influence, increases the risk of serious multicollinearity, and leads to overfitting problems in general. So far, our team has not run into this problem because our studies have tended to involve many thousands of units and relatively few covariates. However, we are considering a few alternative approaches should we confront this situation in the future such as (1) collapsing the covariates into fewer dimensions (using a Mahalanobis distance or principal components based distance) or working with a residualized version of the outcome as described below. 5.2.3 The Rosenbaum Approach The Covariance Adjustment When we have many covariates, sometimes the Lin style approach prevents us from calculating appropriate standard errors and/or can have inflated bias due to overfitting. Rosenbaum (2002) showed an approach in which the outcomes are regressed on covariates, ignoring treatment assignment, and then the residuals from that regression are used to estimate an average treatment effect. We do a similar evaluation of that approach here. make_est_fun&lt;-function(covs){ ## covs is a vector of character names of covariates force(covs) covfmla &lt;- reformulate(covs,response=&quot;Y&quot;) function(data){ data$e_y &lt;- residuals(lm(covfmla,data=data)) obj &lt;-lm_robust(e_y~Znew,data=data) res &lt;- tidy(obj) %&gt;% filter(term==&quot;Znew&quot;) return(res) } } est_fun_correct &lt;- make_est_fun(&quot;cov2&quot;) est_fun_mixed&lt;- make_est_fun(c(&quot;cov1&quot;,&quot;cov2&quot;,&quot;cov3&quot;,&quot;cov4&quot;,&quot;cov5&quot;,&quot;cov6&quot;,&quot;cov7&quot;,&quot;cov8&quot;)) est_fun_incorrect &lt;- make_est_fun(c(&quot;cov1&quot;,&quot;cov2&quot;,&quot;cov3&quot;,&quot;cov4&quot;,&quot;cov5&quot;,&quot;cov6&quot;)) ## est_fun_correct(blah) estCov6 &lt;- declare_estimator(handler = tidy_estimator(est_fun_correct), inquiry=theestimanddat1, label=&quot;CovAdj6: Resid, Correct&quot;) estCov7 &lt;- declare_estimator(handler = tidy_estimator(est_fun_mixed), inquiry=theestimanddat1, label=&quot;CovAdj7: Resid, Mixed&quot;) estCov8 &lt;- declare_estimator(handler = tidy_estimator(est_fun_incorrect), inquiry=theestimanddat1, label=&quot;CovAdj8: Resid, InCorrect&quot;) thedesignsmalldat1PlusRoseEstimators &lt;- thedesignsmalldat1 + estCov0 + estCov1 + estCov2 + estCov3 + estCov4 + estCov5 + estCov6 + estCov7 + estCov8 thedesignbigdat1PlusRoseEstimators &lt;- thedesignbigdat1 + estCov0 + estCov1 + estCov2 + estCov3 + estCov4 + estCov5 + estCov6 + estCov7 + estCov8 set.seed(12345) thediagnosisCovAdj3 &lt;- diagnose_design(thedesignsmalldat1PlusRoseEstimators, sims = sims, bootstrap_sims = 0) thediagnosisCovAdj3 set.seed(12345) thediagnosisCovAdj4 &lt;- diagnose_design(thedesignbigdat1PlusRoseEstimators, sims = sims, bootstrap_sims = 0) thediagnosisCovAdj3 With a small sample (N=20), the Rosenbaum-style approach yields very little bias and quite high power using the correct covariate (&#x201C;CovAdj6: Resid, Correct&#x201D;), but poor performance in terms of bias and coverage with incorrect covariates &#x2014; recall that coverage here uses the t-test that in turn relies on asymptotic approximations, and we are challenging this approximation with a small experiment and overfitting problems. ## See https://haozhu233.github.io/kableExtra/awesome_table_in_html.html kable(reshape_diagnosis(thediagnosisCovAdj3)[7:9,diagcols] ) #%&gt;% kable_styling() %&gt;% scroll_box(width = &quot;100%&quot;, height = &quot;400px&quot;) Estimator Term Mean Estimand Mean Estimate Bias SD Estimate RMSE Power 7 CovAdj6: Resid, Correct Znew 5.03 5.00 -0.03 1.02 1.02 1.00 8 CovAdj7: Resid, Mixed Znew 5.03 2.95 -2.08 1.11 2.35 0.80 9 CovAdj8: Resid, InCorrect Znew 5.03 3.53 -1.50 1.20 1.92 0.85 With a larger experiment, the bias goes down, but coverage is poor with incorrect covariates in this approach as well. We speculate that performance might improve if we fit the covariance adjustment models that produce residuals separately for the treated and control groups. ## See https://haozhu233.github.io/kableExtra/awesome_table_in_html.html kable(reshape_diagnosis(thediagnosisCovAdj4)[7:9,diagcols] ) # %&gt;% kable_styling() %&gt;% scroll_box(width = &quot;100%&quot;, height = &quot;400px&quot;) Estimator Term Mean Estimand Mean Estimate Bias SD Estimate RMSE Power 7 CovAdj6: Resid, Correct Znew 5.45 5.42 -0.03 0.49 0.49 1.00 8 CovAdj7: Resid, Mixed Znew 5.45 5.05 -0.40 0.53 0.67 1.00 9 CovAdj8: Resid, InCorrect Znew 5.45 5.14 -0.31 0.52 0.61 1.00 5.3 How to choose covariates for covariance adjustment? Our analysis plans commonly specify a few covariates based on what we know about the mechanisms and context of the study. In general, if we have a measurement of the outcome before the treatment was assigned, the baseline outcome, we try to use it via blocking and/or via covariance adjustment. When we have access to many covariates, we sometimes use simple machine learning methods to select variables that strongly predict the outcome. 5.3.0.1 Example of using the adaptive lasso for variable selection Here we show how we use baseline data, data collected before the treatment was assigned or new policy implemented, to choose covariates that we then use as we have described above. We tend to use the adaptive lasso rather than the simple lasso because the adaptive lasso has better theoretical properties (insert citation to Zhou) but also because the adaptive lasso tends to produce sparser results &#x2014; and the bias from covariance adjustment can be severe if we add many many covariates to a covriance adjustment procedure. TO DO 5.4 Block-randomized trials We design block-randomized trials by splitting units into groups based on predefined characteristics &#x2014; covariates that cannot be changed by the experimental treatment &#x2014; and then randomly assigning treatment within each group. We use this procedure when we want to increase our ability to detect signal from noise and we think that the noise, or variation in the outcome of the experiment, is driven in part by the covariates that we use for blocking. For example, if we imagine the patterns of energy use will tend to differ according to size of family, we may create blocks or strata of different family sizes and randomly assign an energy saving intervention separately within those blocks. We also design block-randomized experiments when we want to assess effects within and across subgroups (for example, if we want to ensure that we have enough statistical power to detect a difference in effects between veterans and non-veterans). If we have complete random assignment, it is likely that the proportion of veterans assigned treatment will not be exactly same as the proportion of non-veterans receiving treatment. However, if we stratify or block the group on military status, and randomly assign treatment and control within each group, we can then ensure that equal proportions (or numbers) or veterans and non-veterans receive the treatment and control. Most of the general ideas that we demonstrated in the context of completely randomized trials have direct analogues in the case of block randomized trials. The only additional question that arises with block randomized trials is about how to weight the contributions of each individual block when calculating an overall average treatment effect or testing an overall hypothesis about treatment effects. We begin here with the simple case of testing the sharp null of no effects when we have a binary outcome &#x2014; in the case of the Cochran-Mantel-Haenszel test the weighting of the different blocks is automatic. 5.4.1 Testing the null of no effects with binary outcomes and block randomization: Cochran-Mantel-Haenszel (CMH) test for K X 2 X 2 tables We use the CMH test as a test of no effect for block-randomized trials with binary outcome.6 Because the blocks or strata are important to the experiment and outcomes, we want to keep the outcomes for each strata intact rather than pooling the outcomes together. Since we repeat the same experiment across each stratum, the CMH test tells us if the odds ratio in the experiments indicate that there is an association between outcomes and treatment/control across strata (Cochran 1954; Mantel and Haenszel 1959). To set up the CMH test, we need k sets of 2x2 contingency tables. Suppose the table below represents outcomes from stratum i where A,B,C, and D are counts of observations: Assignment Response No response Total Treatment A B A+B Control C D C+D Total A+C B+D A+B+C+D = T The CMH test statistic compares the sum of squared deviation between observed and expected outcomes of an experiment within one stratum to the variance of those outcomes, conditional on marginal totals. CMH=&#x2211;i=1k(Ai&#x2212;E[Ai])&#x2211;VAR[Ai]CMH = \\frac{\\sum_{i=1}^{k} (A_{i} - \\mathrm{E}[{A_{i}}])}{\\sum{\\mathrm{VAR}[{A_i}]}}CMH=&#x2211;VAR[Ai&#x200B;]&#x2211;i=1k&#x200B;(Ai&#x200B;&#x2212;E[Ai&#x200B;])&#x200B; where E[Ai]=(Ai+Bi)(Ai+Ci)Ti\\mathrm{E}[A_{i}] = \\frac{(A_i+B_i)(A_i+C_i)}{T_i}E[Ai&#x200B;]=Ti&#x200B;(Ai&#x200B;+Bi&#x200B;)(Ai&#x200B;+Ci&#x200B;)&#x200B; and VAR[Ai]=(Ai+Bi)(Ai+Ci)(Bi+Di)(Ci+Di)Ti2(Ti&#x2212;1)\\mathrm{VAR}[A_{i}] = \\frac{(A_i+B_i)(A_i+C_i)(B_i+D_i)(C_i+D_i)}{{T_i}^2(T_i-1)}VAR[Ai&#x200B;]=Ti&#x200B;2(Ti&#x200B;&#x2212;1)(Ai&#x200B;+Bi&#x200B;)(Ai&#x200B;+Ci&#x200B;)(Bi&#x200B;+Di&#x200B;)(Ci&#x200B;+Di&#x200B;)&#x200B; In large enough samples, if there are no associations between Treatment and Reponse across strata, we would expect to see an odds ratio which is equal to 1, and, across randomizations and in large samples, this test statistic would have an asymptotic &#x3C7;2\\chi^2&#x3C7;2 distribution with degrees of freedom = 1. The odds ratio in this scenario is the combined weighted odds ratio of each two-armed trial with binary outcomes within one block or stratum. The odds ratio for a given stratum is OR=ABCD=ADBCOR = \\frac{\\frac{A}{B}}{\\frac{C}{D}} = \\frac{AD}{BC}OR=DC&#x200B;BA&#x200B;&#x200B;=BCAD&#x200B; With many strata, we can find a common odds ratio That is, we add the odds ratios of each stratum and weigh it by the total in that stratum. If the odds ratio is greater than 1 then we suspect that there may be an association between the outcome and treatment across all strata and the CMH test statistic will be large. If ORCMH=1OR_{CMH} = 1ORCMH&#x200B;=1, then this supports the null hypothesis that there is no association between treatment and outcome and the CMH test statistic will be small. We can also use the CMH test to compare odds ratios between experiments, rather than compare against the null that the odds ratio = 1. 5.4.2 Estimating an overall Average Treatment Effect Our team nearly always reports a single estimate of the average treatment effect whether or not we randomly assign a policy intervention within blocks or strata. We randomly assign the intervention within each block independently and we tend to define our overall ATE (the estimand) as a simple average of the individual additive treatment effects (for two treatments, remember that we tend to write this unobserved causal effect as &#x3C4;i=yi,0&#x2212;yi,1\\tau_i = y_{i,0} - y_{i,1}&#x3C4;i&#x200B;=yi,0&#x200B;&#x2212;yi,1&#x200B;). So we tend to define the overall ATE as &#x3C4;&#x2C9;=(1/n)&#x2211;i=1n&#x3C4;i\\bar{\\tau}=(1/n) \\sum_{i=1}^n \\tau_i&#x3C4;&#x2C9;=(1/n)&#x2211;i=1n&#x200B;&#x3C4;i&#x200B;. Now, we have randomly assigned within blocks in order to (1) increase precision and (2) enable subgroup analysis. How can we &#x201C;analyze as we have randomized&#x201D; if we want to learn about &#x3C4;&#x2C9;\\bar{\\tau}&#x3C4;&#x2C9; using what we observe? Our approach is to build up from the block-level (see Gerber and Green (2012) for more on this approach). Say, for example, we imagine that the unobserved ATE within a given block, bbb, was ATEb=&#x3C4;&#x2C9;b=(1/nb)&#x2211;i=1nb&#x3C4;i\\text{ATE}_{b}=\\bar{\\tau}_b=(1/n_b)\\sum_{i=1}^{n_b} \\tau_{i}ATEb&#x200B;=&#x3C4;&#x2C9;b&#x200B;=(1/nb&#x200B;)&#x2211;i=1nb&#x200B;&#x200B;&#x3C4;i&#x200B; where we are averaging the individual level treatment effects (&#x3C4;i\\tau_{i}&#x3C4;i&#x200B;) across all nbn_bnb&#x200B; people in block bbb. And now, imagine that we had an experiment with blocks of different sizes (and perhaps with different proportions assigned to treatment within block &#x2014; perhaps certain blocks are more expensive places in which to run an experiment). We could learn about &#x3C4;&#x2C9;\\bar{\\tau}&#x3C4;&#x2C9; with a block-size weighted average of the &#x3C4;&#x2C9;b\\bar{\\tau}_b&#x3C4;&#x2C9;b&#x200B; such that &#x3C4;&#x2C9;nbwt=(1/B)&#x2211;b=1B(nb/n)&#x3C4;&#x2C9;b\\bar{\\tau}_{\\text{nbwt}}= (1/B) \\sum_{b=1}^B (n_b/n) \\bar{\\tau}_b&#x3C4;&#x2C9;nbwt&#x200B;=(1/B)&#x2211;b=1B&#x200B;(nb&#x200B;/n)&#x3C4;&#x2C9;b&#x200B;. We can estimate &#x3C4;&#x2C9;nbwt\\bar{\\tau}_{\\text{nbwt}}&#x3C4;&#x2C9;nbwt&#x200B; with the observed analogue just as we have with the completely randomized experiment (after all, each block is a small completely randomized experiment in this example, and so we can estimate &#x3C4;&#x2C9;b\\bar{\\tau}_b&#x3C4;&#x2C9;b&#x200B; using the unbiased estimator where i&#x2208;ti \\in ti&#x2208;t means &#x201C;for iii in the treatment group&#x201D;, &#x3C4;^b=&#x2211;i&#x2208;tYib/mb&#x2212;&#x2211;i&#x2208;cYib/(nb&#x2212;mb)\\hat{\\tau}_b=\\sum_{i \\in t} Y_{ib}/m_b - \\sum_{i \\in c} Y_{ib}/(n_b - m_b)&#x3C4;^b&#x200B;=&#x2211;i&#x2208;t&#x200B;Yib&#x200B;/mb&#x200B;&#x2212;&#x2211;i&#x2208;c&#x200B;Yib&#x200B;/(nb&#x200B;&#x2212;mb&#x200B;) where mbm_bmb&#x200B; is the number of units assigned to treatment in block bbb. Note that many people do not use this unbiased estimator because the precision of tests based on this estimator are worse that those of another estimator that is slightly biased. We will demonstrate both methods &#x2014; the block-size weighted estimator and what we call the precision-weighted estimator &#x2014; here and offer some reflections on when a biased estimator that tends to produce answers closer to the truth might be preferred over an unbiased estimator where any given estimate may be farther from the truth. This estimator uses harmonic-weights. We have tended to call it a &#x201C;precision-weighted average&#x201D; and the weights on the blocks combine both the block size nbn_bnb&#x200B; and the proportion of the block assigned to treatment pb=(1/nb)&#x2211;i=1nbZibp_b = (1/n_b) \\sum_{i=1}^{n_b} Z_{ib}pb&#x200B;=(1/nb&#x200B;)&#x2211;i=1nb&#x200B;&#x200B;Zib&#x200B; for a binary treatment, ZibZ_{ib}Zib&#x200B; so that the weight is hb=nbpb(1&#x2212;pb)h_b = n_b p_b (1 - p_b)hb&#x200B;=nb&#x200B;pb&#x200B;(1&#x2212;pb&#x200B;) and the estimator is &#x3C4;&#x2C9;hbwt=(1/B)&#x2211;b=1B(1/hb)&#x3C4;&#x2C9;b\\bar{\\tau}_{\\text{hbwt}}= (1/B) \\sum_{b=1}^B (1/h_b) \\bar{\\tau}_b&#x3C4;&#x2C9;hbwt&#x200B;=(1/B)&#x2211;b=1B&#x200B;(1/hb&#x200B;)&#x3C4;&#x2C9;b&#x200B;. First we show multiple approaches to producing estimates using these estimators. And then we demonstrate how (1) ignoring the blocks when estimating can produce problems in both estimation and testing, (2) how the block-size weighted approaches are unbiased but possibly less precise than the precision weighted approaches. ## Create block sizes and create block weights B &lt;- 10 # Number of blocks dat &lt;- data.frame(b=rep(1:B,c(8,20,30,40,50,60,70,80,100,800))) dat$bF &lt;- factor(dat$b) set.seed(2201) ## x1 is a covariate that strongly predicts the outcome without treatment dat &lt;- group_by(dat,b) %&gt;% mutate(nb=n(), x1=rpois(n = nb,lambda=runif(1,min=1,max=2000))) ## The treatment effect varies by size of block (using sqrt(nb) because nb has such a large range.) dat &lt;- group_by(dat,b) %&gt;% mutate(y0=sd(x1)*x1+rchisq(n=nb,df=1), y0=y0*(y0&gt;quantile(y0,.05)), tauib = -(sd(y0))*sqrt(nb) + rnorm(n(),mean=0,sd=sd(y0)), y1=y0+tauib, y1=y1*(y1&gt;0)) blockpredpower &lt;- summary(lm(y0~bF,data=dat))$r.squared To make the differences between the approaches to estimation most vivid, we create a dataset with blocks of widely varying sizes, half of the blocks have half of the units assigned to treatment and the other half 10% of the units assigned to treatment. The baseline outcomes are strongly predicted by the blocks (R2R^2R2 around 0.870.870.87). We will use the DeclareDesign approach to assess bias, coverage and power (or precision) of the different estimators here. The next code block sets up the simulation and also demonstrates different approaches to calculating the same numbers to represent the true underlying ATE (we can only do this because we are using simulation here to learn about different statistical techniques.) ## Using the Declare Design Machinery to ensure that the data here and the ## simulations below match ## Setting up Declare Design: thepop &lt;- declare_population(dat) po_function &lt;- function(data){ data$Y_Z_0 &lt;- data$y0 data$Y_Z_1 &lt;- data$y1 data } theys &lt;- declare_potential_outcomes(handler = po_function) theestimand &lt;- declare_inquiry(ATE = mean(Y_Z_1 - Y_Z_0)) numtreated &lt;- sort(unique(dat$nb))/rep(c(2,10),B/2) theassign &lt;- declare_assignment(Z=block_ra(blocks = bF, block_m=numtreated)) theobsident &lt;- declare_reveal(Y, Z) thedesign &lt;- thepop + theys + theestimand + theassign + theobsident set.seed(2201) dat2 &lt;- draw_data(thedesign) ## Adding rank transformed outcomes for use later. dat2 &lt;- dat2 %&gt;% group_by(b) %&gt;% mutate(y0md = y0 - mean(y0), y1md = y1 - mean(y1), alignedY = Y - mean(Y), rankalignedY = rank(alignedY) ) ## Now add individual level weights to the data. Different textbooks and algebra yield different expressions. We show that they are all the same. dat2 &lt;- dat2 %&gt;% group_by(b) %&gt;% mutate(nb = n(), ## Size of block pib=mean(Z), ## prob of treatment assignment nTb=sum(Z), ## Number treated nCb=nb - nTb, ## Number control nbwt = ( Z/pib ) + ( (1-Z)/(1-pib) ), nbwt2 = nb/nrow(dat2), #hbwt = 2 * (nCb * nTb ) / (nTb + nCb), ## Precision weight/Harmonic #hbwt2 = 2 * ( nbwt2 )*(pib*(1-pib)), hbwt3 = nbwt * ( pib * (1 - pib) ) ) dat2$nbwt3 &lt;- dat2$nbwt2/dat2$nb thepop2 &lt;- declare_population(dat2) thedesign2 &lt;- thepop2 + theys + theestimand + theassign + theobsident ## And create the block level dataset, with block level weights. datB &lt;- group_by(dat2,b) %&gt;% summarize(taub = mean(Y[Z==1]) - mean(Y[Z==0]), truetaub = mean(y1) - mean(y0), nb = n(), nTb = sum(Z), nCb = nb - nTb, estvartaub = (nb/(nb-1)) * ( var(Y[Z==1]) / nTb ) + ( var(Y[Z==0])/nCb ) , pb=mean(Z), nbwt = unique(nb/nrow(dat2)), pbwt = pb * ( 1 - pb), hbwt2 = nbwt * pbwt, hbwt5 = pbwt * nb, hbwt= ( 2*( nCb * nTb ) / (nTb + nCb))) datB$greenlabrule &lt;- 20*datB$hbwt5/sum(datB$hbwt5) ## Notice that all of these different ways to express the harmonic mean weight are the same. datB$hbwt01 &lt;- datB$hbwt/sum(datB$hbwt) datB$hbwt201 &lt;- datB$hbwt2/sum(datB$hbwt2) datB$hbwt501 &lt;- datB$hbwt5/sum(datB$hbwt5) stopifnot(all.equal(datB$hbwt01,datB$hbwt201)) stopifnot(all.equal(datB$hbwt01,datB$hbwt501)) ## What is the &quot;true&quot; ATE? trueATE1 &lt;- with(dat2,mean(y1) - mean(y0)) trueATE2 &lt;- with(datB, sum(truetaub*nbwt)) stopifnot(all.equal(trueATE1,trueATE2)) ## We could define the following as an estimand, too. But it is a bit weird. ## trueATE3 &lt;- with(datB, sum(truetaub*hbwt01)) ## c(trueATE1,trueATE2,trueATE3) ## We can get the same answer using R&apos;s weighted.mean command trueATE2b &lt;- weighted.mean(datB$truetaub,w=datB$nbwt) stopifnot(all.equal(trueATE2b,trueATE2)) Here we can see the design: with(dat2,table(treatment=Z,blocknumber=b)) blocknumber treatment 1 2 3 4 5 6 7 8 9 10 0 4 18 15 36 25 54 35 72 50 720 1 4 2 15 4 25 6 35 8 50 80 Now, we will show multiple ways to get the same answer and later show evidence about bias and precision. Notice that we do not use fixed effects on their own in any of these approaches. There are two approaches that do use fixed effects/indicator variables but they only include them in interaction with the treatment assignment. Below we will show that all of these approaches are unbiased estimators of &#x3C4;&#x2C9;nbwt\\bar{\\tau}_{\\text{nbwt}}&#x3C4;&#x2C9;nbwt&#x200B; (the ATE treating all individuals equally although randomizing within block). For example, below we see 6 different ways to estimate the average treatment effect using block-size weights: simple_block refers to calculating meaen differences within blocks and then taking the block-size weighted averagee of them; diffmeans uses the difference_in_means function from the estimatr package (Blair et al. 2022); lmlin uses the Lin approach to covariance adjustment but uses block indicators instead of other covariates using the lm_lin function; lmlinbyhand verifies that function using matrix operations; intereactionBFE uses the EstimateIWE function from the bfe package (Gibbons 2018); and regwts uses the basic OLS function from R lm with appropriate weights. ### Block size weighting ate_nbwt1 &lt;- with(datB,sum(taub*nbwt)) ate_nbwt2 &lt;- difference_in_means(Y~Z,blocks = b,data=dat2) ate_nbwt3 &lt;- lm_lin(Y~Z,covariates=~bF,data=dat2) ate_nbwt5 &lt;- EstimateIWE(y=&quot;Y&quot;,treatment=&quot;Z&quot;,group=&quot;bF&quot;,controls=NULL,data=as.data.frame(dat2)) ate_nbwt6 &lt;- lm_robust(Y~Z,data=dat2,weights=nbwt) ate_nbwt6a &lt;- lm(Y~Z,data=dat2,weights=nbwt) ate_nbwt6ase &lt;- coeftest(ate_nbwt6a,vcov=vcovHC(ate_nbwt6a,type=&quot;HC2&quot;)) ## This next implements the lm_lin method from Lin 2013 by hand: ## Implementing Lin&apos;s method from https://alexandercoppock.com/Green-Lab-SOP/Green_Lab_SOP.html#taking-block-randomization-into-account-in-ses-and-cis. X &lt;- model.matrix(~bF-1,data=dat2) barX &lt;- colMeans(X) Xmd &lt;- sweep(X,2,barX) stopifnot( all.equal( (X[,3]-mean(X[,3])), Xmd[,3] ) ) ZXmd &lt;- sweep(Xmd,1,dat2$Z,FUN=&quot;*&quot;) stopifnot( all.equal( dat2$Z*Xmd[,3],ZXmd[,3] )) bigX &lt;- cbind(Intercept=1,Z=dat2$Z,Xmd[,-1],ZXmd[,-1]) #ate_nbwt4 &lt;- lm.fit(x=bigX,y=dat$Y) bigXdf &lt;- data.frame(bigX,Y=dat2$Y) ate_nbwt4 &lt;-lm(Y~.-1,data=bigXdf) ate_nbwt4se &lt;- coeftest(ate_nbwt4,vcov.=vcovHC(ate_nbwt4,type=&quot;HC2&quot;)) nbwtates&lt;-c(simple_block=ate_nbwt1, diffmeans=ate_nbwt2$coefficients[[&quot;Z&quot;]], lmlin=ate_nbwt3$coefficients[[&quot;Z&quot;]], lmlinbyhand=ate_nbwt4$coefficients[[&quot;Z&quot;]], interactionBFE=ate_nbwt5$swe.est, regwts=ate_nbwt6$coefficients[[&quot;Z&quot;]] ) nbwtates simple_block diffmeans lmlin lmlinbyhand interactionBFE regwts -12823 -12823 -12823 -12823 -12823 -12823 ## Comparing the Standard Errors ## ate_nbwt1se &lt;- sqrt(sum(datB$nbwt^2 * datB$estvartaub)) ## ## nbwtses &lt;- c(simple_block=ate_nbwt1se, ## diffmeans=ate_nbwt2$std.error, ## lmlin=ate_nbwt3$std.error[[&quot;Z&quot;]], ## interaction1=ate_nbwt4se[&quot;Z&quot;,&quot;Std. Error&quot;], ## interaction2=ate_nbwt5$swe.var^.5, ## wts=ate_nbwt6$std.error[[&quot;Z&quot;]]) ## nbwtses Weighting by block size allows us to define the average treatment effect in a way that treats each unit equally. And we have shown six different ways to estimate this effect. If we want to calculate standard errors for these estimators, so as to produce confidence intervals, we will, in general, be leaving statistical power on the table in exchange for an easier to interpret estimate, an estimator that relates to its underlying target in an unbiased fashion. Below we show an approach which is optimal from the perspective of statistical power, precision, or narrow confidence intervals which we call &#x201C;precision weighted&#x201D; average treatment effects.7 In some literatures using the least squares machinery to calculate the weighted means this approach is called Least Squared Dummy Variables, or &#x201C;fixed effects&#x201D;. However, we show below that these approaches are all versions of a weighted least squares estimator. Below we see that we can estimate the precision-weighted ATE in five different ways: simple_block calculates simple differences of means within blocks and then takes a weighted average of those differences, using the precision weights; lm_fixed_effects1 uses lm_robust with indicators for block; lm_fixed_effects2 uses lm_robust with the fixed_effects option including a factor variable recording block membership; direct_wts uses lm_robust without block-indicators but with precision weights; and demeaned regresses a block-centered version of the outcome on a block-centered version of the treatment indicator. ate_hbwt1 &lt;- with(datB, sum(taub*hbwt01)) ate_hbwt2 &lt;- lm_robust(Y~Z+bF,data=dat2) ate_hbwt3 &lt;- lm_robust(Y~Z,fixed_effects=~bF,data=dat2) ate_hbwt4 &lt;- lm_robust(Y~Z,data=dat2,weights=hbwt3) ate_hbwt5 &lt;- lm_robust(I(Y-ave(Y,b))~I(Z-ave(Z,b)),data=dat2) hbwtates&lt;-c(simple_block=ate_hbwt1, lm_fixed_effects1=ate_hbwt2$coefficients[[&quot;Z&quot;]], lm_fixed_effects2=ate_hbwt3$coefficients[[&quot;Z&quot;]], direct_wts=ate_hbwt4$coefficients[[&quot;Z&quot;]], demeaned=ate_hbwt5$coefficient[[2]]) hbwtates simple_block lm_fixed_effects1 lm_fixed_effects2 direct_wts demeaned -13981 -13981 -13981 -13981 -13981 ## ate_hbwt1se &lt;- sqrt(sum(datB$hbwt01^2 * datB$estvartaub)) ## ## hbwtses &lt;- c(simple_block=ate_hbwt1se, ## diffmeans=ate_hbwt2$std.error[[&quot;Z&quot;]], ## lmfe=ate_hbwt3$std.error[[&quot;Z&quot;]], ## wts=ate_hbwt4$std.error[[&quot;Z&quot;]], ## demean=ate_hbwt5$std.error[[2]]) ## hbwtses ## ## nbwtses Now, we claimed that the block size weighted estimator is unbiased but perhaps less precise than the precision-weighted estimator. We use DeclareDesign to to compare the performance of these estimators. We focus here on the use of least squares to calculate the weighted averages and standard errors but, as we showed above, one could calculate the estimates easily without using least squares. We implement those estimators as functions usable by the diagnose_design function in the next code block. # Define estimators that can be repeated in the simulation below estnowtHC2 &lt;- declare_estimator(Y~Z, inquiry=theestimand, model=lm_robust, label=&quot;E1: Ignores Blocks, Design SE&quot;) estnowtIID &lt;- declare_estimator(Y~Z, inquiry=theestimand, model=lm, label=&quot;E0: Ignores Blocks, IID SE&quot;) estnbwt1 &lt;- declare_estimator(Y~Z, inquiry=theestimand, model=difference_in_means, blocks = b,label=&quot;E2: Diff Means Block Size Weights, Design SE&quot;) estnbwt2 &lt;- declare_estimator(Y~Z, inquiry=theestimand, model=lm_lin, covariates=~bF, label=&quot;E3: Treatment Interaction with Block Indicators, Design SE&quot;) iwe_est_fun &lt;- function(data) { obj &lt;- EstimateIWE(y=&quot;Y&quot;, treatment=&quot;Z&quot;, group=&quot;bF&quot;, controls = NULL, data = data) res &lt;- summary.iwe(obj)[&quot;SWE&quot;,] res$term &lt;- &quot;Z&quot; return(res) } estnbwt3 &lt;- declare_estimator(handler = tidy_estimator(iwe_est_fun), inquiry = theestimand, label = &quot;E4: Treatment Interaction with Block Indicators, Design SE&quot;) nbwt_est_fun &lt;- function(data){ data$newnbwt &lt;- with(data,( Z/pib ) + ( (1-Z)/(1-pib) ) ) obj &lt;-lm_robust(Y~Z,data=data,weights = newnbwt) res &lt;- tidy(obj) %&gt;% filter(term==&quot;Z&quot;) return(res) } hbwt_est_fun &lt;- function(data){ data$newnbwt &lt;- with(data,( Z/pib ) + ( (1-Z)/(1-pib) ) ) data$newhbwt &lt;- with(data, newnbwt * ( pib * (1 - pib) ) ) obj &lt;-lm_robust(Y~Z,data=data,weights = newhbwt) res &lt;- tidy(obj) %&gt;% filter(term==&quot;Z&quot;) return(res) } estnbwt4 &lt;- declare_estimator(handler = tidy_estimator(nbwt_est_fun), inquiry=theestimand, label=&quot;E5: Least Squares with Block Size Weights, Design SE&quot;) esthbwt1 &lt;- declare_estimator(Y~Z+bF, inquiry=theestimand, model=lm_robust,label=&quot;E6: Precision Weights via Fixed Effects, Design SE&quot;) esthbwt2 &lt;- declare_estimator(Y~Z, inquiry=theestimand, model=lm_robust,fixed_effects=~bF,label=&quot;E7: Precision Weights via Demeaning, Design SE&quot;) esthbwt3 &lt;- declare_estimator(handler = tidy_estimator(hbwt_est_fun), inquiry=theestimand,label=&quot;E8: Direct Precision Weights, Design SE&quot;) direct_demean_fun &lt;- function(data){ data$Y &lt;- with(data, Y - ave(Y,b)) data$Z &lt;- with(data, Z - ave(Z,b)) obj &lt;- lm_robust(Y~Z,data=data) data.frame(term = &quot;Z&quot; , estimate = obj$coefficients[[2]], std.error = obj$std.error[[2]], statistic = obj$statistic[[2]], p.value=obj$p.value[[2]], conf.low=obj$conf.low[[2]], conf.high=obj$conf.high[[2]], df=obj$df[[2]], outcome=&quot;Y&quot;) } esthbwt4 &lt;- declare_estimator(handler = tidy_estimator(direct_demean_fun), inquiry=theestimand ,label=&quot;E9: Direct Demeaning, Design SE&quot;) theestimators &lt;- ls(patt=&quot;^est.*?wt&quot;) theestimators checkest &lt;- sapply(theestimators,function(x){ get(x)(as.data.frame(dat2))[c(&quot;estimate&quot;,&quot;std.error&quot;)]}) checkest thedesignPlusEstimators &lt;- thedesign2 + estnowtHC2 + estnowtIID + estnbwt1 + estnbwt2 + estnbwt3 + estnbwt4 + esthbwt1 + esthbwt2 + esthbwt3 + esthbwt4 ## Verifying that this works with a fixed population ## datv1 &lt;- draw_data(thedesign) ##datv2 &lt;- draw_data(thedesign) ##table(datv1$Z,datv2$Z) sims &lt;- 200 set.seed(12345) thediagnosis &lt;- diagnose_design(thedesignPlusEstimators, sims = sims, bootstrap_sims = 0) We see that the estimator using block-size weights (E2, E3, E4, or E5) all eliminate bias (within simulation error). The estimators ignoring blocks (E0 and E1), have bias, and in this simulation, the precision weighed estimators (E6&#x2013;E9) also show high bias &#x2014; with some of them also producing poor coverage or false positive rates (E7 and E9). The diagnostic output also shows us the &#x201C;SD Estimate&#x201D; (which is a good estimate of the standard error of the estimate) and the &#x201C;Mean SE&#x201D; (which is the average of the analytic estimates of the standard error). In the case of E2, E3, E4 or E5 the Mean SE is larger than the SD Estimate &#x2014; this is good in that it means that our analytic standard errors will be conservative. However, we also would prefer that our analytic standard errors not be too conservative, for example, E5 looks good but the mean analytic standard error is quite high compared to E4, for example. ## See https://haozhu233.github.io/kableExtra/awesome_table_in_html.html kable(reshape_diagnosis(thediagnosis)[,diagcols] ) # %&gt;% kable_styling() %&gt;% scroll_box(width = &quot;100%&quot;, height = &quot;600px&quot;) Estimator Term Mean Estimand Mean Estimate Bias SD Estimate RMSE Power E0: Ignores Blocks, IID SE Z -12900.57 -12355.37 545.20 106.01 555.36 1.00 E1: Ignores Blocks, Design SE Z -12900.57 -12355.37 545.20 106.01 555.36 1.00 E2: Diff Means Block Size Weights, Design SE Z -12900.57 -12900.11 0.46 115.65 115.36 1.00 E3: Treatment Interaction with Block Indicators, Design SE Z -12900.57 -12900.11 0.46 115.65 115.36 1.00 E4: Treatment Interaction with Block Indicators, Design SE Z -12900.57 -12900.11 0.46 115.65 115.36 1.00 E5: Least Squares with Block Size Weights, Design SE Z -12900.57 -12900.11 0.46 115.65 115.36 1.00 E6: Precision Weights via Fixed Effects, Design SE Z -12900.57 -14143.05 -1242.48 208.64 1259.79 1.00 E7: Precision Weights via Demeaning, Design SE Z -12900.57 -14143.05 -1242.48 208.64 1259.79 1.00 E8: Direct Precision Weights, Design SE Z -12900.57 -14143.05 -1242.48 208.64 1259.79 1.00 E9: Direct Demeaning, Design SE Z -12900.57 -14143.05 -1242.48 208.64 1259.79 1.00 simdesigns &lt;- get_simulations(thediagnosis) ## simdesigns &lt;- simulate_design(thedesign,sims=sims) simmeans &lt;- simdesigns %&gt;% group_by(estimator) %&gt;% summarize(expest=mean(estimate)) ## Now compare to better behaved outcomes. g &lt;- ggplot(data=simdesigns,aes(x=estimate,color=estimator)) + geom_density() + geom_vline(xintercept=trueATE1) + geom_point(data=simmeans,aes(x=expest,y=rep(0,10)),shape=17,size=6) + theme_bw() + theme(legend.position = c(.9,.8)) print(g) Since we wondered whether these biases might be exacerbated by the highly skewed nature of our outcome data (which is designed to look like administrative outcomes in its prevalence of zeros and long tails), we transformed the outcomes to ranks. This less skewed outcome nearly erases the bias from the block-sizee weighted estimators, and the precision-weighted approach also performs well. Ignoring the blocked design is still a problem here &#x2014; E0 and E1 showing high bias. po_functionNorm &lt;- function(data){ data &lt;- data %&gt;% group_by(b) %&gt;% mutate(Y_Z_0=rank(y0), Y_Z_1=rank(y1)) as.data.frame(data) } theysNorm &lt;- declare_potential_outcomes(handler = po_functionNorm) thedesignNorm &lt;- thepop2 + theysNorm + theestimand + theassign + theobsident datNorm &lt;- draw_data(thedesignNorm) thedesignPlusEstimatorsNorm &lt;- thedesignNorm + estnowtHC2 + estnowtIID + estnbwt1 + estnbwt2 + estnbwt3 + estnbwt4 + esthbwt1 + esthbwt2 + esthbwt3 + esthbwt4 sims &lt;- 200 thediagnosisNorm &lt;- diagnose_design(thedesignPlusEstimatorsNorm, sims = sims, bootstrap_sims = 0) kable(reshape_diagnosis(thediagnosisNorm)[,diagcols] ) # %&gt;% kable_styling() %&gt;% scroll_box(width = &quot;100%&quot;, height = &quot;600px&quot;) Estimator Term Mean Estimand Mean Estimate Bias SD Estimate RMSE Power E0: Ignores Blocks, IID SE Z 0.00 -127.08 -127.08 1.85 127.10 1.00 E1: Ignores Blocks, Design SE Z 0.00 -127.08 -127.08 1.85 127.10 1.00 E2: Diff Means Block Size Weights, Design SE Z 0.00 0.11 0.11 1.70 1.70 0.00 E3: Treatment Interaction with Block Indicators, Design SE Z 0.00 0.11 0.11 1.70 1.70 0.00 E4: Treatment Interaction with Block Indicators, Design SE Z 0.00 0.11 0.11 1.70 1.70 0.00 E5: Least Squares with Block Size Weights, Design SE Z 0.00 0.11 0.11 1.70 1.70 0.00 E6: Precision Weights via Fixed Effects, Design SE Z 0.00 0.11 0.11 1.36 1.36 0.00 E7: Precision Weights via Demeaning, Design SE Z 0.00 0.11 0.11 1.36 1.36 0.00 E8: Direct Precision Weights, Design SE Z 0.00 0.11 0.11 1.36 1.36 0.00 E9: Direct Demeaning, Design SE Z 0.00 0.11 0.11 1.36 1.36 0.00 simdesignsNorm &lt;- get_simulations(thediagnosisNorm) ## simdesigns &lt;- simulate_design(thedesign,sims=sims) simmeansNorm &lt;- simdesignsNorm %&gt;% group_by(estimator) %&gt;% summarize(expest=mean(estimate)) g2 &lt;- ggplot(data=simdesignsNorm,aes(x=estimate,color=estimator)) + geom_density() + geom_vline(xintercept=trueATE1) + geom_point(data=simmeansNorm,aes(x=expest,y=rep(0,10)),shape=17,size=6) + theme_bw() + theme(legend.position = c(.9,.8)) print(g2) In a pair-randomized design, we know that bias should not arise from ignoring the blocking structure but we could double our statistical power by taking the pairing into account (Bowers 2011). This next simulation changes the design to still have unequal sized blocks, but with all of the blocks having the same probability of treatment assignment although they vary greatly in size. Here only two esimators show appreciable bias (E5 and E8). However, ignoring the blocks leads to overly conservative standard errors. theassignEqual &lt;- declare_assignment(Z=block_ra(blocks = bF)) thedesignNormEqual &lt;- thepop2 + theysNorm + theestimand + theassignEqual + theobsident datNormEqual &lt;- draw_data(thedesignNormEqual) thedesignPlusEstimatorsNormEqual &lt;- thedesignNormEqual + estnowtHC2 + estnowtIID + estnbwt1 + estnbwt2 + estnbwt3 + estnbwt4 + esthbwt1 + esthbwt2 + esthbwt3 + esthbwt4 sims &lt;- 200 thediagnosisNormEqual &lt;- diagnose_design(thedesignPlusEstimatorsNormEqual, sims = sims, bootstrap_sims = 0) kable(reshape_diagnosis(thediagnosisNormEqual)[,diagcols] ) # %&gt;% kable_styling() %&gt;% scroll_box(width = &quot;100%&quot;, height = &quot;600px&quot;) Estimator Term Mean Estimand Mean Estimate Bias SD Estimate RMSE Power E0: Ignores Blocks, IID SE Z 0.00 -0.26 -0.26 4.75 4.75 0.00 E1: Ignores Blocks, Design SE Z 0.00 -0.26 -0.26 4.75 4.75 0.00 E2: Diff Means Block Size Weights, Design SE Z 0.00 -0.26 -0.26 4.75 4.75 0.00 E3: Treatment Interaction with Block Indicators, Design SE Z 0.00 -0.26 -0.26 4.75 4.75 0.00 E4: Treatment Interaction with Block Indicators, Design SE Z 0.00 -0.26 -0.26 4.75 4.75 0.00 E5: Least Squares with Block Size Weights, Design SE Z 0.00 77.64 77.64 4.10 77.75 1.00 E6: Precision Weights via Fixed Effects, Design SE Z 0.00 -0.26 -0.26 4.75 4.75 0.00 E7: Precision Weights via Demeaning, Design SE Z 0.00 -0.26 -0.26 4.75 4.75 0.00 E8: Direct Precision Weights, Design SE Z 0.00 127.10 127.10 2.72 127.13 1.00 E9: Direct Demeaning, Design SE Z 0.00 -0.26 -0.26 4.75 4.75 0.00 simdesignsNormEqual &lt;- get_simulations(thediagnosisNorm) simmeansNormEqual &lt;- simdesignsNormEqual %&gt;% group_by(estimator) %&gt;% summarize(expest=mean(estimate)) ## Now compare to better behaved outcomes. g3 &lt;- ggplot(data=simdesignsNormEqual,aes(x=estimate,color=estimator)) + geom_density() + geom_vline(xintercept=trueATE1) + geom_point(data=simmeansNormEqual,aes(x=expest,y=rep(0,10)),shape=17,size=6) + theme_bw() + theme(legend.position = c(.9,.8)) print(g3) 5.4.2.1 Summary of Approaches to the Analysis of Block Randomized Trials Our team prefers block randomized trials because of their potential to increase statistical power (and thus provide more bang for the research buck) as well as their ability to let us focus on subgroup effects when relevant. The approach of our team to the analysis of blocked experiments has been guided by evidence like that shown here: we analyze as we randomize to avoid bias and increase statistical power, and we are careful in our choice of weighting approaches. Different designs will require different specific decisions &#x2014; sometimes we may be willing to trade a small amount of bias for a guarantee that our estimates will be closer to the truth and more precise, on average (i.e.&#xA0;trade mean-squared error for bias). Other studies will be so large or small that one or another strategy will become obvious. We use our Analysis Plans to specify these approaches and simulation studies like those shown here when we are uncertain about the applicability of statistical rules of thumb to any given design. 5.5 Cluster-randomized trials A cluster randomized trial tends to distinguish signal from noise less well than an experiment where we can assign treatment directly to individuals because the number of independent pieces of information available to learn about the treatment effect is closer to the number of clusters (each of which tends to be assigned to treatment independently of each other) than it is to the number of dependent observations within a cluster (See 10 Things You Need to Know about Cluster Randomization). Since we analyze as we randomize, a cluster randomized experiment requires that we (1) weight the combination of cluster-level average treatment effects by cluster size if we are trying to estimate the average of the individual level causal effects (Middleton and Aronow 2015) and (2) change how we calculate standard errors and ppp-values to account for the fact that uncertainty is generated at the level of the cluster and not at the level of the individual (Hansen and Bowers 2008; Gerber and Green 2012). For example, imagine that we had 10 clusters (administrative offices, physicians groups, etc..) with half assigned to treatment and half assigned to control. set.seed(12345) ## Randomly assign half of the clusters to treatment and half to control dat3$Zcluster &lt;- cluster_ra(cluster=dat3$cluster) # dat3$falsestratum &lt;- rep(1,nrow(dat3)) # dat3$iprweight &lt;- with(dat3,ipr(Zcluster,falsestratum,clusterF)) with(dat3,table(Zcluster,cluster)) cluster Zcluster 1 2 3 4 5 6 7 8 9 10 0 8 0 30 40 0 0 0 0 100 800 1 0 20 0 0 50 60 70 80 0 0 So, although our data has 1258 observations, we do not have 1258 pieces of independent information about the effect of the treatment because people were assigned in groups. Rather we have some amount of information in between 100 and the number of clusters, in this case, 10. For example, here we can see the number of people within each cluster &#x2014; and notice that all of the people are coded as either control or treatment because assignment is at the level of the cluster. ## Setup the cluster-randomized design library(ICC) iccres &lt;- ICCest(x=clusterF,y=Y,data=dat3) dat3$varweight &lt;- 1/(iccres$vara + (iccres$varw/dat3$nb)) thepop3 &lt;- declare_population(dat3) po_functionCluster &lt;- function(data){ data$Y_Zcluster_0 &lt;- data$y0 data$Y_Zcluster_1 &lt;- data$y1 data } theysCluster &lt;- declare_potential_outcomes(handler = po_functionCluster) theestimandCluster &lt;- declare_inquiry(ATE = mean(Y_Zcluster_1 - Y_Zcluster_0)) theassignCluster &lt;- declare_assignment(Zcluster=cluster_ra(clusters=cluster)) theobsidentCluster &lt;- declare_reveal(Y, Zcluster) thedesignCluster &lt;- thepop3 + theysCluster + theestimandCluster + theassignCluster + theobsidentCluster datCluster &lt;- draw_data(thedesignCluster) In everyday practice, with more than about 50 clusters, we produce estimates and using more or less the same kinds software as we do above but changing the standard error calculations to use the the CR2 cluster-robust standard error (Pustejovsky 2019). For example, here are two approaches to such adjustment. estAndSE4a &lt;- difference_in_means(Y~Zcluster,data=datCluster,clusters=cluster) estAndSE4a Design: Clustered Estimate Std. Error t value Pr(&gt;|t|) CI Lower CI Upper DF Zcluster -15597 8336 -1.871 0.1232 -37356 6162 4.76 estAndSE4b &lt;- lm_robust(Y~Zcluster,data=datCluster,clusters=cluster,se_type=&quot;CR2&quot;) estAndSE4b Estimate Std. Error t value Pr(&gt;|t|) CI Lower CI Upper DF (Intercept) 15707 8332 1.885 0.1409 -8542 39955 3.578 Zcluster -15597 8336 -1.871 0.1232 -37356 6162 4.760 5.5.1 Bias when cluster size is correlated with potential outcomes When clusters have unequal sizes, we worry about bias in addition to appropriate estimates of precision (Middleton and Aronow 2015) (see also https://declaredesign.org/blog/bias-cluster-randomized-trials.html). Here we demonstrate how to reduce the bias, and also how bias can emerge in the analysis of a cluster randomized trial. # Define estimators that can be repeated in the simulation below estC0 &lt;- declare_estimator(Y~Zcluster, inquiry=theestimand, model=lm, label=&quot;C0: Ignores Clusters, IID SE&quot;) estC1 &lt;- declare_estimator(Y~Zcluster, inquiry=theestimand, model=lm_robust, label=&quot;C1: Ignores Clusters, CR2 SE&quot;) estC2 &lt;- declare_estimator(Y~Zcluster, inquiry=theestimand, model=lm_robust, clusters=cluster, se_type=&quot;stata&quot;,label=&quot;C2: OLS Clusters, Stata RCSE&quot;) estC3 &lt;- declare_estimator(Y~Zcluster, inquiry=theestimand, model=difference_in_means, clusters = cluster,label=&quot;C3: Diff Means Cluster, CR2 SE&quot;) estC4 &lt;- declare_estimator(Y~Zcluster, inquiry=theestimand, model=lm_robust, clusters = cluster,se_type=&quot;CR2&quot;,label=&quot;C4: OLS Cluster, CR2 SE&quot;) estC5 &lt;- declare_estimator(Y~Zcluster, inquiry=theestimand, model=horvitz_thompson, clusters=cluster, simple=FALSE,condition_prs=.5,label=&quot;C5: Horvitz-Thompson Cluster, Young SE&quot;) estC6 &lt;- declare_estimator(Y~Zcluster, inquiry=theestimand, model=lm_robust, weights= nb , clusters=cluster, se_type=&quot;CR2&quot;,label=&quot;C6: OLS Clusters with ClusterSize Weights, CR2 RCSE&quot;) estC7 &lt;- declare_estimator(Y~Zcluster + nb, inquiry=theestimand, model=lm_robust, weights=varweight, clusters=cluster, se_type=&quot;CR2&quot;, label=&quot;C7: OLS Clusters with Weights, CR2 RCSE&quot;) estC8 &lt;- declare_estimator(Y~Zcluster+nb, inquiry=theestimand, model=lm_robust, clusters=cluster, se_type=&quot;CR2&quot;, label=&quot;C8: OLS Clusters with adj for cluster size, CR2 RCSE&quot;) estC9 &lt;- declare_estimator(Y~Zcluster, inquiry=theestimand, model=lm_lin,covariates=~nb, clusters=cluster, se_type=&quot;CR2&quot;, label=&quot;C9: OLS Clusters with adj for cluster size, CR2 RCSE&quot;) # estC10 &lt;- declare_estimator(Y~Zcluster, inquiry=theestimand, # model=lm_robust, weights= ipr(Zcluster,falsestratum,clusterF), # clusters=cluster, se_type=&quot;CR2&quot;, # label=&quot;C10: OLS Clusters with IPR Weights, CR2 RCSE&quot;) bt &lt;- balanceTest(Zcluster~Y+cluster(cluster),data=datCluster) bt0 &lt;- balanceTest(Zcluster~Y,data=datCluster) theestimatorsC &lt;- ls(patt=&quot;^estC[0-9]&quot;) theestimatorsC checkestC &lt;- sapply(theestimatorsC,function(x){ message(x) get(x)(as.data.frame(datCluster))[c(&quot;estimate&quot;,&quot;std.error&quot;)]}) checkestC thedesignClusterPlusEstimators &lt;- thedesignCluster + estC0 + estC1 + estC2 + estC3 + estC4 + estC5 + estC6 + estC7 + estC8 + estC9 #+ estC10 sims &lt;- 200 set.seed(12345) thediagnosisCluster &lt;- diagnose_design(thedesignClusterPlusEstimators, sims = sims, bootstrap_sims = 0) reshape_diagnosis(thediagnosisCluster)[, diagcols] Estimator Term Mean Estimand Mean Estimate 1 C0: Ignores Clusters, IID SE Zcluster -12900.57 -15166.65 2 C1: Ignores Clusters, CR2 SE Zcluster -12900.57 -15166.65 3 C2: OLS Clusters, Stata RCSE Zcluster -12900.57 -15166.65 4 C3: Diff Means Cluster, CR2 SE Zcluster -12900.57 -15166.65 5 C4: OLS Cluster, CR2 SE Zcluster -12900.57 -15166.65 6 C5: Horvitz-Thompson Cluster, Young SE Zcluster -12900.57 -12710.34 7 C6: OLS Clusters with ClusterSize Weights, CR2 RCSE Zcluster -12900.57 -12670.05 8 C7: OLS Clusters with Weights, CR2 RCSE Zcluster -12900.57 -19047.82 9 C8: OLS Clusters with adj for cluster size, CR2 RCSE Zcluster -12900.57 -19120.43 10 C9: OLS Clusters with adj for cluster size, CR2 RCSE Zcluster -12900.57 61051.64 Bias SD Estimate RMSE Power 1 -2266.08 5916.55 6321.84 1.00 2 -2266.08 5916.55 6321.84 1.00 3 -2266.08 5916.55 6321.84 0.76 4 -2266.08 5916.55 6321.84 0.32 5 -2266.08 5916.55 6321.84 0.32 6 190.23 5595.09 5584.33 0.32 7 230.51 5931.54 5921.18 0.24 8 -6147.26 6717.76 9093.48 0.24 9 -6219.86 6709.44 9136.65 0.26 10 73952.20 124497.09 144537.04 0.06 The results of our simulation using 200 simulations show how certain approaches can yield very biased estimates and other approaches can reduce the bias. The C5 and C6 estimators have the lowest bias in this particular example with very few clusters. We also see the problems with the standard errors &#x2014; the actual standard errors (in &#x201C;SD Estimate&#x201D;) should be much higher than those estimated by the approaches which ignore the clustered design (C0 and C1). ## See https://haozhu233.github.io/kableExtra/awesome_table_in_html.html kable(reshape_diagnosis(thediagnosisCluster)[,diagcols] ) # %&gt;% kable_styling() %&gt;% scroll_box(width = &quot;100%&quot;, height = &quot;800px&quot;) Estimator Term Mean Estimand Mean Estimate Bias SD Estimate RMSE Power C0: Ignores Clusters, IID SE Zcluster -12900.57 -15166.65 -2266.08 5916.55 6321.84 1.00 C1: Ignores Clusters, CR2 SE Zcluster -12900.57 -15166.65 -2266.08 5916.55 6321.84 1.00 C2: OLS Clusters, Stata RCSE Zcluster -12900.57 -15166.65 -2266.08 5916.55 6321.84 0.76 C3: Diff Means Cluster, CR2 SE Zcluster -12900.57 -15166.65 -2266.08 5916.55 6321.84 0.32 C4: OLS Cluster, CR2 SE Zcluster -12900.57 -15166.65 -2266.08 5916.55 6321.84 0.32 C5: Horvitz-Thompson Cluster, Young SE Zcluster -12900.57 -12710.34 190.23 5595.09 5584.33 0.32 C6: OLS Clusters with ClusterSize Weights, CR2 RCSE Zcluster -12900.57 -12670.05 230.51 5931.54 5921.18 0.24 C7: OLS Clusters with Weights, CR2 RCSE Zcluster -12900.57 -19047.82 -6147.26 6717.76 9093.48 0.24 C8: OLS Clusters with adj for cluster size, CR2 RCSE Zcluster -12900.57 -19120.43 -6219.86 6709.44 9136.65 0.26 C9: OLS Clusters with adj for cluster size, CR2 RCSE Zcluster -12900.57 61051.64 73952.20 124497.09 144537.04 0.06 simdesigns &lt;- get_simulations(thediagnosis) ## simdesigns &lt;- simulate_design(thedesign,sims=sims) simmeans &lt;- simdesigns %&gt;% group_by(estimator) %&gt;% summarize(expest=mean(estimate)) The following plot shows many of the estimators produce results far from the truth (shown by the vertical black bar), and also shows the diversity in precision of the estimators. ## Now compare to better behaved outcomes. gDiagClust &lt;- ggplot(data=simdesigns,aes(x=estimate,color=estimator)) + geom_density() + geom_vline(xintercept=trueATE1) + geom_point(data=simmeans,aes(x=expest,y=rep(0,10)),shape=17,size=6) + theme_bw() #+ #theme(legend.position = c(.9,.8)) print(gDiagClust) 5.5.2 Incorrect false positive rates from tests and confidence intervals When we have few clusters, analytic standard errors such as those used by difference_in_means and lm_robust may lead to incorrect false positive rates for our hypothesis tests or confidence intervals. We demonstrate how adjusting for cluster-randomization can help ensure that the false positive rates of our hypothesis tests is controlled/known, we also discuss the limitations of these approaches. The next code block compares the false positive rates of two different approaches to adjusting standard errors in a cluster randomized trial. To distinguish between the problems of bias arising from unequal sized clusters and problems of false positive rates or covereage arising from the fact that we have fewer clusters than units, we use a design with equal numbers of units per cluster: with(dat1,table(Zcluster,buildingID)) buildingID Zcluster 1 2 3 4 5 6 7 8 9 10 0 10 0 10 10 0 0 0 0 10 10 1 0 10 0 0 10 10 10 10 0 0 In this case, with equal sized clusters, a simple outcome, and equal numbers of clusters in treatment and control, we see that the CR2 standard error controls the false positive rate (less than 5% of the 1000 simulations testing a true null hypothesis of no effects return a ppp-value of less than .05) while the &#x201C;Stata&#x201D; standard error has a slightly too high false positive rate of r fprateStata05 at the 5% error level. We also saw above that not controlling at all yields very poor coverage (see the rows for C0 and C1) &#x2014; and we saw that the &#x201C;Stata&#x201D; approach has poor coverage relative to the CR2 based approaches as well, in this case with only 10 clusters. checkFP &lt;- function(dat, setype = &quot;CR2&quot;) { ## Break any relationship between treatment and outcomes by permuting ## or shuffling the treatment variable. This means that H0, the null, ## of no effects is true. dat$newZ &lt;- cluster_ra(cluster = dat$buildingID) newest &lt;- lm_robust(Y ~ newZ, dat = dat, clusters = buildingID, se_type = setype) return(nullp = newest$p.value[&quot;newZ&quot;]) } smalldat &lt;- dat1[, c(&quot;Y&quot;, &quot;buildingID&quot;)] set.seed(123) fpresCR2 &lt;- replicate(1000, checkFP(dat = smalldat)) set.seed(123) fpresStata &lt;- replicate(1000, checkFP(dat = smalldat, setype = &quot;stata&quot;)) fprateCR205 &lt;- mean(fpresCR2 &lt;= .05) fprateCR205 [1] 0.045 fprateStata05 &lt;- mean(fpresStata &lt;= .05) fprateStata05 [1] 0.054 The following plot shows that, in this case the Stata standard error tends to make slightly too many false positive errors (shown by open circles above the 45 degree line) and the CR2 standard error tends control the error rate of the test (with black dots below the 45 degree line). plot(ecdf(fpresCR2),pch=19) plot(ecdf(fpresStata),pch=21,add=TRUE) abline(0,1) When a simulation like the above shows false positive rate problems with the CR2 standard error, we then use permutation-based randomization inference (rather than the asymptotic justified randomization inference of the CR2 standard error). [TO DO: An example using permutation based inference and its false positive rate]. References "],
["poweranalysis.html", "Chapter 6 Power Analysis 6.1 An example of the off-the-shelf approach 6.2 An example of the simulation approach 6.3 When to use which approach 6.4 Additional examples of the simulation approach", " Chapter 6 Power Analysis In this section we provide examples of how we assess statistical power for different experimental research designs. We often prefer to use simulation to assess the power of different research designs because we rarely have designs that fit easily into the assumptions made by analytic tools. However, for the sake of an example, we will show a version that uses simulation that produces the same answer as the faster analytic version. Imagine that we thought that a study would have an effect of about 1 standard deviation &#x2013; this is more or less the effect difference we have created in our example dataset so far. How large a study do we need in order to distinguish this effect from noise? population &lt;- declare_population(dat1) potentials &lt;- declare_potential_outcomes(Y ~ Z * y1 + (1-Z) * y0) assignment &lt;- declare_assignment(Z=complete_ra(N)) reveal &lt;- declare_reveal(Y, Z) design &lt;- population + potentials + assignment + reveal simdat1 &lt;- draw_data(design) # Notice that we have new potential outcomes called Y_Z_1 and Y_Z_0 that are # copies of y0 and y1 stopifnot(with(simdat1, cor(y0, Y_Z_0)) == 1) estimand &lt;- declare_inquiry(ATE = mean(Y_Z_1 - Y_Z_0)) estimator &lt;- declare_estimator(Y ~ Z, inquiry = estimand, label = &quot;Simple D-I-M&quot;) # We can see how the estimator function works using some data simulated based on # the design estimator(simdat1) estimator term estimate std.error statistic p.value conf.low conf.high df outcome inquiry 1 Simple D-I-M Z 5.571 0.8153 6.834 7.069e-10 3.953 7.189 98 Y ATE designPlusEst &lt;- design + estimand + estimator Now that the setup is complete, we can assess the statistical power of our proposed design with N=100N=100N=100 and an effect of roughly 1 SD. The output below shows the statistical power as well as the false positive rate (called &#x201C;Coverage&#x201D; below) as well as the difference between the mean difference we calculate and the true average treatment effect (called &#x201C;Bias&#x201D; below). diagnose_design(designPlusEst, sims = 1000) Research design diagnosis based on 1000 simulations. Diagnosis completed in 30 secs. Diagnosand estimates with bootstrapped standard errors in parentheses (100 replicates). Design Inquiry Estimator Outcome Term N Sims Mean Estimand Mean Estimate Bias designPlusEst ATE Simple D-I-M Y Z 1000 5.45 5.47 0.02 (0.00) (0.02) (0.02) SD Estimate RMSE Power Coverage 0.71 0.71 1.00 0.97 (0.02) (0.02) (0.00) (0.01) The analytic approach suggests more power than the simulation based approach &#x2013; a difference that we suspect arises from our fairly skewed outcome. power.t.test(n = 100, delta = 1, sd = 1) Two-sample t test power calculation n = 100 delta = 1 sd = 1 sig.level = 0.05 power = 1 alternative = two.sided NOTE: n is number in *each* group For ways to compare different sample sizes, effect sizes, etc. see more information from the DeclareDesign package. 6.1 An example of the off-the-shelf approach To demonstrate how a power analysis might work in principle, consider another example using the R function power.t.test(). When using this function, there are three parameters that we&#x2019;re most concerned with, two of which are specified by the user, and the third of which is then calculated and returned by the function. These are: n = sample size, or number of observations;, delta = the target effect size, or a minimum detectable effect (MDE); and power = the probability of detecting an effect if in fact there is a true effect of size delta. Say, for example, you want to know the MDE for a two-arm study with 1,000 participants. Using power.t.test() you would specify: power.t.test( n = 500, # n denotes the number of obs. per treatment arm power = 0.8 # use traditional power threshold of 80% ) Two-sample t test power calculation n = 500 delta = 0.1774 sd = 1 sig.level = 0.05 power = 0.8 alternative = two.sided NOTE: n is number in *each* group If we wanted to extract the MDE we could write: power.t.test( n = 500, # number of obs. per treatment arm power = 0.8 # traditional power threshold of 80% )$delta [1] 0.1774 We can similarly extract other parameters, like sample size and power, using $n or $power instead of $delta. If you need to, you can adjust other parameters, like the standard deviation of the response, the level of the test, or whether the test is one-sided rather than two-sided. There are also other functions available for different types of outcomes. For example, if you have a binary response, you can use power.prop.test() to calculate power for a difference in proportions test. An equivalent approach in Stata is as follows: power twomeans 0, power(0.8) n(1000) sd(1) Stata users can learn more about available tools by checking out Stata&#x2019;s plethora of relevant help files. 6.2 An example of the simulation approach We can compare this approach with power.t.test() to the output from a computational approach, which we define in the code chunk below. Results are shown in the subsequent Figure XXXX. Though clearly the computational estimates are slightly different, they comport quite well with the analytic estimates. # Workflow for power simulation toolkit: # Replicate, Estimate, Evaluate (REE) # # replicate_design(...) ---: Generate multiple replicates of a # a simulated d.g.p. + treatment assignment. # estimate(...) -----------: Estimate the null test-stat for treatment(s). # evaluate_power(...) -----: Evaluate power to detect non-zero effects. # evaluate_mde(...) -------: Find MDE, searching over range of effect sizes. # evaluate_bias(...) ------: Compute bias and other diagnostics. ##### REPLICATE a design ##### replicate_design &lt;- function(R = 200, ...) { design &lt;- function() { #require(magrittr) #require(fabricatr) fabricatr::fabricate( ... ) %&gt;% list } rep &lt;- replicate( n = R, expr = design() ) for(i in 1:length(rep)) { rep[[i]] %&gt;% dplyr::mutate( sim = i ) } return(rep) } ##### ESTIMATE the null ##### estimate &lt;- function( formula, vars, data = NULL, estimator = estimatr::lm_robust ) { # require(magrittr) data %&gt;% purrr::map( ~ estimator( formula, data = . ) %&gt;% estimatr::tidy() %&gt;% dplyr::filter( .data$term %in% vars ) ) %&gt;% dplyr::bind_rows() %&gt;% dplyr::mutate( sim = rep(1:length(data), each = n() / length(data)), term = factor(.data$term, levels = vars) ) } ##### EVALUATE power ##### evaluate_power &lt;- function( data, delta, level = 0.05 ) { if(missing(delta)) { stop(&quot;Specify &apos;delta&apos; to proceed.&quot;) } # require(foreach) # require(magrittr) foreach::foreach( i = 1:length(delta), .combine = &quot;bind_rows&quot; ) %do% { data %&gt;% dplyr::mutate( delta = delta[i], new_statistic = (.data$estimate + .data$delta) / .data$std.error ) %&gt;% dplyr::group_split(.data$term) %&gt;% purrr::map( ~ { tibble::tibble( term = .$term, delta = .$delta, p.value = foreach( j = 1:length(.$new_statistic), .combine = &quot;c&quot; ) %do% mean(abs(.$statistic) &gt;= abs(.$new_statistic[j])) ) } ) } %&gt;% group_by(.data$term, .data$delta) %&gt;% summarize( power = mean(.data$p.value &lt;= level), .groups = &quot;drop&quot; ) } ##### EVALUATE Min. Detectable Effect ##### evaluate_mde &lt;- function( data, delta_range = c(0, 1), how_granular = 0.01, level = 0.05, min_power = 0.8 ) { eval &lt;- evaluate_power( data = data, delta = seq(delta_range[1], delta_range[2], how_granular), level = level ) %&gt;% dplyr::group_by( .data$term ) %&gt;% dplyr::summarize( MDE = min(.data$delta[.data$power &gt;= min_power]), .groups = &quot;drop&quot; ) return(eval) } ##### EVALUATE Bias ##### evaluate_bias &lt;- function( data, ATE = 0 ) { #require(magrittr) smry &lt;- data %&gt;% dplyr::mutate( ATE = rep(ATE, len = n()) ) %&gt;% dplyr::group_by( .data$term ) %&gt;% dplyr::summarize( &quot;True ATE&quot; = unique(.data$ATE), &quot;Mean Estimate&quot; = mean(.data$estimate), Bias = mean(.data$estimate - .data$ATE), MSE = mean((.data$estimate - .data$ATE)^2), Covarage = mean(.data$conf.low &lt;= .data$ATE &amp; .data$conf.high &gt;= .data$ATE), &quot;SD of Estimates&quot; = sd(.data$estimate), &quot;Mean SE&quot; = mean(.data$std.error), Power = mean(.data$p.value &lt;= 0.05), .groups = &quot;drop&quot; ) return(smry) } n &lt;- 1000 d &lt;- 0.2 power_data &lt;- tibble( d = seq(0, 0.5, len = 200), power = power.t.test(n = n / 2, delta = d)$power ) # save plot; later add simulation results g &lt;- ggplot(power_data) + geom_line( aes(d, power, linetype = &quot;power.t.test()&quot;) ) + labs( x = expression(delta), y = &quot;Power&quot;, title = &quot;Power for Simple Difference in Means Test&quot; ) + scale_y_continuous( n.breaks = 6 ) + geom_hline( yintercept = 0.8, col = &quot;grey25&quot;, alpha = 08 ) + ggridges::theme_ridges( center_axis_labels = TRUE, font_size = 10 ) # Add results from simulation to compare sim_power_data &lt;- replicate_design( N = n, y = rnorm(N), x = randomizr::complete_ra( N, m = N / 2 ) ) %&gt;% estimate( form = y ~ x, vars = &quot;x&quot; ) %&gt;% evaluate_power( delta = seq(0, 0.5, len = 200) ) g + geom_line( data = sim_power_data, aes(delta, power, linetype = &quot;simulation&quot;), color = &quot;grey25&quot; ) + labs( linetype = &quot;Method:&quot; ) + theme( legend.position = &quot;bottom&quot; ) Figure 1. Off-the-shelf versus simulation power estimates We produce these computational estimates using some helpful tools currently housed in OES&#x2019;s GitHub code library and in the chunk above. These tools are designed around a simple workflow, and remove some of the programming required to calculate power computationally. These tools are centered around the following workflow: Replicate Estimate Evaluate The first step in the workflow, Replicate, entails designing a data-generating process (which may include only an outcome variable and treatment assignment) and simulating this process multiple times to create randomly drawn datasets from a population. Each of these is a sample replicate. The next step, Estimate, entails estimating effects for select treatments under the null hypothesis for each sample replicate. This provides a distribution of test statistics under the null hypothesis. Finally, the last step, Evaluate, entails evaluating power to detect an effect over a range of alternative effect sizes. This workflow is supported by three functions: replicate_design(), estimate(), and evaluate_power(). Here&#x2019;s the script used to generate the computational estimates shown in Figure 1: # 1. Replicate: rep &lt;- replicate_design( R = 200, # Number of sample replicates N = 1000, # Sample size of each replicate y = rnorm(N), # Normally distributed response x = rbinom(N, 1, 0.5) # Binary treatment indicator ) # 2. Estimate: est &lt;- estimate( y ~ x, vars = &quot;x&quot;, data = rep ) # 3. Evaluate: pwr_eval_sim &lt;- evaluate_power( data = est, delta = seq(0, 0.5, len = 200) ) The final product of this approach, the object pwr_eval_sim, reports the power for each of the user-specified effect sizes (delta) for each of the model terms specified by the vars command in the estimate() function. The output can be used to plot power curves or to compute minimum detectable effects. This approach makes computational power analysis vastly more efficient from a programming perspective, while also providing ample room for flexibility in both design and estimation strategy. For example, replicate_design() is a wrapper for fabricate() in the fabricatr package. This gives users the ability to generate multi-level or nested data-generating processes, specify covariates, or determine whether treatment randomization is done within blocks or by clusters. Additionally, by default, estimates are returned using lm_robust() from the estimatr package, but alternative estimators can be specified. Say, for example, you have a binary response and a set of covariates and your design calls for using logistic regression. You can generate estimates for such a design as follows: logit &lt;- function(...){ glm(..., family = binomial)} est &lt;- estimate( y ~ x + z1 + z2, data = rep, estimator = logit ) These tools, in short, while imposing a particular workflow, provide significant latitude in terms of design and estimation. However, other tools exist as well. DeclareDesign provides useful tools, for example. And there is no shortage of simulation examples online of a variety of research designs. 6.3 When to use which approach Of course, for a simple difference in means test, the programming required for an analytical solution is much much less involved than the computational solution. When is the latter worth the extra investment? In cases where we&#x2019;re interested in the power to detect a simple difference in means, or a difference in proportions for binary responses, it is probably sufficient to use power.t.test() (for means) or power.prop.test() (for proportions). However, OES projects often involve design features or analytic strategies that are difficult to account for using off-the-shelf tools. For example, we often include covariates in our statistical models when we analyze outcomes in order to enhance the precision of our estimates of treatment effects. If the gain in precision is small, then it might not be important to account for this in power calculations in the design phase of the project. However, if we expect a substantial gain in precision due to including covariates, then we probably want to account for this in our design. The natural way to do this is by simulation, where we can include the covariates in the &#x201C;replicate&#x201D; and &#x201C;estimate&#x201D; steps. Accounting for covariates in this way is especially useful if we can use real historical or pre-treatment data that represent the correlations we expect to see between covariates and outcomes in our later analysis of the trial data. More complex design features or analytic strategies may make investing in the simulation approach even more worthwhile, or downright necessary. Examples include heterogeneity in treatment effects, a multi-arm or factorial design, or block randomization with differing probabilities of treatment between blocks &#x2013; none of which can practically be accounted for with off-the-shelf tools. In the next section, we provide some additional examples of simulations for more complex design features or analytic strategies. 6.4 Additional examples of the simulation approach Here we provide two examples of research designs where simulation is well worth the extra effort. Attendant R code is included in the discussion to illustrate how to use tools in the code library to streamline the approach. 6.4.1 A two-by-two design with interaction One instance where computational power analysis may be worth the investment is in assessing power for a two-by-two factorial design with an interaction. In such a design, the goal is to assess not only the power to detect main effects (the average effect of each individual treatment), but also power to detect a non-zero interaction effect between the treatments. Say we have a design with 1,000 observations and we would like to know the effect of two treatments on a binary outcome with a baseline of 0.25. Each treatment is assigned to M=500M = 500M=500 individuals at random, resulting in four roughly equal sized groups of observations after randomization: (1) a control group, (2) those assigned to treatment 1 but not treatment 2, (3) those assigned to treatment 2 but not treatment 1, and (4) those assigned to both treatment 1 and 2. We can easily calculate power to detect the main effect of each treatment, in addition to their interaction, using replication, estimation, and evaluation as follows: two_by_two &lt;- # Replicate a 2x2 design replicate_design( N = 1000, y = rbinom(N, 1, 0.25), x1 = complete_ra(N, m = N / 2) - 1 / 2, # center treatment indicators x2 = complete_ra(N, m = N / 2) - 1 / 2 ) %&gt;% # Estimate main and interaction effects estimate( form = y ~ x1 + x2 + x1:x2, vars = c(&quot;x1&quot;, &quot;x2&quot;, &quot;x1:x2&quot;) ) %&gt;% # Evaluate power evaluate_power( delta = seq(0, 0.25, len = 200) ) Using the output reported in the object two_by_two, we can easily plot the power curves for each of the main effects and the interaction effect, as shown in Figure 2. ggplot(two_by_two) + geom_line( aes(delta, power, linetype = term) ) + geom_hline( yintercept = 0.8, color = &quot;grey25&quot;, alpha = 0.8 ) + scale_y_continuous( n.breaks = 6 ) + labs( x = expression(delta), y = &quot;Power&quot;, title = &quot;Power for a 2x2 Design&quot;, linetype = &quot;Effect for...&quot; ) + ggridges::theme_ridges( font_size = 10, center_axis_labels = TRUE ) + theme( legend.position = &quot;bottom&quot; ) Figure 2. Computational power analysis for a 2&#xD7;22\\times 22&#xD7;2 design. Of course, we could have relied on some reasonable analytical assumptions to arrive at these estimates without resorting to simulation (see a helpful discussion here), but running a simulation saves us the trouble. 6.4.2 Covariate adjustment with the Lin estimator Another scenario where computational power analysis is worth the investment is if a design calls for covariate adjustment. This is common in OES projects, and, in several instances, the Lin (2013) saturated estimator is the analytical solution employed. Devising an off-the-shelf method to calculate power for such a study is possible, but would likely require investing time doing background research to ensure its accuracy. Alternatively, we could simply replicate, estimate, and evaluate such a design computationally. The results will be just as accurate, without the added hassle of programming the appropriate analytical solution. Suppose we have a sample of 1,000 observations and a continuous outcome variable. We wish to assess the effect of some policy intervention on this continuous outcome. Our design calls for randomly assigning M=250M = 250M=250 individuals to receive the intervention, and the rest to control. In addition to having data on the outcome and on treatment assignment, say that we also anticipate obtaining a dataset of covariates for our 1,000 observations. This data contains two variables that are prognostic of the outcome. We&#x2019;ll call these z1 and z2. The first is a continuous measure; the latter a binary indicator. Our design calls for adjusting for these covariates via the Lin estimator to improve the precision of our estimated treatment effect. We can simulate such a design, and further justify covariate adjustment, using the following replication, estimation, and evaluation. We begin by replicating the data-generating process: rep_data &lt;- replicate_design( N = 1000, z1 = rnorm(N, sd = 3), # continuous covariate z2 = rbinom(N, 1, 0.25), # binary covariate cz1 = z1 - mean(z1), # make mean centered versions of the covariates cz2 = z2 - mean(z2), y = 0.8 * z1 - 1 * z2 + rnorm(N), # simulate the data-generating process x = complete_ra(N, m = N / 4) # randomly assign 25% of obs. to treatment ) We then estimate and evaluate. For comparison, power is computed both with covariate adjustment via the Lin estimator and without covariate adjustment: # With the Lin Estimator lin_adjust &lt;- rep_data %&gt;% estimate( form = y ~ x + z1 + z2 + x:cz1 + x:cz2, vars = &quot;x&quot; ) %&gt;% evaluate_power( delta = seq(0, 0.5, len = 200) ) # With no Covariate Adjustment no_adjust &lt;- rep_data %&gt;% estimate( form = y ~ x, vars = &quot;x&quot; ) %&gt;% evaluate_power( delta = seq(0, 0.5, len = 200) ) We can now easily compare results under these alternative empirical strategies. Figure 3 shows the power curves for each approach. As is clear, the Lin estimator provides substantial improvements in power. With covariate adjustment, we&#x2019;re powered to detect an effect nearly 40% the size of the MDE without controlling for z1 and z2. And it only took a few lines of code to get this result. bind_rows( lin_adjust %&gt;% mutate(Method = &quot;Lin&quot;), no_adjust %&gt;% mutate(Method = &quot;No Covariates&quot;) ) %&gt;% ggplot() + geom_line( aes(delta, power, linetype = Method) ) + geom_hline( yintercept = 0.8, color = &quot;grey25&quot;, alpha = 0.8 ) + scale_y_continuous( n.breaks = 6 ) + labs( x = expression(delta), y = &quot;Power&quot;, title = &quot;Power with Lin Adjustment&quot;, linetype = &quot;Method:&quot; ) + ggridges::theme_ridges( font_size = 10, center_axis_labels = TRUE ) + theme( legend.position = &quot;bottom&quot; ) Figure 3. Simulating power with the Lin estimator. 6.4.3 Incorporating DeclareDesign into OES Power Tools We can also use DeclareDesign within the Replicate, Estimate, Evaluate framework. This involves using DeclareDesign to draw estimates, and then feeding the results into the OES evaluate_power() function. We compare the DeclareDesign approach to the OES Replicate and Estimate steps below. First, we simulate a simple design with the OES tools: eval &lt;- replicate_design( R = 1000, N = 100, Y = rnorm(N), Z = rbinom(N, 1, 0.5) ) %&gt;% estimate( form = Y ~ Z, vars = &quot;Z&quot; ) %&gt;% evaluate_power( delta = seq(0, 0.6, len = 10) ) Then, we do the same with DeclareDesign, declaring a population, potential outcomes, assignments, a target quantity of interest, and an estimator: design &lt;- declare_population( N = 100, U = rnorm(N) ) + declare_potential_outcomes( Y ~ 0 * Z + U ) + declare_assignment(Z=complete_ra(N, prob = 0.5) ) + declare_inquiry(ATE = mean(Y_Z_1 - Y_Z_0)) + declare_measurement(Y = reveal_outcomes(Y ~ Z)) + declare_estimator( Y ~ Z, inquiry = &quot;ATE&quot;, model = lm_robust ) We then use draws from this design within the OES tools: dd_eval &lt;- replicate( n = 1000, expr = draw_estimates(design) %&gt;% list ) %&gt;% bind_rows() %&gt;% evaluate_power( delta = seq(0, 0.6, len = 10) ) We show the similarity between the two approaches to generating the simulated data in the figure below. bind_rows( eval %&gt;% mutate(method = &quot;OES Power Tools&quot;), dd_eval %&gt;% mutate(method = &quot;DeclareDesign&quot;) ) %&gt;% ggplot() + geom_line( aes(delta, power, linetype = method) ) + labs( x = expression(delta), y = &quot;Power&quot;, linetype = NULL ) + scale_y_continuous( n.breaks = 6 ) + geom_hline( yintercept = 0.8, col = &quot;grey25&quot;, size = 1, alpha = 0.8 ) + ggridges::theme_ridges( center_axis_labels = TRUE, font_size = 10 ) + theme( legend.position = &quot;bottom&quot; ) References "],
["working-with-data.html", "Chapter 7 Working with data 7.1 General questions we ask of a data set", " Chapter 7 Working with data Our team works with administrative data, data not collected specifically for the purpose of evaluating the impact of new policy ideas. This means that we, and our agency collaborators, spend a ton of time cleaning, merging, and checking data. Here, we describe some standard practices that we have developed over time. 7.1 General questions we ask of a data set Are there any duplicated observations? (This mostly means rows in a rectangular data set). If there is an ID variable, are there duplicated IDs? Are there missing data on outcomes? Why are outcomes missing? Are there missing data on our record of treatment assignment? Why might we not know whether or not a given unit was assigned the new policy intervention? "],
["glossary-of-terms.html", "Chapter 8 Glossary of Terms", " Chapter 8 Glossary of Terms Average treatment effect ATE "],
["appendix.html", "Chapter 9 Appendix", " Chapter 9 Appendix "],
["references.html", "References", " References "]
]
