```{r setupdat13,  echo=FALSE, include=FALSE}
if(!exists("dat1")){ dat1 <- read.csv(here::here('dat1.csv')) }
```

# Randomization choices

After working together with our agency partners to translate insights from the social and behavioral sciences into potential policy recommendations, we assess those new ideas by observing differences or changes in real world outcomes (usually measured using existing administrative data).^[We develop the recommendations themselves following [processes](https://oes.gsa.gov/methods/) described in more detail on our website.] In most cases, we design a randomized control trial (an RCT) to ensure that the differences or changes we observe are driven by the policy intervention itself. Here, we show examples of the different methods we consider for randomly assigning units to treatment. These form the core of the different types of RCTs that we use to build evidence about the effectiveness of the new policies.

## Coin flipping vs urn-drawing randomization

Many discussions of RCTs begin by talking about the intervention being assigned to units (people,  schools, offices, districts) "by the flip of a coin," or **simple** random assignment. Each unit's assignment to treatment occurs separately, and there is no *ex ante* guarantee as to exactly what the final number of treated or control units will be. We don't always use this method in practice, even though it is a useful way to introduce the idea that RCTs guarantee fair access to a new policy.

The following code contrasts coin-flipping style random assignment with drawing-from-an-urn style, or **complete** random assignment (where a fixed number of units are randomly chosen for treatment). Coin-flipping based experiments are still valid and tell us about the underlying counterfactuals, but they can have less statistical power, so we try to avoid them where possible.

Notice that the simple random assignment implemented in the code below results in more observations in the treatment group (group `T`) than in the control group (group `C`). Complete random assignment will always assign 5 units to the treatment, 5 to the control.

<!-- Adds copy code button -->
```{r klippych4, echo=FALSE, include=TRUE}
klippy::klippy(all_precode = T, position = c("top", "right"))
```

<!-- Used (and iteratively updated) in the {oes_code_tab} snippets below. -->
<!-- set chapter number and reset count -->
```{r, include = F, echo = F}
# cnum is modified automatically to iteratively count chunks
# when using the oes_code_tab markdown snippet. each use of
# the snippet adds a value of 1.
ch <- 4
cnum <- 0
```

::: {.tab} 
<button class="tablinks" onclick="unrolltab(event, 'ch4R1')">R code</button>
<button class="tablinks" onclick="unrolltab(event, 'ch4Stata1')">Stata code</button>
<button class="tablinks" onclick="unrolltab(event, 'ch4Hide1')">Hide</button>
::: {#ch4R1 .tabcontent} 
<br />
```{r, highlight = F, eval = F}
## Start with a small experiment with only 10 units
n <- 10

## Set a random seed for replicability
set.seed(12345)

## Function to add more intuitive labels
labelTC <- function(assignments) {ifelse(assignments == 1, "T", "C")}

## Simulation using functions from the randomizr package
trt_coinflip <- labelTC(simple_ra(n))
trt_urn <- labelTC(complete_ra(n))

## Coin flipping does not guarantee half and half treated and control.
## Drawing from an urn, guarantees half treated and control.
table(trt_coinflip)
table(trt_urn)

## Alternative approach using base R
# set.seed(12345)
# trt_coinflip <- labelTC(rbinom(10, size = 1, prob = .5))
# trt_urn <- labelTC(sample(rep(c(1, 0), n / 2)))
# table(trt_coinflip)
# table(trt_urn)
```
:::
::: {#ch4Stata1 .tabcontent} 
<br />
```{stata, highlight = F, eval = F}
** Start with a small experiment with only 10 units
clear
global n = 10
set obs $n

** Set a random seed for replicability
set seed 12345

** Simulation using functions from the randomizr package
* ssc install randomizr

simple_ra trt_coinflip
* Or, e.g.: gen trt_coinflip = rbinomial(1, 0.5)

complete_ra trt_urn
/* 
* Or, e.g.:
local num_treat = $n/2
gen rand = runiform()
sort rand
gen trt_urn = 1 in 1/`num_treat'
replace trt_urn = 0 if missing(trt_urn)
drop rand
*/

** Add more informative labels
label define tc 0 "C" 1 "T"
label values trt_coinflip tc
label values trt_urn tc

** Coin flipping does not guarantee half and half treated and control.
** Drawing from an urn, guarantees half treated and control.
table trt_coinflip
table trt_urn
```
::: 
::: {#ch4Hide1 .tabcontent}
::: 
:::

```{r completera, echo = F, eval = T, results = "hold", collapse = T}
## Start with a small experiment with only 10 units
n <- 10

## Set a random seed for replicability
set.seed(12345)

## Function to add more intuitive labels
labelTC <- function(assignments) {ifelse(assignments == 1, "T", "C")}

## Simulation using functions from the randomizr package
trt_coinflip <- labelTC(simple_ra(n))
trt_urn <- labelTC(complete_ra(n))

## Coin flipping does not guarantee half and half treated and control.
## Drawing from an urn, guarantees half treated and control.
table(trt_coinflip)
table(trt_urn)

## Alternative approach using base R
# set.seed(12345)
# trt_coinflip <- labelTC(rbinom(10, size = 1, prob = .5))
# trt_urn <- labelTC(sample(rep(c(1, 0), n / 2)))
# table(trt_coinflip)
# table(trt_urn)
```

## Randomization into 2 or more groups

We often use the `randomizr` R package [@R-randomizr] for simple designs
rather than the base R `sample` function because `randomizr` does some
quality control checks. Notice that we implement a check on our code below with the `stopifnot` command: the code will stop and issue a warning if we didn't actually assign 1/4 of the observations to the treatment condition. Here, we assign the units first to 2 arms with equal probability (`Z2armEqual`). Then, to show how the code works, we assign them to 2 arms where one arm has only a $\frac{1}{4}$ probability of receiving treatment (e.g., imagine a design with an expensive intervention). Last, we assign them based on a design with 4 different arms, each with equal probability (e.g., one control group and three different treatments under consideration). We often use $Z$ to refer to the variable recording our intervention arms.

::: {.tab} 
<button class="tablinks" onclick="unrolltab(event, 'ch4R2')">R code</button>
<button class="tablinks" onclick="unrolltab(event, 'ch4Stata2')">Stata code</button>
<button class="tablinks" onclick="unrolltab(event, 'ch4Hide2')">Hide</button>
::: {#ch4R2 .tabcontent} 
<br />
```{r, highlight = F, eval = F}
N <- nrow(dat1)
set.seed(12345)

## Two equal arms
dat1$Z2armEqual <- labelTC(complete_ra(N))

## Two unequal arms: .25 chance of treatment (.75 chance of control0
dat1$Z2armUnequalA <- labelTC(complete_ra(N,prob=.25))
stopifnot(sum(dat1$Z2armUnequalA=="T")==N/4)
dat1$Z2armUnequalB <- labelTC(complete_ra(N,m=N/4))

## Four equal arms
dat1$Z4arms <- complete_ra(N, m_each=rep(N/4,4))

table(Z2armEqual=dat1$Z2armEqual)
table(Z2armUnequalA=dat1$Z2armUnequalA)
table(Z2armUnequalB=dat1$Z2armUnequalB)
table(Z4arms=dat1$Z4arms)
```
:::
::: {#ch4Stata2 .tabcontent} 
<br />
```{stata, highlight = F, eval = F}
** Return to data for the fake experiment.
import delimited using "dat1.csv", clear

qui count
global N = r(N)
set seed 12345

** Two equal arms
complete_ra z2armEqual
label define tc 0 "C" 1 "T"
label values z2armEqual tc

** Two unequal arms: .25 chance of treatment (.75 chance of control)
complete_ra z2armUnequalA, prob(0.25)
label values z2armUnequalA tc
qui sum z2armUnequalA
global expected = $N/4
assert r(sum) == $expected
complete_ra z2armUnequalB, m($expected)
label values z2armUnequalB tc

** Four equal arms
local count_list : di _dup(4) "$expected " // List of sample sizes for each group
macro list _count_list
complete_ra z4arms, m_each(`count_list')

table z2armEqual
table z2armUnequalA
table z2armUnequalB
table z4arms
```
::: 
::: {#ch4Hide2 .tabcontent}
::: 
:::

```{r completera2, echo = F, eval = T, results = "hold", collapse = T}
N <- nrow(dat1)
set.seed(12345)

## Two equal arms
dat1$Z2armEqual <- labelTC(complete_ra(N))

## Two unequal arms: .25 chance of treatment (.75 chance of control0
dat1$Z2armUnequalA <- labelTC(complete_ra(N,prob=.25))
stopifnot(sum(dat1$Z2armUnequalA=="T")==N/4)
dat1$Z2armUnequalB <- labelTC(complete_ra(N,m=N/4))

## Four equal arms
dat1$Z4arms <- complete_ra(N,m_each=rep(N/4,4))

table(Z2armEqual=dat1$Z2armEqual)
table(Z2armUnequalA=dat1$Z2armUnequalA)
table(Z2armUnequalB=dat1$Z2armUnequalB)
table(Z4arms=dat1$Z4arms)
```

## Factorial designs

It's possible to test the effects of more than one intervention while losing less statistical power by randomly assigning multiple treatments independently of each other. The simplest design that we use for this purpose is the $2 \times 2$ **factorial** design. For example, in the next table we see that we have assigned `r N/2` observations to each arm of two separate interventions. Since the randomization of `treatment1` is independent of `treatment2`, we can assess the effects of each treatment separately and pay less of a power penalty (unless one of the treatments dramatically increases the variance of the outcome compared to a hypothetical experiment with only one treatment assigned).

::: {.tab} 
<button class="tablinks" onclick="unrolltab(event, 'ch4R3')">R code</button>
<button class="tablinks" onclick="unrolltab(event, 'ch4Stata3')">Stata code</button>
<button class="tablinks" onclick="unrolltab(event, 'ch4Hide3')">Hide</button>
::: {#ch4R3 .tabcontent} 
<br />
```{r, highlight = F, eval = F}
## Two equal arms, adding a second cross treatment
dat1$Z2armEqual2 <- labelTC(complete_ra(N))
table(treatment1=dat1$Z2armEqual,treatment2=dat1$Z2armEqual2)
```
:::
::: {#ch4Stata3 .tabcontent} 
<br />
```{stata, highlight = F, eval = F}
** Two equal arms, adding a second cross treatment
complete_ra z2armEqual2
label var z2armEqual2 "Treatment 2"
label var z2armEqual "Treatment 1"
label val z2armEqual2 tc
table z2armEqual z2armEqual2
```
::: 
::: {#ch4Hide3 .tabcontent}
::: 
:::

```{r factdesign2x2, eval = T, echo = F}
## Two equal arms, adding a second cross treatment
dat1$Z2armEqual2 <- labelTC(complete_ra(N))
table(treatment1=dat1$Z2armEqual,treatment2=dat1$Z2armEqual2)
```

Although factorial designs allow us to test more than one intervention at the
same time, they may not provide the same degree of power when testing
hypotheses about the *interaction* between the two treatments. If we want to
learn about how two different interventions work together, then the sample
size requirements will be much larger than if we were satisfied with learning about each treatment separately.^[But see @small2011structured, for instance, for a way to increase the power of tests for such questions. This approach is not currently part of our standard practice.]

Importantly, [recent work](https://direct.mit.edu/rest/article-abstract/doi/10.1162/rest_a_01317/115272/Factorial-Designs-Model-Selection-and-Incorrect?redirectedFrom=fulltext) highlights some important concerns regarding (1) the consequences of omitting interaction terms when estimating separate effects of each treatment and (2) the interpretation of factorial treatment effects [@muralidharan2023factorial]. First, on (1), even if the interaction between treatments is not of academic or policy relevance, including it in the estimation model may be important for making correct inferences. Specifically, if the true interaction effect is not zero, excluding it from the model could increase the risk of Type I errors (i.e., false positives).

Meanwhile, on (2), consider a two-arm factorial design with two Treatments, A and B, with 25% of the sample is in each treatment condition. An estimated effect of Treatment A from a model without an interaction should be interpreted as a weighted average of the effects of A across two subsamples: those receiving B (50%), and those not receiving B (50%). This weighted average treatment effect may or may not provide useful information about the likely effects of Treatment A if it is scaled up later.^[As the authors discuss, this could be framed as either an internal or external validity concern.] For instance, there may be a substantial interaction with treatment B, which is rarely administered in reality and which will not be scaled up alongside A. The subgroup effect of A among "not B" is then more policy relevant, but the subgroup effect of A among "receiving B" pulls the overall estimated average effect of A away from it.

To deal with those issues, the OES Methods Team recommends estimating treatment effects in factorial experiments using a model that includes an interaction, at least as a robustness check.

## Block random assignment

### The benefits of blocking

Statistical power depends not only on the size of the experiment and the strength of the treatment effect, but also on the amount of "noise" (non-treatment-related variability) in the outcome measure. Block-randomized designs can help reduce this noise while simultaneously minimizing estimation error---the amount that our particular experiment's estimate differs from the truth.  

In a **block-randomized**, or stratified, design, we randomly assign units to the policy intervention *within* pre-specified groups.^[Some studies instead employ "post-stratification," where simple or complete randomization might be used to assign treatment, but the data are still analyzed *as-if they were blocked*. See @miratrix2013adjusting, for instance, for more discussion of this strategy. Post-stratification is more likely than blocking to increase variance (relative to a simple difference in means) when there is not actually any between-strata variation in potential outcomes. Post-stratification with too many strata may also have variance costs, which is again less likely when treatment assignment is blocked.] Suppose we are evaluating whether dedicated navigators can increase the percentage of students living in public housing who complete federal financial aid applications (FAFSA). Our experiment will send navigators to two of four eligible buildings, two of which are large and two of which are small. In a real study we can never know the outcome in all buildings both with and without navigators (the "fundamental problem of causal inference" from the last chapter). But if we could, we might have the data below:

```{r maketab1, echo = F}
tibble(
  Building = c(1:4, "Mean"),
  Size = c(rep.int("Large", 2), rep.int("Small", 2), " "),
  `% FAFSA (No Navigator)` = c(20, 30, 20, 30, 25),
  `% FAFSA (With Navigator)` = c(60, 70, 30, 40, 50)
  ) %>%
  knitr::kable() %>%
  kableExtra::row_spec(4, extra_css = "border-bottom: 1px solid") %>%
  kableExtra::row_spec(5, extra_css = "border-bottom: 1px solid") %>%
  kable_styling(full_width = F)

# This LaTex code was used previously
# \begin{center}
# \begin{tabular}{ccccc}
# &&& FAFSA \% & FAFSA \% \\
# Building & Size & Navigators? & if No Navigators & if Navigators   \\ \hline
# 1 & Large & $\_$ & 20 & 60 \\
# 2 & Large & $\_$ & 30 & 70 \\
# 3 & Small & $\_$ & 20 & 30 \\
# 4 & Small & $\_$ & 30 & 40 \\ \hline
# && Means: & 25 & 50 \\
# \end{tabular}
# \end{center}
```

The true average treatment effect for this sample is the average under treatment (i.e., the average treated potential outcome) minus the average under control (i.e., the average control potential outcome): $\text{ATE} = 50-25=25$ percent more applications per building when a navigator is deployed.

In a real study, we might randomly allocate two buildings to treatment and two buildings to control. If complete random assignment led to us treating the first two buildings, then we might observe:

```{r maketab2, echo = F}
tibble(
  Building = c(1:4, "Mean"),
  Size = c(rep.int("Large", 2), rep.int("Small", 2), " "),
  Treated = c(1, 1, 0, 0, " "),
  `% FAFSA (No Navigator)` = c(" ", " ", 20, 30, 25),
  `% FAFSA (With Navigator)` = c(60, 70, " ", " ", 65)
  ) %>%
  knitr::kable() %>%
  kableExtra::row_spec(4, extra_css = "border-bottom: 1px solid") %>%
  kableExtra::row_spec(5, extra_css = "border-bottom: 1px solid") %>%
  kable_styling(full_width = F)

# This LaTex code was used previously
# \begin{center}
# \begin{tabular}{ccccc}
# &&& FAFSA \% & FAFSA \% \\
# Building & Size & Navigators? & if No Navigators & if Navigators   \\ \hline
# 1 & Large & Yes &  & 60 \\
# 2 & Large & Yes &  & 70 \\
# 3 & Small & No & 20 &  \\
# 4 & Small & No & 30 &  \\ \hline
# && Means: & 25 & 65 \\
# \end{tabular}
# \end{center}
```

This yields a treatment effect estimate of $65-25 = 40$ percent more applications due to the presence of a navigator. This is *larger* than the true value of $25$.

Or, if random assignment led to the other two buildings being treated instead, we might then observe:

```{r maketab3, echo = F}
tibble(
  Building = c(1:4, "Mean"),
  Size = c(rep.int("Large", 2), rep.int("Small", 2), " "),
  Treated = c(0, 0, 1, 1, " "),
  `% FAFSA (No Navigator)` = c(20, 30, " ", " ", 25),
  `% FAFSA (With Navigator)` = c(" ", " ", 30, 40, 35)
  ) %>%
  knitr::kable() %>%
  kableExtra::row_spec(4, extra_css = "border-bottom: 1px solid") %>%
  kableExtra::row_spec(5, extra_css = "border-bottom: 1px solid") %>%
  kable_styling(full_width = F)

# This LaTex code was used previously
# \begin{center}
# \begin{tabular}{ccccc}
# &&& FAFSA \% & FAFSA \% \\
# Building & Size & Navigators? & if No Navigators & if Navigators   \\ \hline
# 1 & Large & $\_$ &  & 60 \\
# 2 & Large & $\_$ & 30 &  \\
# 3 & Small & $\_$ &  & 30 \\
# 4 & Small & $\_$ & 30 &  \\ \hline
# && Means: & 30 & 45 \\
# \end{tabular}
# \end{center}
```

This, in contrast, yields an estimated treatment effect of $35-25 = 10$ percentage point more applications due to the navigators -- now *smaller* than the true value of $25$.

All of the possible (equiprobable) assignments with two treated and two control units, along with their estimated treatment effects, are listed in the table below:

```{r maketab4, echo = F}
tibble(
  Assignments = c("TTCC", "CTCT", "TCCT", "CTTC", "TCTC", "CCTT"),
 `Estimated Effect` = c(40, 35, 25, 25, 15, 10)
  ) %>%
  knitr::kable() %>%
  kable_styling(full_width = F)
# This LaTex code was used previously
# \begin{center}
# \begin{tabular}{cc}
# Assignments & Est TE \\ \hline
# YYNN & 40  \\ 
# NYNY & 35 \\
# YNNY & 25 \\
# NYYN & 25 \\
# YNYN & 15 \\
# NNYY & 10
# \end{tabular}
# \end{center}
```

These possible treatment effect estimates have a mean equal to the true value of 25, illustrating the difference in means is an unbiased estimator. However, some of these estimates are far from the truth, and they have a lot of variability.

To design an experiment that better estimates the true value, and does so with more statistical power (less variability), we can randomly assign units within *blocks*. In general, units should be sorted into different blocks based on their similarity across one or more characteristics that we expect to be correlated with our outcome. Here, blocking implies restricting the possible random assignments to those that have one large and one small building in each treatment group:

```{r maketab5, echo = F}
tibble(
  Assignments = c("CTCT", "TCCT", "CTTC", "TCTC"),
 `Estimated Effect` = c(35, 25, 25, 15)
  ) %>%
  knitr::kable() %>%
  kable_styling(full_width = F)
# This LaTex code was used previously
# \begin{center}
# \begin{tabular}{cc}
# Assignments & Est TE \\ \hline
# NYNY & 35 \\
# YNNY & 25 \\
# NYYN & 25 \\
# YNYN & 15 \\
# \end{tabular}
# \end{center}
```

With this blocked design restricting the random assignments that are possible, we now get an estimate that is no more than 10 percentage points from the truth. Further, our estimates will have less variability (an SD of `r round(sd(c(35, 25, 25, 15)), 2)` rather than `r round(sd(c(35, 25, 25, 15, 40, 10)), 2)`). This improves the statistical power of our design.

For a more realistic example, suppose we are designing an experiment
where the sample includes patients from two different hospitals. We might randomly assign patients to treatment and control *within* each hospital. For instance, we might assign half of the patients in hospital "A" to treatment and half to control, then do the same in hospital "B."^[We use "block" rather than "strata" throughout this document to refer to groups of experimental units that are fixed *before random assignment occurs*.] Below, we have `r nrow(dat1) / 2` units in hospital "A" and `r nrow(dat1) / 2` in hospital "B":

::: {.tab} 
<button class="tablinks" onclick="unrolltab(event, 'ch4R4')">R code</button>
<button class="tablinks" onclick="unrolltab(event, 'ch4Stata4')">Stata code</button>
<button class="tablinks" onclick="unrolltab(event, 'ch4Hide4')">Hide</button>
::: {#ch4R4 .tabcontent} 
<br />
```{r, highlight = F, eval = F}
dat1$blockID <- gl(n = 2, k = N/2, labels = c("Block A", "Block B"))
with(dat1,table(blockID=dat1$blockID))
```
:::
::: {#ch4Stata4 .tabcontent} 
<br />
```{stata, highlight = F, eval = F}
local Anum = $N/2
gen blockID = .
tempvar rand
gen `rand' = runiform()
sort `rand'
replace blockID = 1 in 1/`Anum'
replace blockID = 2 if missing(blockID)
label define blocklab 1 "Block A" 2 "Block B"
label values blockID blocklab
table blockID
```
::: 
::: {#ch4Hide4 .tabcontent}
::: 
:::

```{r blockra1, eval = T, echo = F}
dat1$blockID <- gl(n = 2, k = N/2, labels = c("Block A", "Block B"))
with(dat1,table(blockID=dat1$blockID))
```

We assign half of the units in each hospital to each treatment condition:

::: {.tab} 
<button class="tablinks" onclick="unrolltab(event, 'ch4R5')">R code</button>
<button class="tablinks" onclick="unrolltab(event, 'ch4Stata5')">Stata code</button>
<button class="tablinks" onclick="unrolltab(event, 'ch4Hide5')">Hide</button>
::: {#ch4R5 .tabcontent} 
<br />
```{r, highlight = F, eval = F}
dat1$Z2armBlocked <- labelTC(block_ra(blocks=dat1$blockID))
with(dat1, table(blockID, Z2armBlocked))
```
:::
::: {#ch4Stata5 .tabcontent} 
<br />
```{stata, highlight = F, eval = F}
block_ra z2armBlocked, block_var(blockID) replace
label val z2armBlocked tc
table blockID z2armBlocked
capture drop __00* // Clean up block_var temporary var
```
::: 
::: {#ch4Hide5 .tabcontent}
::: 
:::

```{r blockra15, echo = F, eval = T}
dat1$Z2armBlocked <- labelTC(block_ra(blocks=dat1$blockID))
with(dat1, table(blockID, Z2armBlocked))
```

If, say, there were fewer people eligible for treatment in hospital "A" --- or perhaps the intervention was more expensive in that block --- we might choose different treatment probabilities for each block. The code below assigns half of the hospital "A" patients to treatment, but only a quarter of those from hospital "B". Again, we also check that this code worked. This approach is an informal version of one of the best practices for writing code in general, called "unit testing." See the [EGAP Guide to Workflow](https://egap.org/resource/10-things-to-know-about-project-workflow/) for more examples.

::: {.tab} 
<button class="tablinks" onclick="unrolltab(event, 'ch4R6')">R code</button>
<button class="tablinks" onclick="unrolltab(event, 'ch4Stata6')">Stata code</button>
<button class="tablinks" onclick="unrolltab(event, 'ch4Hide6')">Hide</button>
::: {#ch4R6 .tabcontent} 
<br />
```{r, highlight = F, eval = F}
## Blocked assignment, unequal probability
dat1$Z2armBlockedUneqProb <- labelTC(block_ra(blocks=dat1$blockID, block_prob=c(.5,.25)))
with(dat1, table(blockID, Z2armBlockedUneqProb))

## Unit testing
NumTreatedB <- sum(dat1$Z2armBlockedUneqProb=="T" & dat1$blockID=="Block B")
ExpectedNumTreatedB <- sum(dat1$blockID=="Block B")/4
stopifnot(NumTreatedB==ceiling(ExpectedNumTreatedB) | NumTreatedB==floor(ExpectedNumTreatedB))
```
:::
::: {#ch4Stata6 .tabcontent} 
<br />
```{stata, highlight = F, eval = F}
** Blocked assignment, unequal probability
block_ra z2armBlockedUneqProb, block_var(blockID) block_prob(0.5 0.25) replace
label val z2armBlockedUneqProb tc
table blockID z2armBlockedUneqProb
capture drop __00* // Clean up block_var temporary var

** Unit Testing
qui count if z2armBlockedUneqProb == 1 & blockID == 2
global NumTreatedB = `r(N)'
qui count if blockID == 2
global ExpectedNumTreatedB = `r(N)'/4
assert ($NumTreatedB == ceil($ExpectedNumTreatedB)) | ($NumTreatedB == floor($ExpectedNumTreatedB))
```
::: 
::: {#ch4Hide6 .tabcontent}
::: 
:::

```{r blockra2, echo = F, eval = T, results = "hold", collapse=T}
## Blocked assignment, unequal probability
dat1$Z2armBlockedUneqProb <- labelTC(block_ra(blocks=dat1$blockID, block_prob=c(.5,.25)))
with(dat1, table(blockID, Z2armBlockedUneqProb))

## Unit testing
NumTreatedB <- sum(dat1$Z2armBlockedUneqProb=="T" & dat1$blockID=="Block B")
ExpectedNumTreatedB <- sum(dat1$blockID=="Block B")/4
stopifnot(NumTreatedB==ceiling(ExpectedNumTreatedB) | NumTreatedB==floor(ExpectedNumTreatedB))
```

Our team tries to implement block-randomized assignment whenever possible in
order to increase the statistical power of our experiments. We also often find
it useful in cases where different administrative units are implementing the
treatment, or when we expect different groups of people to have different reactions to the treatment.

### Using a few covariates to create blocks

If we have background information on a few covariates, we can create blocks by hand through a process like the one demonstrated here:

::: {.tab} 
<button class="tablinks" onclick="unrolltab(event, 'ch4R7')">R code</button>
<button class="tablinks" onclick="unrolltab(event, 'ch4Stata7')">Stata code</button>
<button class="tablinks" onclick="unrolltab(event, 'ch4Hide7')">Hide</button>
::: {#ch4R7 .tabcontent} 
<br />
```{r, highlight = F, eval = F}
## For example, make three groups from the cov2 variable
dat1$cov2cat <- with(dat1, cut(cov2, breaks=3))
table(dat1$cov2cat, exclude=c())
with(dat1, tapply(cov2, cov2cat, summary))

## And we can make blocks that are the same on two covariates
dat1$cov1bin <- as.numeric(dat1$cov1>median(dat1$cov1)) # Binarize cov1
dat1$blockV2 <- droplevels(with(dat1, interaction(cov1bin, cov2cat)))
table(dat1$blockV2, exclude=c())

## And then assign within these blocks
set.seed(12345)
dat1$ZblockV2 <- labelTC(block_ra(blocks = dat1$blockV2))
with(dat1, table(blockV2, ZblockV2, exclude=c()))
```
:::
::: {#ch4Stata7 .tabcontent} 
<br />
```{stata, highlight = F, eval = F}
** For example, make three groups from the cov2 variable
egen cov2cat = cut(cov2), group(3) label
table cov2cat
bysort cov2cat : sum cov2 // Divides into intervals differently from R

** And we can make blocks that are the same on two covariates
qui sum cov1, d
gen cov1bin = cond(cov1 > r(p50), 1, 0) // Similar to ifelse() in R
decode cov2cat, generate(string_cov2cat)
gen blockV2 = string(cov1bin) + " " + string_cov2cat
table blockV2

** And then assign within these blocks
set seed 12345
block_ra zblockV2, block_var(blockV2)
label val zblockV2 tc
table blockV2 zblockV2
capture drop __00* // Clean up
```
::: 
::: {#ch4Hide7 .tabcontent}
::: 
:::

```{r cutcovs, eval = T, echo = F, collapse=T, results = "hold"}
## For example, make three groups from the cov2 variable
dat1$cov2cat <- with(dat1, cut(cov2, breaks=3))
table(dat1$cov2cat, exclude=c())
with(dat1, tapply(cov2, cov2cat, summary))

## And we can make blocks that are the same on two covariates
dat1$cov1bin <- as.numeric(dat1$cov1>median(dat1$cov1)) # Binarize cov1
dat1$blockV2 <- droplevels(with(dat1, interaction(cov1bin, cov2cat)))
table(dat1$blockV2, exclude=c())

## And then assign within these blocks
set.seed(12345)
dat1$ZblockV2 <- labelTC(block_ra(blocks = dat1$blockV2))
with(dat1, table(blockV2, ZblockV2, exclude=c()))
```

### Blocking using many covariates

If instead we have many background variables, we can increase precision by thinking about blocking as a problem of "matching," or creating sets of
units which are as similar as possible across the entire set of covariates [@moore2012multivariate;@moore2016bT063]. Here we show two approaches using R.

Creating pairs:

```{r}
## Using the blockTools package
mvblocks <- block(dat1, id.vars="id", block.vars=c("cov1","cov2"), algorithm="optimal")
dat1$blocksV3 <- createBlockIDs(mvblocks, data=dat1, id.var = "id")
dat1$ZblockV3 <- labelTC(block_ra(blocks = dat1$blocksV3))

## Just show the first ten pairs
with(dat1,table(blocksV3,ZblockV3,exclude=c()))[1:10,]
```

Creating larger blocks:

```{r}
## Using the quickblock package
distmat <- distances(dat1, dist_variables = c("cov1bin", "cov2"), id_variable = "id", normalize="mahalanobiz")
distmat[1:5,1:5]
quantile(as.vector(distmat), seq(0,1,.1))

## The caliper argument helps prevent ill-matched points
mvbigblock <- quickblock(distmat, size_constraint = 6L, caliper = 2.5)

## Look for missing points
table(mvbigblock,exclude=c()) # One point dropped due to caliper
dat1$blocksV4 <- mvbigblock
dat1$notblocked <- is.na(dat1$blocksV4) 
dat1$ZblockV4[dat1$notblocked==F] <- labelTC(block_ra(blocks = dat1$blocksV4))
with(dat1, table(blocksV4, ZblockV4, exclude=c()))[1:10,]
```

It's worth pausing to examine the differences within blocks. We'll focus on the proportion of people in category "1" on the binary covariate (notice that the blocks are homogeneous on this covariate), as well as the difference between the largest and smallest value of the continuous covariate. This table also illustrates that, due to our use of a caliper when calling `quickblock` above, one observation was not included in treatment assignment.

```{r blockingeval}
blockingDescEval <- dat1 %>% 
  group_by(blocksV4) %>%
  summarize(
    cov2diff = max(abs(cov2)) - min(abs(cov2)),
    cov1 = mean(cov1bin),
    count_in_block = n()
    )

blockingDescEval
```

### Disadvantages

Block-randomized assignment and analysis can help reduce both estimation error and non-treatment-related variability in our data. It may also be useful for ensuring equal distribution of treatment arms within less common subgroups in our sample, which can be especially important for preserving our statistical power when estimating heterogenous treatment effects.

However, there are some practical disadvantages to consider. Block-randomized assignment can make treatment administration more complicated, both in terms of implementation by our partners and determining how we should incorporate blocking into our estimation strategy. Especially when a project needs to be rolled out on a short timeline, including a more complex blocking scheme might not be realistic.

It may often be reasonable to conclude that the benefits of blocking are not worth the extra effort it entails. Adjusting for prognostic (i.e., correlated with $Y$) covariates during the analysis stage may provide sufficient improvements in precision, especially in cases where we expect a design to already be reasonably powered under simple or complete random assignment or where we do not expect subgroup analyses (particularly for rare subgroups) to be a key component of an evaluation. But note that in smaller samples without block-randomized assignment, there might be a relatively greater risk of increasing variance by, e.g., adjusting for non-prognostic covariates [@miratrix2013adjusting].

## Cluster random assignment

We often implement a new policy intervention at the level of some group of people --- like a doctor's practice, or a building, or some other administrative unit. Even though we have `r nrow(dat1)` units in our example data, imagine now that they are grouped into 10 buildings, and the policy intervention is at the building level. Below, we assign `r nrow(dat1)/2` of those units to treatment and `r nrow(dat1)/2` to control. Everyone in each building has the same treatment assignment.

::: {.tab} 
<button class="tablinks" onclick="unrolltab(event, 'ch4R8')">R code</button>
<button class="tablinks" onclick="unrolltab(event, 'ch4Stata8')">Stata code</button>
<button class="tablinks" onclick="unrolltab(event, 'ch4Hide8')">Hide</button>
::: {#ch4R8 .tabcontent} 
<br />
```{r, highlight = F, eval = F}
## Make an indicator for cluster membership
ndat1 <- nrow(dat1)
dat1$buildingID <- rep(1:(ndat1/10), length=ndat1)
set.seed(12345)
dat1$Zcluster <- labelTC(cluster_ra(cluster=dat1$buildingID))
with(dat1, table(Zcluster, buildingID))
```
:::
::: {#ch4Stata8 .tabcontent} 
<br />
```{stata, highlight = F, eval = F}
** Make an indicator for cluster membership
qui count
global ndat1 = r(N)
egen buildingID = seq(), from(1) to(10) block(1)
set seed 12345
cluster_ra zcluster, cluster_var(buildingID)
table zcluster buildingID
```
::: 
::: {#ch4Hide8 .tabcontent}
::: 
:::

```{r clusterRA, eval = T, echo = F}
## Make an indicator for cluster membership
ndat1 <- nrow(dat1)
dat1$buildingID <- rep(1:(ndat1/10), length=ndat1)
set.seed(12345)
dat1$Zcluster <- labelTC(cluster_ra(cluster=dat1$buildingID))
with(dat1, table(Zcluster, buildingID))
```

Cluster randomized designs raise new questions about estimation, testing, and statistical power. We describe our approaches to estimation and power
analysis of cluster randomized designs in the chapter on analysis decisions.

## Other randomized designs

In the past, our team has also employed stepped-wedge style designs, saturation designs aimed at discovering whether the effects of the experimental intervention are communicated across people (via some spillover or network mechanism), and
designs where we try to isolate certain experimental units (like buildings)
from each other so that we can focus our learning about the effects of the
intervention in isolation (rather than the effects when people can communicate with each other about the intervention). There are a variety of more specialized randomization designs that may be appropriate for particular projects, and the options discussed above should not be treated as exhaustive. We may expand on some of these other randomization options here in the future.

## As-if random assignment

In some circumstances, we might judge that randomly assigning a treatment of interest would be logistically infeasible. There are a variety of methods we have applied in such cases in the past to ensure assignment to treatment is at least idiosyncratic or arbitrary. What is key here is not really that assignment is random, *per se*. Instead, assignment must be conditionally independent of a unit's potential outcomes [@holland:1986a]. Random assignment is simply the best way of guaranteeing this. But sometimes, there are available methods of assigning treatment non-randomly that we decide are likely to satisfy this condition. Of course, relying on any as-if random assignment procedure makes it especially important to review evidence of appropriate treatment administration afterwards (see the next section).

We list a few examples of as-if random assignment procedures below, all of which link to Analysis Plans pre-registered on the OES website. But note that an as-if random procedure that satisfies conditional independence in one study will not necessarily satisfy it in another. This needs to be determined on a case-by-case basis. When possible, it may be ideal to layer two arbitrary assignment procedures on top of each other rather than rely on the plausibility of only one.

* Grouping people into partitions based on the last two digits of their SSN and then [rotating each partition's treatment assignment monthly](https://oes.gsa.gov/assets/analysis/1902-analysis-plan.pdf)

* Assigning program applicants to different conditions based on the [last digits of their submission time and submission day](https://oes.gsa.gov/assets/analysis/2310-decreasing-SNAP-denial-rates_analysis-plan.pdf) (are both even, both odd, or is it mixed?)

* Assigning callers to different conditions [based on the last four digits of their phone number](https://oes.gsa.gov/assets/analysis/2309-decreasing-abandonment-of-calls-to-988-analysis-plan.pdf)

## Assessing randomization (balance testing)

If we have covariates, we can evaluate the implementation of a random assignment procedure by exploring covariate differences across treatment arms.^[If there are no available covariates, we can at least evaluate whether the distribution of treated units appears consistent with our intended strategy.] The goal is to ensure that the distribution of covariates appears consistent with our intended randomization strategy [@imai2008misunderstandings]. Since the intention is to draw conclusions about the realized sample and not to make inferences about some broader population, it is important to think carefully about what we learn from significance tests when assessing balance.

### Separate tests for each covariate

To see the issues significance testing raises here, consider the common practice of performing separate difference-in-means tests for each of many different covariates. *First*, this approach raises "multiple testing" concerns (see Chapter 5). Briefly, it would be easy to discover one or a few covariates with noticeable mean differences due to random chance rather than real implementation problems. That is, in a well-operating experiment, we would expect some baseline imbalances for individual covariates---roughly 5 in 100. *Second*, the relationship between sample size and significance is important. If a sample is too small, large imbalances (suggesting problems with treatment administration) may still be statistically insignificant: the balance test is too underpowered to be informative. On the other hand, when working with large samples, negligible mean differences may still be significant, leading to spurious conclusions of imbalance.

When comparing individual covariates across treatment arms, one strategy for dealing with the large-sample issue is to rely on equivalence tests, such as the "Two One-Sided Test" (TOST) procedure [@hartman2018equivalence; @rainey2014arguing].^[This is also useful when interpreting "null effects" for a study's primary outcome measure.] Another option is to evaluate statistics that are less sensitive to sample size like standardized mean differences (e.g., Cohen's $d$) and variance ratios. Here, it may be useful for transparency to pre-register what would represent evidence of meaningful treatment administration problems.^[It's sometimes helpful to go further and look at things like Kolmogorov-Smirnov statistics, Q-Q plots, or statistics for various polynomial transformations of continuous covariates.] A common heuristic is that values of Cohen's $d$ greater than around 0.2 or 0.25, or variance ratios greater than 2 and less than 0.5, are more likely to represent a meaningful imbalance [@stuart2010matching]. But these are not hard and fast rules.

There are fewer options for dealing with the small-sample issue above, though procedures employing randomization inference may perform relatively better in small samples [@reichardt1999justifying; @hansen_covariate_2008]. Remember that if the sample is too small to perform a well-powered balance test, it may also be too under-powered for treatment effect estimation.

### Omnibus tests

Instead of performing separate tests for each covariate, a different tactic is to judge whether a sample appears sufficiently incompatible with the joint (or "omnibus") null hypothesis of no average covariate differences between treatment arms. Above all, this helps sidestep the multiple testing concerns noted above. This kind of test is often implemented [in practice](https://blogs.worldbank.org/en/impactevaluations/tools-trade-joint-test-orthogonality-when-testing-balance) by OLS regressing a treatment indicator on a set of covariates and then calculating a p-value based on the model's F-statistic. Failing to find a significant difference in an *omnibus F-test* does not "prove" that there are no imbalances. But finding a statistically significant difference tells you that a closer look at covariate balance is needed.

An omnibus F-statistic can also be calculated using a [Wald test](https://stats.oarc.ucla.edu/other/mult-pkg/faq/general/faqhow-are-the-likelihood-ratio-wald-and-lagrange-multiplier-score-tests-different-andor-similar/). The idea is to compare an *unrestricted* model regressing treatment on the set of covariates with a *restricted* model including only an intercept. The Wald test then provide evidence on the null hypothesis that the unrestricted model does not fit the data appreciably better. This method is useful when estimating balance across covariates while adjusting for block fixed effects. Here, both models should include block fixed effects, and the restricted model should simply drop the covariates (but is no longer be "intercept-only"). The Wald test more easily accommodates modifications like alternate standard errors (e.g.: HC2) than other alternatives like a likelihood ratio (LR) test.

Both of these procedures are normally applied in a sampling-based statistical inference framework. But design-based inference (see Chapter 3) is likely more appropriate for balance-testing. Failing to reject the null hypothesis in a randomization-based setting---where "random error" comes from permuting treatment assignment---implies that observed imbalances are not sufficiently distinguishable from the distribution of imbalance estimates we'd expect in a well-run experiment. Notice that this is exactly what we hope to learn from balance tests. Additionally, randomization-based omnibus tests may control the Type-I error rate (i.e, avoids spurious conclusions that a sample is imbalanced) in small-to-moderate samples better than an omnibus LR test [@kerwin2024striking; @hansen_covariate_2008]. See @hansen_covariate_2008 for more discussion of this issue and an introduction to an alternative omnibus balance statistic $(d^{2})$.

### Summary

For OES randomized evaluations, it is preferred to perform some form of omnibus balance test, and to write out a brief plan for exploring individual imbalances further if this test rejects the null hypothesis. Time-permitting, more extensive balance checks are always valuable.

It can be important for balance testing procedures to account for any clustering or blocking employed when assigning treatment [@hansen_covariate_2008]. For blocking, this implies calculating balance within blocks (and then potentially aggregating across blocks to yield a single overall estimate). For clustering, this may imply exploring whether a comparison of cluster-level aggregates (rather than a comparison of individual units) provides evidence of treatment administration problems. Or it might imply just accounting for within-cluster independence when performing significance tests.

All of those modifications are straightforward when estimating an omnibus balance statistic using a regression model. But they can also be incorporated into covariate-by-covariate statistics. For instance, in a blocked design, it may be reasonable to calculate separate standardized mean differences within each block (dividing within-block differences by the overall sample-level standard deviation) and then take a weighted average across blocks (where block-level estimates are weighted by their share of the overall sample size).

### Coded examples

We provide R and Stata examples of various procedures discussed above. In all cases, we evaluate balance for two covariates---`cov1` and `cov2`---under one of the randomized designs discussed earlier in this chapter: complete randomization (`Z2armEqual`), clustered randomization (`Zcluster`), or blocked randomization (`ZblockV2`). First, we illustrate methods for calculating standardized mean differences (SMDs; mean difference divided by pooled SD) and variance ratios (VRs) for each covariate under different randomization designs. Although there are canned packages in R (e.g., `cobalt`) and Stata (e.g., `tebalance`) that can perform some of these calculations, we show how to do them manually.

```{r, eval = F, echo = F, include = F}
# To use in the stata code
write.csv(dat1, "Stata code/ch4finaldat1.csv", row.names = F)
```

::: {.tab} 
<button class="tablinks" onclick="unrolltab(event, 'ch4R10')">R code</button>
<button class="tablinks" onclick="unrolltab(event, 'ch4Stata10')">Stata code</button>
<button class="tablinks" onclick="unrolltab(event, 'ch4Hide10')">Hide</button>
::: {#ch4R10 .tabcontent} 
<br />
```{r, highlight = F, eval = F}
## List covariates to evaluate
covlist <- c("cov1", "cov2")

## Define convenience function to perform calculations.
## May be applied to the whole dataset, cluster-level aggregates,
## or individual blocks. Assumes only 2 arms. See use below.
bstats_2arm <- function(
  d = dat1, # data to calculate balance within
  n = NULL, # total sample size; only use when d is a subset (blocking)
  tr = "Z2armEqual", # treatment var name
  y = "cov2", # evaluate balance for what covariate?
  psd = sd(dat1$cov2) # what sd to use in SMDs?
  ) {
  
  # Check whether a total sample size was specified.
  # Default is to impute this from d (assume d is not a subset).
  if (is.null(n)) {
    n <- nrow(d)
  }
  
  # Get levels of treatment var, always decreasing order
  lvl <- unique(d[,tr])
  lvl <- sort(lvl, decreasing = T)
  
  # Difference in means
  dm <- mean(d[d[,tr]==lvl[1],y], na.rm = T) - 
    mean(d[d[,tr]==lvl[2],y], na.rm = T)
  
  # Standardize using provided SD
  smd <- dm/psd
  
  # Variance ratio
  vr <- var(d[d[,tr]==lvl[1],y], na.rm = T) / 
    var(d[d[,tr]==lvl[2],y], na.rm = T)
  
  # Proportion of total sample in subset
  # (1 if n was not set)
  prop <- nrow(d)/n
  
  # Prepare output
  out <- list(
    diff_means = dm,
    smd = smd,
    vr = vr,
    prop = prop
    )
  
  return(out)
  
}

## Iterate through this covariate list, building a table
btab <- data.frame(var = covlist)
for (cov in covlist) {

  # approach 1: overall mean differences and SMDs
  sel <- btab$var == cov
  pooled <- sd(dat1[,cov])
  overall <- bstats_2arm(y = cov, psd = pooled)
  btab$diff[sel] <- overall$diff_means
  btab$smd[sel] <- overall$smd
  btab$vratio[sel] <- overall$vr 
  
  # approach 2: cluster-level mean differences and SMD.
  # approach 1 might be applied for clustered designs as well.
  by_groups <- list(dat1$buildingID, dat1$Zcluster)
  clust <- aggregate(dat1[,cov], by = by_groups, mean)
  clust <- bstats_2arm(d = clust, tr = "Group.2", y = "x", psd = pooled)
  btab$cluster_diff[sel] <- clust$diff_means
  btab$cluster_smd[sel] <- clust$smd
  btab$cluster_vratio[sel] <- clust$vr
  
  # approach 3: weighted avg of in-block mean differences and SMDs.
  blocked <- by(
    data = dat1, 
    INDICES = dat1$blockV2,
    FUN = function(x) {
      bstats_2arm(x, nrow(dat1), "ZblockV2", cov, pooled)
      },
    simplify = F
    )
  smd_block <- sapply(blocked, function(x) x$smd)
  dm_block <- sapply(blocked, function(x) x$diff_means)
  vr_block <- sapply(blocked, function(x) x$vr)
  prop_block <- sapply(blocked, function(x) x$prop)
  btab$block_diff[sel] <- weighted.mean(dm_block,prop_block)
  btab$block_smd[sel] <- weighted.mean(smd_block,prop_block)
  btab$block_vratio[sel] <- weighted.mean(vr_block,prop_block)
  
}

## View output
btab[, c(1, 3:4, 6:7, 9:10)]
```
:::
::: {#ch4Stata10 .tabcontent} 
<br />
```{stata, highlight = F, eval = F}
** List covariates to evaluate
global covlist cov1 cov2

** Define convenience program to perform calculations.
** May be applied to the whole dataset, cluster-level aggregates,
** or individual blocks. Assumes only 2 arms. See use below.
capture program drop bstats_2arm
program define bstats_2arm, rclass sortpreserve byable(onecall)

	* varname = evaluate balance for what covariate?
	* tr = treatment var name
	* n = total sample size; only use when applied to a subset (Blocks)
	syntax varname [if] [in], tr(varname) ///
		[ n(string) psd(string) ttestopt(string) ]
	
	* Get list of by vars, passed from "by varlist:" or "bysort varlist:"
	local by "`_byvars'"
	
	* Identify the correct sample to use (if/in)
	marksample touse
	
	* Further by var setup.
	* Confirm something was passed.
	capture confirm variable `by'
	
	* If not, treat all obs as in same group.
	if _rc != 0 {
		tempvar group
		qui gen `group' = 1 if `touse'	
	}
	
	* If something was, set this up as a grouping var.
	else {		
		tempvar group
		qui egen `group' = group(`by') if `touse'		
	}
	
	* In either case, get the levels as a macro.
	qui levelsof `group' if `touse', local(by_levels)
	local len : word count `by_levels'
	
	* Make rownames, used below
	local rownames
	foreach l of local by_levels {
		local rownames `rownames' "`l'"
	}
	
	* Get n as sample size in memory, if not specified
	if "`n'"=="" {
		qui count if `touse'
		local n = r(N)
	}
	
	* Get psd as pooled sd of varname, if not specified.
	* Though varname indicated above, still mapped to `varlist'.
	if "`psd'"=="" {
		qui sum `varlist' if `touse', d
		local psd = r(sd)
	}

	* Loop through those levels, calculating
	* desired stats and saving in a matrix.
	matrix stats = J(`len', 5, .)
	matrix rownames stats = `rownames'
	matrix colnames stats = "dm" "smd" "vr" "prop" "group_n"
	local i = 0
	foreach l of local by_levels {
		
		* Update index
		local ++i
		
		* Difference in means (treatment level 2 is greater value)
		qui ttest `varlist' if `touse' & `group' == `l', by(`tr') `ttestopt'
		local dm = r(mu_2) - r(mu_1)
		matrix stats[`i', 1] = `dm'
		
		* Standardize using provided SD
		local smd = `dm'/`psd'
		matrix stats[`i', 2] = `smd'
		
		* Variance ratio
		qui ttest `varlist' if `touse' & `group' == `l', by(`tr') `ttestopt'
		local vr = (r(sd_2)^2)/(r(sd_1)^2)
		matrix stats[`i', 3] = `vr'
		
		* Proportion of total sample in subset
		* (1 if n was not set)
		qui count if `touse' & `group' == `l'
		local prop = r(N)/`n'
		local group_n = r(N)
		matrix stats[`i', 4] = `prop'
		matrix stats[`i', 5] = `group_n'
			
	}
	
	* Prepare output
	return matrix stats = stats

end

** Iterate through this covariate list, building a table
local clen : word count $covlist
matrix btab = J(`clen', 6, .)
local rownames
foreach g of global covlist {
	local rownames `rownames' "`g'"
}
matrix rownames btab = `rownames'
matrix colnames btab = "smd" "vratio" "cluster_smd" "cluster_vratio" "block_smd" "block_vratio"
local k = 0
foreach var of varlist $covlist {
	
	* approach 1: overall mean differences and SMDs
	local ++k
	qui sum `var', d
	local pooled = r(sd)
	bstats_2arm `var', tr(z2armequal) psd(`pooled')
	matrix btab[`k', 1] = r(stats)[1,"smd"]
	matrix btab[`k', 2] = r(stats)[1,"vr"]
	
	* approach 2: cluster-level mean differences and SMD.
	* approach 1 might be applied for clustered designs as well.
	preserve
		qui collapse (mean) cov1 cov2 (first) zcluster, by(buildingid)
		bstats_2arm `var', tr(zcluster) psd(`pooled') ttestopt("reverse")
	restore
	matrix btab[`k', 3] = r(stats)[1,"smd"]
	matrix btab[`k', 4] = r(stats)[1,"vr"]
	
	* approach 3: weighted avg of in-block mean differences and SMDs.
	preserve
		bysort blockv2: bstats_2arm `var', tr(zblockv2) psd(`pooled') n(100)
		qui clear
		qui svmat r(stats), names(col)
		qui sum smd [iw=prop]
		local blocked_smd = r(mean)
		qui sum vr [iw=prop]
		local blocked_vr = r(mean)
	restore
	matrix btab[`k', 5] = `blocked_smd'
	matrix btab[`k', 6] = `blocked_vr'
	
}

** View output
matrix list btab
```
::: 
::: {#ch4Hide10 .tabcontent}
::: 
:::

```{r cbycbalance, eval = T, echo = F}
## List covariates to evaluate
covlist <- c("cov1", "cov2")

## Define convenience function to perform calculations.
## May be applied to the whole dataset, cluster-level aggregates,
## or individual blocks. Assumes only 2 arms. See use below.
bstats_2arm <- function(
  d = dat1, # data to calculate balance within
  n = NULL, # total sample size; only use when d is a subset (blocking)
  tr = "Z2armEqual", # treatment var name
  y = "cov2", # evaluate balance for what covariate?
  psd = sd(dat1$cov2) # what sd to use in SMDs?
  ) {
  
  # Check whether a total sample size was specified.
  # Default is to impute this from d (assume d is not a subset).
  if (is.null(n)) {
    n <- nrow(d)
  }
  
  # Get levels of treatment var, always decreasing order
  lvl <- unique(d[,tr])
  lvl <- sort(lvl, decreasing = T)
  
  # Difference in means
  dm <- mean(d[d[,tr]==lvl[1],y], na.rm = T) - 
    mean(d[d[,tr]==lvl[2],y], na.rm = T)
  
  # Standardize using provided SD
  smd <- dm/psd
  
  # Variance ratio
  vr <- var(d[d[,tr]==lvl[1],y], na.rm = T) / 
    var(d[d[,tr]==lvl[2],y], na.rm = T)
  
  # Proportion of total sample in subset
  # (1 if n was not set)
  prop <- nrow(d)/n
  
  # Prepare output
  out <- list(
    diff_means = dm,
    smd = smd,
    vr = vr,
    prop = prop
    )
  
  return(out)
  
}

## Iterate through this covariate list, building a table
btab <- data.frame(var = covlist)
for (cov in covlist) {

  # approach 1: overall mean differences and SMDs
  sel <- btab$var == cov
  pooled <- sd(dat1[,cov])
  overall <- bstats_2arm(y = cov, psd = pooled)
  btab$diff[sel] <- overall$diff_means
  btab$smd[sel] <- overall$smd
  btab$vratio[sel] <- overall$vr 
  
  # approach 2: cluster-level mean differences and SMD.
  # approach 1 might be applied for clustered designs as well.
  by_groups <- list(dat1$buildingID, dat1$Zcluster)
  clust <- aggregate(dat1[,cov], by = by_groups, mean)
  clust <- bstats_2arm(d = clust, tr = "Group.2", y = "x", psd = pooled)
  btab$cluster_diff[sel] <- clust$diff_means
  btab$cluster_smd[sel] <- clust$smd
  btab$cluster_vratio[sel] <- clust$vr
  
  # approach 3: weighted avg of in-block mean differences and SMDs.
  blocked <- by(
    data = dat1, 
    INDICES = dat1$blockV2,
    FUN = function(x) {
      bstats_2arm(x, nrow(dat1), "ZblockV2", cov, pooled)
      },
    simplify = F
    )
  smd_block <- sapply(blocked, function(x) x$smd)
  dm_block <- sapply(blocked, function(x) x$diff_means)
  vr_block <- sapply(blocked, function(x) x$vr)
  prop_block <- sapply(blocked, function(x) x$prop)
  btab$block_diff[sel] <- weighted.mean(dm_block,prop_block)
  btab$block_smd[sel] <- weighted.mean(smd_block,prop_block)
  btab$block_vratio[sel] <- weighted.mean(vr_block,prop_block)
  
}

## View SMDs
btab[, c(1, 3:4, 6:7, 9:10)]
```

Applying common heuristics that SMDs should be less than about 0.25 while VRs should be between about 0.5 and 2 [@stuart2010matching], the R output for the completely randomized design doesn't show sufficient evidence of imbalance, though the variance ratios come close (and may therefore be worth a closer look). In contrast, in the clustered design (where balance is assessed by comparing cluster-level aggregates), the variance ratios are more obviously concerning. This makes sense given that treatment was assigned at the level of only 10 clusters. Lastly, for the blocked design (where balance is assessed within each block, and then this is aggregated across blocks), recall that blocks were assigned based on first dividing `cov2` into three groups and then determining whether `cov1` was above it's median. Both groupings are relatively coarse, and the sample sizes within some blocks are quite small, leading blocked randomization to also perform more poorly in terms of variance ratios.

Whether it is better to evaluate overall balance in a blocked/clustered design, or whether it is better to adapt the balance test in some way to the randomization design, depends on the study. But remember that evaluating overall balance in these settings can sometimes yield misleading conclusions [@hansen_covariate_2008].

Next, we illustrate methods for performing an omnibus Wald test under each randomization design, with examples of inference based on either standard asymptotic approximations or permutations of treatment assignment (i.e., randomization inference). Again, while there are canned packages to perform tasks like simulating the randomization distribution (see Chapter 5), we perform this manually for illustration.

::: {.tab} 
<button class="tablinks" onclick="unrolltab(event, 'ch4R11')">R code</button>
<button class="tablinks" onclick="unrolltab(event, 'ch4Stata11')">Stata code</button>
<button class="tablinks" onclick="unrolltab(event, 'ch4Hide11')">Hide</button>
::: {#ch4R11 .tabcontent} 
<br />
```{r, highlight = F, eval = F}
## Prepare data for this exercise
dat1_ <- dat1
dat1_$Z2armEqual <- ifelse(dat1_$Z2armEqual == "T", 1, 0)
dat1_$Zcluster <- ifelse(dat1_$Zcluster == "T", 1, 0)
dat1_$ZblockV2 <- ifelse(dat1$ZblockV2 == "T", 1, 0)
dat1_$blockV2 <- as.factor(dat1_$blockV2)

## Complete RA omnibus balance check
bmod0 <- lm(Z2armEqual ~ cov1 + cov2, data=dat1_)
bmod1 <- lm(Z2armEqual ~ 1, data=dat1_)
wald <- waldtest(bmod0, bmod1, vcov = vcovHC(bmod0, type = "HC2"))

## Repeat using randomization inference
ri_dist <- sapply(
  1:1000,
  function(.x) {
    dat1_$Z2armEqual_ri <- complete_ra(nrow(dat1_))
    unrestr <- lm(Z2armEqual_ri ~ cov1 + cov2, data=dat1_)
    restr <- lm(Z2armEqual_ri ~ 1, data=dat1_)
    waldtest(unrestr, restr, vcov = vcovHC(unrestr, type = "HC2"))$F[2]
    }
  )
wald$ri_p <- c(NA, mean(abs(wald$F[2]) <= abs(ri_dist)))

## Cluster RA omnibus balance check (error adjust only)
bmod0b <- lm(Zcluster ~ cov1 + cov2, data=dat1_)
bmod1b <- lm(Zcluster ~ 1, data=dat1_)
waldb <- waldtest(bmod0b, bmod1b, vcov = vcovCL(bmod0b, cluster = dat1_$buildingID, type = "HC2"))

## Repeat using randomization inference
ri_dist <- sapply(
  1:1000,
  function(.x) {
    dat1_$Zcluster_ri <- cluster_ra(cluster=dat1_$buildingID)
    unrestr <- lm(Zcluster_ri ~ cov1 + cov2, data=dat1_)
    restr <- lm(Zcluster_ri ~ 1, data=dat1_)
    waldtest(unrestr, restr, vcov = vcovCL(unrestr, cluster = dat1_$buildingID, type = "HC2"))$F[2]
    }
  )
waldb$ri_p <- c(NA, mean(abs(waldb$F[2]) <= abs(ri_dist)))

## Cluster RA balance check: Wald test
by_groups <- list(dat1_$buildingID, dat1_$Zcluster)
clust <- aggregate(dat1_[,covlist], by = by_groups, mean)
bmod2 <- lm(Group.2 ~ cov1 + cov2, data=clust)
bmod3 <- lm(Group.2 ~ 1, data=clust)
wald_cl <- waldtest(bmod2, bmod3, vcov = vcovHC(bmod2, type = "HC2"))

## Repeat using randomization inference
ri_dist <- sapply(
  1:1000,
  function(.x) {
    dat1_$Zcluster_ri <- cluster_ra(cluster=dat1_$buildingID)
    by_groups <- list(dat1_$buildingID, dat1_$Zcluster_ri)
    clust_ri <- aggregate(dat1_[,covlist], by = by_groups, mean)
    unrestr <- lm(Group.2 ~ cov1 + cov2, data=clust_ri)
    restr <- lm(Group.2 ~ 1, data=clust_ri)
    waldtest(unrestr, restr, vcov = vcovHC(unrestr, type = "HC2"))$F[2]
    }
  )
wald_cl$ri_p <- c(NA, mean(abs(wald_cl$F[2]) <= abs(ri_dist)))

## Blocked RA omnibus balance check
bmod4 <- lm(ZblockV2 ~ cov1 + cov2 + blockV2, data=dat1_)
bmod5 <- lm(ZblockV2 ~ blockV2, data=dat1_)
wald_bl <- waldtest(bmod4, bmod5, vcov = vcovHC(bmod4, type = "HC2"))

## Repeat using randomization inference
ri_dist <- sapply(
  1:1000,
  function(.x) {
    dat1_$ZblockV2_ri <- block_ra(blocks = dat1$blockV2)
    unrestr <- lm(ZblockV2_ri ~ cov1 + cov2 + blockV2, data=dat1_)
    restr <- lm(ZblockV2_ri ~ blockV2, data=dat1_)
    waldtest(unrestr, restr, vcov = vcovHC(unrestr, type = "HC2"))$F[2]
    }
  )
wald_bl$ri_p <- c(NA, mean(abs(wald_bl$F[2]) <= abs(ri_dist)))

## Organize
omnibus <- data.frame(
  design = c("complete RA", "clustered RA (SE only)", "clustered RA", "blocked RA"),
  wald_p = c(wald$`Pr(>F)`[2], waldb$`Pr(>F)`[2], wald_cl$`Pr(>F)`[2], wald_bl$`Pr(>F)`[2]),
  ri_p = c(wald$ri_p[2], waldb$ri_p[2], wald_cl$ri_p[2], wald_bl$ri_p[2])
  )
omnibus$wald_p <- round(omnibus$wald_p, 3)
omnibus$ri_p <- round(omnibus$ri_p, 4)

## Review
omnibus
```
:::
::: {#ch4Stata11 .tabcontent} 
<br />
```{stata, highlight = F, eval = F}
** Prepare data for this exercise, save prior data
tempfile restore
save `restore', replace
local treatlist z2armequal zcluster zblockv2
foreach l of local treatlist {
	replace `l' = cond(`l' == "T", "1", "0")
	destring `l', replace
}

** Complete RA omnibus balance check
qui reg z2armequal cov1 cov2, vce(hc2)
test cov1=cov2=0
local waldp = r(p)
local waldf = r(F)

** Repeat using randomization inference
capture program drop ri_draw
program define ri_draw, rclass
	capture drop riZ
	complete_ra riZ
	qui reg riZ cov1 cov2, vce(hc2)
	qui test cov1=cov2=0
	return scalar ri_F = r(F)
end
preserve
	simulate ///
	ri_F = r(ri_F), ///
	reps(1000) nodots: ///
	ri_draw
	gen equal_or_greater = abs(ri_F) >= abs(`waldf')
	qui sum equal_or_greater, meanonly
	local waldp_ri = r(mean)
restore

** Cluster RA omnibus balance check (error adjust only)
* (Note: this is CR1, not CR2)
qui reg zcluster cov1 cov2, cluster(buildingid)
qui test cov1=cov2=0
local waldb_p = r(p)
local waldb_f = r(F)

** Repeat using randomization inference
capture program drop ri_draw
program define ri_draw, rclass
	capture drop riZ
	qui sum zcluster
	local zsum = r(sum)
	cluster_ra riZ, cluster_var(buildingid)
	qui reg riZ cov1 cov2, cluster(buildingid)
	qui test cov1=cov2=0
	return scalar ri_F = r(F)
end
preserve
	simulate ///
	ri_F = r(ri_F), ///
	reps(1000) nodots: ///
	ri_draw
	gen equal_or_greater = abs(ri_F) >= abs(`waldb_f')
	qui sum equal_or_greater, meanonly
	local waldb_p_ri = r(mean)
restore

** Cluster RA balance check: Wald test
preserve
	collapse (mean) cov1 cov2 (first) zcluster, by(buildingid)
	qui reg zcluster cov1 cov2, vce(hc2)
	qui test cov1=cov2=0
restore
local wald_clp = r(p)
local wald_clf = r(F)

** Repeat using randomization inference
capture program drop ri_draw
program define ri_draw, rclass
	capture drop riZ
	qui sum zcluster
	local zsum = r(sum)
	cluster_ra riZ, cluster_var(buildingid)
	preserve
		collapse (mean) cov1 cov2 (first) riZ, by(buildingid)
		qui reg riZ cov1 cov2, vce(hc2)
		qui test cov1=cov2=0
	restore
	return scalar ri_F = r(F)
end
preserve
	simulate ///
	ri_F = r(ri_F), ///
	reps(1000) nodots: ///
	ri_draw
	gen equal_or_greater = abs(ri_F) >= abs(`wald_clf')
	qui sum equal_or_greater, meanonly
	local wald_clp_ri = r(mean)
restore

** Blocked RA omnibus balance check
encode blockv2, gen(fblockv2)
qui reg zblockv2 cov1 cov2 i.fblockv2, vce(hc2)
test cov1=cov2=0
local wald_blp = r(p)
local wald_blf = r(F)

** Repeat using randomization inference
capture program drop ri_draw
program define ri_draw, rclass
	capture drop riZ
	block_ra riZ, block_var(blockv2)
	qui reg riZ cov1 cov2 i.fblockv2, vce(hc2)
	qui test cov1=cov2=0
	return scalar ri_F = r(F)
end
preserve
	simulate ///
	ri_F = r(ri_F), ///
	reps(1000) nodots: ///
	ri_draw
	gen equal_or_greater = abs(ri_F) >= abs(`wald_blf')
	qui sum equal_or_greater, meanonly
	local wald_blp_ri = r(mean)
restore

** Organize
matrix omnibus = J(4, 2, .)
matrix rownames omnibus = "complete RA" "clustered RA (se only)" "clustered RA" "blocked RA"
matrix colnames omnibus = "wald_p" "ri_p"
matrix omnibus[1,1] = round(`waldp', 0.001)
matrix omnibus[1,2] = round(`waldp_ri', 0.001)
matrix omnibus[2,1] = round(`waldb_p', 0.001)
matrix omnibus[2,2] = round(`waldb_p_ri', 0.001)
matrix omnibus[3,1] = round(`wald_clp', 0.001)
matrix omnibus[3,2] = round(`wald_clp_ri', 0.001)
matrix omnibus[4,1] = round(`wald_blp', 0.001)
matrix omnibus[4,2] = round(`wald_blp_ri', 0.001)

** Review
matrix list omnibus
```
::: 
::: {#ch4Hide11 .tabcontent}
::: 
:::

```{r omnibal, eval = T, echo = F}
## Prepare data for this exercise
dat1_ <- dat1
dat1_$Z2armEqual <- ifelse(dat1_$Z2armEqual == "T", 1, 0)
dat1_$Zcluster <- ifelse(dat1_$Zcluster == "T", 1, 0)
dat1_$ZblockV2 <- ifelse(dat1$ZblockV2 == "T", 1, 0)
dat1_$blockV2 <- as.factor(dat1_$blockV2)

## Complete RA omnibus balance check
bmod0 <- lm(Z2armEqual ~ cov1 + cov2, data=dat1_)
bmod1 <- lm(Z2armEqual ~ 1, data=dat1_)
wald <- waldtest(bmod0, bmod1, vcov = vcovHC(bmod0, type = "HC2"))

## Repeat using randomization inference
ri_dist <- sapply(
  1:1000,
  function(.x) {
    dat1_$Z2armEqual_ri <- complete_ra(nrow(dat1_))
    unrestr <- lm(Z2armEqual_ri ~ cov1 + cov2, data=dat1_)
    restr <- lm(Z2armEqual_ri ~ 1, data=dat1_)
    waldtest(unrestr, restr, vcov = vcovHC(unrestr, type = "HC2"))$F[2]
    }
  )
wald$ri_p <- c(NA, mean(abs(wald$F[2]) <= abs(ri_dist)))

## Cluster RA omnibus balance check (error adjust only)
bmod0b <- lm(Zcluster ~ cov1 + cov2, data=dat1_)
bmod1b <- lm(Zcluster ~ 1, data=dat1_)
waldb <- waldtest(bmod0b, bmod1b, vcov = vcovCL(bmod0b, cluster = dat1_$buildingID, type = "HC2"))

## Repeat using randomization inference
ri_dist <- sapply(
  1:1000,
  function(.x) {
    dat1_$Zcluster_ri <- cluster_ra(cluster=dat1_$buildingID)
    unrestr <- lm(Zcluster_ri ~ cov1 + cov2, data=dat1_)
    restr <- lm(Zcluster_ri ~ 1, data=dat1_)
    waldtest(unrestr, restr, vcov = vcovCL(unrestr, cluster = dat1_$buildingID, type = "HC2"))$F[2]
    }
  )
waldb$ri_p <- c(NA, mean(abs(waldb$F[2]) <= abs(ri_dist)))

## Cluster RA balance check: Wald test
by_groups <- list(dat1_$buildingID, dat1_$Zcluster)
clust <- aggregate(dat1_[,covlist], by = by_groups, mean)
bmod2 <- lm(Group.2 ~ cov1 + cov2, data=clust)
bmod3 <- lm(Group.2 ~ 1, data=clust)
wald_cl <- waldtest(bmod2, bmod3, vcov = vcovHC(bmod2, type = "HC2"))

## Repeat using randomization inference
ri_dist <- sapply(
  1:1000,
  function(.x) {
    dat1_$Zcluster_ri <- cluster_ra(cluster=dat1_$buildingID)
    by_groups <- list(dat1_$buildingID, dat1_$Zcluster_ri)
    clust_ri <- aggregate(dat1_[,covlist], by = by_groups, mean)
    unrestr <- lm(Group.2 ~ cov1 + cov2, data=clust_ri)
    restr <- lm(Group.2 ~ 1, data=clust_ri)
    waldtest(unrestr, restr, vcov = vcovHC(unrestr, type = "HC2"))$F[2]
    }
  )
wald_cl$ri_p <- c(NA, mean(abs(wald_cl$F[2]) <= abs(ri_dist)))

## Blocked RA omnibus balance check
bmod4 <- lm(ZblockV2 ~ cov1 + cov2 + blockV2, data=dat1_)
bmod5 <- lm(ZblockV2 ~ blockV2, data=dat1_)
wald_bl <- waldtest(bmod4, bmod5, vcov = vcovHC(bmod4, type = "HC2"))

## Repeat using randomization inference
ri_dist <- sapply(
  1:1000,
  function(.x) {
    dat1_$ZblockV2_ri <- block_ra(blocks = dat1$blockV2)
    unrestr <- lm(ZblockV2_ri ~ cov1 + cov2 + blockV2, data=dat1_)
    restr <- lm(ZblockV2_ri ~ blockV2, data=dat1_)
    waldtest(unrestr, restr, vcov = vcovHC(unrestr, type = "HC2"))$F[2]
    }
  )
wald_bl$ri_p <- c(NA, mean(abs(wald_bl$F[2]) <= abs(ri_dist)))

## Organize
omnibus <- data.frame(
  design = c("complete RA", "clustered RA (SE only)", "clustered RA", "blocked RA"),
  wald_p = c(wald$`Pr(>F)`[2], waldb$`Pr(>F)`[2], wald_cl$`Pr(>F)`[2], wald_bl$`Pr(>F)`[2]),
  ri_p = c(wald$ri_p[2], waldb$ri_p[2], wald_cl$ri_p[2], wald_bl$ri_p[2])
  )
omnibus$wald_p <- round(omnibus$wald_p, 3)
omnibus$ri_p <- round(omnibus$ri_p, 4)

## Review
omnibus
```

In these examples, both standard asymptotic and randomization-based inference yield similar conclusions in terms of failing to reject the joint null of overall imbalance (where "overall imbalance" is quantified using the F-statistic from a Wald test). Given the relatively small sample size in this example, though we do not see sufficient evidence to reject the joint null, concerns that the omnibus test could be under-powered might still support evaluating covariate-by-covariate statistics as we do above as supplementary evidence. 

Finally, while we do not report results in text, we also illustrate how to perform the $d^2$ omnibus test mentioned above for a few different randomization designs. This is implemented in R using the function `balanceTest()` from the package `RItools` [@hansen_covariate_2008; @bowers_ritools_2016]. In Stata, this is implemented through a command that calls the R package and applies it to the data in memory (so it still requires an R installation on the user's computer). This function provides significance test results based on large-sample approximations to the "permutation" or "randomization" distribution of $d^2$ statistic, but its randomization distribution could be computed manually as in our examples above. You can think of the $d^2$ test statistic as a summary measure of mean differences across each covariate.

::: {.tab} 
<button class="tablinks" onclick="unrolltab(event, 'ch4R9')">R code</button>
<button class="tablinks" onclick="unrolltab(event, 'ch4Stata9')">Stata code</button>
<button class="tablinks" onclick="unrolltab(event, 'ch4Hide9')">Hide</button>
::: {#ch4R9 .tabcontent} 
<br />
```{stata, highlight = F, eval = F}
## Complete RA
balanceTest(Z~cov1+cov2, data=dat1)

## Blocked RA
balanceTest(ZblockV3~cov1+cov2+strata(blocksV3), 

## Blocked RA
balanceTest(ZblockV3~cov1+cov2+strata(blocksV3)+cluster(clusterID), data=dat1)
```
:::
::: {#ch4Stata9 .tabcontent} 
<br />
```{stata, highlight = F, eval = F}
* ssc install xbalance.
* See "help xbalance" for additional Stata setup instructions.
* This is calling the RItools R package, so you will need R installed.
* The necessary path to Rterm.exe may look something like this:
global Rterm_path "C:\Program Files\R\R-4.2.1\bin\x64\Rterm.exe"

** Complete RA
gen single_block = 1 // To force only unstratified balance testing
label val zblockV2 // Remove value labels first
label val z
xbalance z single_block cov1 cov2

** Block RA
* zblockV2 instead of zblockV3
* zblockV3 is generated using blockTools, with no Stata equivalent
xbalance zblockV2 blockV2 cov1 cov2 
```
::: 
::: {#ch4Hide9 .tabcontent}
::: 
:::

### What to do with "failed" randomization assessments?

Observing a $p$-value of less than .05 in an omnibus test like the one above ought to trigger extra scrutiny about implementation and/or how the data were recorded. For example, we might respond by contacting our agency partner to learn more about how random numbers were generated or how random assignment code was used (particularly if we didn't perform the random assignment ourselves). In many circumstances, this follow-up investigation might suggest that random assignment was implemented correctly, and that our understanding of the design or the data was simply incorrect (i.e., the balance test was not performed correctly). But sometimes, a follow-up investigation may not turn up any misunderstandings at all. In those situations, we will need to determine whether our rejection of the null hypothesis of appropriate random assignment is a false positive, or evidence of a more systematic problem.

If our rejection of the null hypothesis appears to be driven by one or more covariates that are substantively important---say, the variable `age` looks very imbalanced between treated and control groups in a health-related randomized trial---then we might present both the unadjusted results and a separate set of results that adjust for the covariate(s) in question (e.g., through a stratified difference-in-means estimator, or by using them as controls in a linear regression). Large differences between the adjusted and unadjusted estimates might help us interpret our findings: estimating separate effects within different age groups, for example, might tell us something useful about the particular context of a study and inform the conclusions we draw.

```{r savedat1, echo=FALSE}
write.csv(dat1,file="dat1_with_designs.csv")
```
