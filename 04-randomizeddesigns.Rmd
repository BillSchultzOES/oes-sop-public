```{r setupdat13,  echo=FALSE, include=FALSE}
if(!exists("dat1")){ dat1 <- read.csv(here::here('dat1.csv')) }
```

# Randomization choices

After working together with our agency partners to translate insights from the social and behavioral sciences into potential policy recommendations, we assess those new ideas by observing differences or changes in real world outcomes (usually measured using existing administrative data).^[We develop the recommendations themselves following [processes](https://oes.gsa.gov/methods/) described in more detail on our website.] In most cases, we design a randomized control trial (an RCT) to ensure that the differences or changes we observe are driven by the policy intervention itself. Here, we show examples of the different methods we consider for randomly assigning units to treatment. These form the core of the different types of RCTs that we use to build evidence about the effectiveness of the new policies.

## Coin flipping vs urn-drawing randomization

Many discussions of RCTs begin by talking about the intervention being assigned to units (people,  schools, offices, districts) "by the flip of a coin," or **simple** random assignment. Each unit's assignment to treatment occurs separately, and there is no *ex ante* guarantee as to exactly what the final number of treated or control units will be. We don't always use this method in practice, even though it is a useful way to introduce the idea that RCTs guarantee fair access to a new policy.

The following code contrasts coin-flipping style random assignment with drawing-from-an-urn style, or **complete** random assignment (where a fixed number of units are randomly chosen for treatment). Coin-flipping based experiments are still valid and tell us about the underlying counterfactuals, but they can have less statistical power, so we try to avoid them where possible.

Notice that the simple random assignment implemented in the code below results in more observations in the treatment group (group `T`) than in the control group (group `C`). Complete random assignment will always assign 5 units to the treatment, 5 to the control.

<!-- Adds copy code button -->
```{r klippych4, echo=FALSE, include=TRUE}
klippy::klippy(all_precode = T, position = c("top", "right"))
```

<!-- Used (and iteratively updated) in the {oes_code_tab} snippets below. -->
<!-- set chapter number and reset count -->
```{r, include = F, echo = F}
# cnum is modified automatically to iteratively count chunks
# when using the oes_code_tab markdown snippet. each use of
# the snippet adds a value of 1.
ch <- 4
cnum <- 0
```

::: {.tab} 
<button class="tablinks" onclick="unrolltab(event, 'ch4R1')">R code</button>
<button class="tablinks" onclick="unrolltab(event, 'ch4Stata1')">Stata code</button>
<button class="tablinks" onclick="unrolltab(event, 'ch4Hide1')">Hide</button>
::: {#ch4R1 .tabcontent} 
<br />
```{r, highlight = F, eval = F}
## Start with a small experiment with only 10 units
n <- 10

## Set a random seed for replicability
set.seed(12345)

## Function to add more intuitive labels
labelTC <- function(assignments) {ifelse(assignments == 1, "T", "C")}

## Simulation using functions from the randomizr package
trt_coinflip <- labelTC(simple_ra(n))
trt_urn <- labelTC(complete_ra(n))

## Coin flipping does not guarantee half and half treated and control.
## Drawing from an urn, guarantees half treated and control.
table(trt_coinflip)
table(trt_urn)

## Alternative approach using base R
# set.seed(12345)
# trt_coinflip <- labelTC(rbinom(10, size = 1, prob = .5))
# trt_urn <- labelTC(sample(rep(c(1, 0), n / 2)))
# table(trt_coinflip)
# table(trt_urn)
```
:::
::: {#ch4Stata1 .tabcontent} 
<br />
```{stata, highlight = F, eval = F}
** Start with a small experiment with only 10 units
clear
global n = 10
set obs $n

** Set a random seed for replicability
set seed 12345

** Simulation using functions from the randomizr package
* ssc install randomizr

simple_ra trt_coinflip
* Or, e.g.: gen trt_coinflip = rbinomial(1, 0.5)

complete_ra trt_urn
/* 
* Or, e.g.:
local num_treat = $n/2
gen rand = runiform()
sort rand
gen trt_urn = 1 in 1/`num_treat'
replace trt_urn = 0 if missing(trt_urn)
drop rand
*/

** Add more informative labels
label define tc 0 "C" 1 "T"
label values trt_coinflip tc
label values trt_urn tc

** Coin flipping does not guarantee half and half treated and control.
** Drawing from an urn, guarantees half treated and control.
table trt_coinflip
table trt_urn
```
::: 
::: {#ch4Hide1 .tabcontent}
::: 
:::

```{r completera, echo = F, eval = T, results = "hold", collapse = T}
## Start with a small experiment with only 10 units
n <- 10

## Set a random seed for replicability
set.seed(12345)

## Function to add more intuitive labels
labelTC <- function(assignments) {ifelse(assignments == 1, "T", "C")}

## Simulation using functions from the randomizr package
trt_coinflip <- labelTC(simple_ra(n))
trt_urn <- labelTC(complete_ra(n))

## Coin flipping does not guarantee half and half treated and control.
## Drawing from an urn, guarantees half treated and control.
table(trt_coinflip)
table(trt_urn)

## Alternative approach using base R
# set.seed(12345)
# trt_coinflip <- labelTC(rbinom(10, size = 1, prob = .5))
# trt_urn <- labelTC(sample(rep(c(1, 0), n / 2)))
# table(trt_coinflip)
# table(trt_urn)
```

## Randomization into 2 or more groups

We often use the `randomizr` R package [@R-randomizr] for simple designs
rather than the base R `sample` function because `randomizr` does some
quality control checks. Notice that we implement a check on our code below with the `stopifnot` command: the code will stop and issue a warning if we didn't actually assign 1/4 of the observations to the treatment condition. Here, we assign the units first to 2 arms with equal probability (`Z2armEqual`). Then, to show how the code works, we assign them to 2 arms where one arm has only a $\frac{1}{4}$ probability of receiving treatment (e.g., imagine a design with an expensive intervention). Last, we assign them based on a design with 4 different arms, each with equal probability (e.g., one control group and three different treatments under consideration). We often use $Z$ to refer to the variable recording our intervention arms.

::: {.tab} 
<button class="tablinks" onclick="unrolltab(event, 'ch4R2')">R code</button>
<button class="tablinks" onclick="unrolltab(event, 'ch4Stata2')">Stata code</button>
<button class="tablinks" onclick="unrolltab(event, 'ch4Hide2')">Hide</button>
::: {#ch4R2 .tabcontent} 
<br />
```{r, highlight = F, eval = F}
N <- nrow(dat1)
set.seed(12345)

## Two equal arms
dat1$Z2armEqual <- labelTC(complete_ra(N))

## Two unequal arms: .25 chance of treatment (.75 chance of control0
dat1$Z2armUnequalA <- labelTC(complete_ra(N,prob=.25))
stopifnot(sum(dat1$Z2armUnequalA=="T")==N/4)
dat1$Z2armUnequalB <- labelTC(complete_ra(N,m=N/4))

## Four equal arms
dat1$Z4arms <- complete_ra(N, m_each=rep(N/4,4))

table(Z2armEqual=dat1$Z2armEqual)
table(Z2armUnequalA=dat1$Z2armUnequalA)
table(Z2armUnequalB=dat1$Z2armUnequalB)
table(Z4arms=dat1$Z4arms)
```
:::
::: {#ch4Stata2 .tabcontent} 
<br />
```{stata, highlight = F, eval = F}
** Return to data for the fake experiment.
import delimited using "dat1.csv", clear

qui count
global N = r(N)
set seed 12345

** Two equal arms
complete_ra z2armEqual
label define tc 0 "C" 1 "T"
label values z2armEqual tc

** Two unequal arms: .25 chance of treatment (.75 chance of control)
complete_ra z2armUnequalA, prob(0.25)
label values z2armUnequalA tc
qui sum z2armUnequalA
global expected = $N/4
assert r(sum) == $expected
complete_ra z2armUnequalB, m($expected)
label values z2armUnequalB tc

** Four equal arms
local count_list : di _dup(4) "$expected " // List of sample sizes for each group
macro list _count_list
complete_ra z4arms, m_each(`count_list')

table z2armEqual
table z2armUnequalA
table z2armUnequalB
table z4arms
```
::: 
::: {#ch4Hide2 .tabcontent}
::: 
:::

```{r completera2, echo = F, eval = T, results = "hold", collapse = T}
N <- nrow(dat1)
set.seed(12345)

## Two equal arms
dat1$Z2armEqual <- labelTC(complete_ra(N))

## Two unequal arms: .25 chance of treatment (.75 chance of control0
dat1$Z2armUnequalA <- labelTC(complete_ra(N,prob=.25))
stopifnot(sum(dat1$Z2armUnequalA=="T")==N/4)
dat1$Z2armUnequalB <- labelTC(complete_ra(N,m=N/4))

## Four equal arms
dat1$Z4arms <- complete_ra(N,m_each=rep(N/4,4))

table(Z2armEqual=dat1$Z2armEqual)
table(Z2armUnequalA=dat1$Z2armUnequalA)
table(Z2armUnequalB=dat1$Z2armUnequalB)
table(Z4arms=dat1$Z4arms)
```

## Factorial Designs

It's possible to test the effects of more than one intervention while losing less statistical power by randomly assigning multiple treatments independently of each other. The simplest design that we use for this purpose is the $2 \times 2$ **factorial** design. For example, in the next table we see that we have assigned `r N/2` observations to each arm of two separate interventions. Since the randomization of `treatment1` is independent of `treatment2`, we can assess the effects of each treatment separately and pay less of a power penalty (unless one of the treatments dramatically increases the variance of the outcome compared to a hypothetical experiment with only one treatment assigned).

::: {.tab} 
<button class="tablinks" onclick="unrolltab(event, 'ch4R3')">R code</button>
<button class="tablinks" onclick="unrolltab(event, 'ch4Stata3')">Stata code</button>
<button class="tablinks" onclick="unrolltab(event, 'ch4Hide3')">Hide</button>
::: {#ch4R3 .tabcontent} 
<br />
```{r, highlight = F, eval = F}
## Two equal arms, adding a second cross treatment
dat1$Z2armEqual2 <- labelTC(complete_ra(N))
table(treatment1=dat1$Z2armEqual,treatment2=dat1$Z2armEqual2)
```
:::
::: {#ch4Stata3 .tabcontent} 
<br />
```{stata, highlight = F, eval = F}
** Two equal arms, adding a second cross treatment
complete_ra z2armEqual2
label var z2armEqual2 "Treatment 2"
label var z2armEqual "Treatment 1"
label val z2armEqual2 tc
table z2armEqual z2armEqual2
```
::: 
::: {#ch4Hide3 .tabcontent}
::: 
:::

```{r factdesign2x2, eval = T, echo = F}
## Two equal arms, adding a second cross treatment
dat1$Z2armEqual2 <- labelTC(complete_ra(N))
table(treatment1=dat1$Z2armEqual,treatment2=dat1$Z2armEqual2)
```

Although factorial designs allow us to test more than one intervention at the
same time, they may not provide the same degree of power when testing
hypotheses about the *interaction* between the two treatments. If we want to
learn about how two different interventions work together, then the sample
size requirements will be much larger than if we were satisfied with learning about each treatment separately.^[But see @small2011structured, for instance, for a way to increase the power of tests for such questions. This approach is not currently part of our standard practice.]

Importantly, [recent work](https://direct.mit.edu/rest/article-abstract/doi/10.1162/rest_a_01317/115272/Factorial-Designs-Model-Selection-and-Incorrect?redirectedFrom=fulltext) highlights some important concerns regarding (1) the consequences of omitting interaction terms when estimating separate effects of each treatment and (2) the interpretation of factorial treatment effects [@muralidharan2023factorial]. First, on (1), even if the interaction between treatments is not of academic or policy relevance, including it in the estimation model may be important for making correct inferences. Specifically, if the true interaction effect is not zero, excluding it from the model could increase the risk of Type I errors (i.e., false positives).

Meanwhile, on (2), consider a two-arm factorial design with two Treatments, A and B, with 25% of the sample is in each treatment condition. An estimated effect of Treatment A from a model without an interaction should be interpreted as a weighted average of the effects of A across two subsamples: those receiving B (50%), and those not receiving B (50%). This weighted average treatment effect may or may not provide useful information about the likely effects of Treatment A if it is scaled up later.^[As the authors discuss, this could be framed as either an internal or external validity concern.] For instance, there may be a substantial interaction with treatment B, which is rarely administered in reality and which will not be scaled up alongside A. The subgroup effect of A among "not B" is then more policy relevant, but the subgroup effect of A among "receiving B" pulls the overall estimated average effect of A away from it.

To deal with those issues, the OES Methods Team recommends estimating treatment effects in factorial experiments using a model that includes an interaction, at least as a robustness check.

## Block Random Assignment

### The benefits of blocking

Statistical power depends not only on the size of the experiment and the strength of the treatment effect, but also on the amount of "noise" (non-treatment-related variability) in the outcome measure. Block-randomized designs can help reduce this noise while simultaneously minimizing estimation error---the amount that our particular experiment's estimate differs from the truth.  

In a **block-randomized**, or stratified, design, we randomly assign units to the policy intervention *within* pre-specified groups.^[Some studies instead employ "post-stratification," where simple or complete randomization might be used to assign treatment, but the data are still analyzed *as-if they were blocked*. See @miratrix2013adjusting, for instance, for more discussion of this strategy. Post-stratification is more likely than blocking to increase variance (relative to a simple difference in means) when there is not actually any between-strata variation in potential outcomes. Post-stratification with too many strata may also have variance costs, which is again less likely when treatment assignment is blocked.] Suppose we are evaluating whether dedicated navigators can increase the percentage of students living in public housing who complete federal financial aid applications (FAFSA). Our experiment will send navigators to two of four eligible buildings, two of which are large and two of which are small. In a real study we can never know the outcome in all buildings both with and without navigators (the "fundamental problem of causal inference" from the last chapter). But if we could, we might have the data below:

```{r maketab1, echo = F}
tibble(
  Building = c(1:4, "Mean"),
  Size = c(rep.int("Large", 2), rep.int("Small", 2), " "),
  `% FAFSA (No Navigator)` = c(20, 30, 20, 30, 25),
  `% FAFSA (With Navigator)` = c(60, 70, 30, 40, 50)
  ) %>%
  knitr::kable() %>%
  kableExtra::row_spec(4, extra_css = "border-bottom: 1px solid") %>%
  kableExtra::row_spec(5, extra_css = "border-bottom: 1px solid") %>%
  kable_styling(full_width = F)

# This LaTex code was used previously
# \begin{center}
# \begin{tabular}{ccccc}
# &&& FAFSA \% & FAFSA \% \\
# Building & Size & Navigators? & if No Navigators & if Navigators   \\ \hline
# 1 & Large & $\_$ & 20 & 60 \\
# 2 & Large & $\_$ & 30 & 70 \\
# 3 & Small & $\_$ & 20 & 30 \\
# 4 & Small & $\_$ & 30 & 40 \\ \hline
# && Means: & 25 & 50 \\
# \end{tabular}
# \end{center}
```

The true average treatment effect for this sample is the average under treatment (i.e., the average treated potential outcome) minus the average under control (i.e., the average control potential outcome): $\text{ATE} = 50-25=25$ percent more applications per building when a navigator is deployed.

In a real study, we might randomly allocate two buildings to treatment and two buildings to control. If complete random assignment led to us treating the first two buildings, then we might observe:

```{r maketab2, echo = F}
tibble(
  Building = c(1:4, "Mean"),
  Size = c(rep.int("Large", 2), rep.int("Small", 2), " "),
  Treated = c(1, 1, 0, 0, " "),
  `% FAFSA (No Navigator)` = c(" ", " ", 20, 30, 25),
  `% FAFSA (With Navigator)` = c(60, 70, " ", " ", 65)
  ) %>%
  knitr::kable() %>%
  kableExtra::row_spec(4, extra_css = "border-bottom: 1px solid") %>%
  kableExtra::row_spec(5, extra_css = "border-bottom: 1px solid") %>%
  kable_styling(full_width = F)

# This LaTex code was used previously
# \begin{center}
# \begin{tabular}{ccccc}
# &&& FAFSA \% & FAFSA \% \\
# Building & Size & Navigators? & if No Navigators & if Navigators   \\ \hline
# 1 & Large & Yes &  & 60 \\
# 2 & Large & Yes &  & 70 \\
# 3 & Small & No & 20 &  \\
# 4 & Small & No & 30 &  \\ \hline
# && Means: & 25 & 65 \\
# \end{tabular}
# \end{center}
```

This yields a treatment effect estimate of $65-25 = 40$ percent more applications due to the presence of a navigator. This is *larger* than the true value of $25$.

Or, if random assignment led to the other two buildings being treated instead, we might then observe:

```{r maketab3, echo = F}
tibble(
  Building = c(1:4, "Mean"),
  Size = c(rep.int("Large", 2), rep.int("Small", 2), " "),
  Treated = c(0, 0, 1, 1, " "),
  `% FAFSA (No Navigator)` = c(20, 30, " ", " ", 25),
  `% FAFSA (With Navigator)` = c(" ", " ", 30, 40, 35)
  ) %>%
  knitr::kable() %>%
  kableExtra::row_spec(4, extra_css = "border-bottom: 1px solid") %>%
  kableExtra::row_spec(5, extra_css = "border-bottom: 1px solid") %>%
  kable_styling(full_width = F)

# This LaTex code was used previously
# \begin{center}
# \begin{tabular}{ccccc}
# &&& FAFSA \% & FAFSA \% \\
# Building & Size & Navigators? & if No Navigators & if Navigators   \\ \hline
# 1 & Large & $\_$ &  & 60 \\
# 2 & Large & $\_$ & 30 &  \\
# 3 & Small & $\_$ &  & 30 \\
# 4 & Small & $\_$ & 30 &  \\ \hline
# && Means: & 30 & 45 \\
# \end{tabular}
# \end{center}
```

This, in contrast, yields an estimated treatment effect of $35-25 = 10$ percentage point more applications due to the navigators -- now *smaller* than the true value of $25$.

All of the possible (equiprobable) assignments with two treated and two control units, along with their estimated treatment effects, are listed in the table below:

```{r maketab4, echo = F}
tibble(
  Assignments = c("TTCC", "CTCT", "TCCT", "CTTC", "TCTC", "CCTT"),
 `Estimated Effect` = c(40, 35, 25, 25, 15, 10)
  ) %>%
  knitr::kable() %>%
  kable_styling(full_width = F)
# This LaTex code was used previously
# \begin{center}
# \begin{tabular}{cc}
# Assignments & Est TE \\ \hline
# YYNN & 40  \\ 
# NYNY & 35 \\
# YNNY & 25 \\
# NYYN & 25 \\
# YNYN & 15 \\
# NNYY & 10
# \end{tabular}
# \end{center}
```

These possible treatment effect estimates have a mean equal to the true value of 25, illustrating the difference in means is an unbiased estimator. However, some of these estimates are far from the truth, and they have a lot of variability.

To design an experiment that better estimates the true value, and does so with more statistical power (less variability), we can randomly assign units within *blocks*. In general, units should be sorted into different blocks based on their similarity across one or more characteristics that we expect to be correlated with our outcome. Here, blocking implies restricting the possible random assignments to those that have one large and one small building in each treatment group:

```{r maketab5, echo = F}
tibble(
  Assignments = c("CTCT", "TCCT", "CTTC", "TCTC"),
 `Estimated Effect` = c(35, 25, 25, 15)
  ) %>%
  knitr::kable() %>%
  kable_styling(full_width = F)
# This LaTex code was used previously
# \begin{center}
# \begin{tabular}{cc}
# Assignments & Est TE \\ \hline
# NYNY & 35 \\
# YNNY & 25 \\
# NYYN & 25 \\
# YNYN & 15 \\
# \end{tabular}
# \end{center}
```

With this blocked design restricting the random assignments that are possible, we now get an estimate that is no more than 10 percentage points from the truth. Further, our estimates will have less variability (an SD of `r round(sd(c(35, 25, 25, 15)), 2)` rather than `r round(sd(c(35, 25, 25, 15, 40, 10)), 2)`). This improves the statistical power of our design.

For a more realistic example, suppose we are designing an experiment
where the sample includes patients from two different hospitals. We might randomly assign patients to treatment and control *within* each hospital. For instance, we might assign half of the patients in hospital "A" to treatment and half to control, then do the same in hospital "B."^[We use "block" rather than "strata" throughout this document to refer to groups of experimental units that are fixed *before random assignment occurs*.] Below, we have `r nrow(dat1) / 2` units in hospital "A" and `r nrow(dat1) / 2` in hospital "B":

::: {.tab} 
<button class="tablinks" onclick="unrolltab(event, 'ch4R4')">R code</button>
<button class="tablinks" onclick="unrolltab(event, 'ch4Stata4')">Stata code</button>
<button class="tablinks" onclick="unrolltab(event, 'ch4Hide4')">Hide</button>
::: {#ch4R4 .tabcontent} 
<br />
```{r, highlight = F, eval = F}
dat1$blockID <- gl(n = 2, k = N/2, labels = c("Block A", "Block B"))
with(dat1,table(blockID=dat1$blockID))
```
:::
::: {#ch4Stata4 .tabcontent} 
<br />
```{stata, highlight = F, eval = F}
local Anum = $N/2
gen blockID = .
tempvar rand
gen `rand' = runiform()
sort `rand'
replace blockID = 1 in 1/`Anum'
replace blockID = 2 if missing(blockID)
label define blocklab 1 "Block A" 2 "Block B"
label values blockID blocklab
table blockID
```
::: 
::: {#ch4Hide4 .tabcontent}
::: 
:::

```{r blockra1, eval = T, echo = F}
dat1$blockID <- gl(n = 2, k = N/2, labels = c("Block A", "Block B"))
with(dat1,table(blockID=dat1$blockID))
```

We assign half of the units in each hospital to each treatment condition:

::: {.tab} 
<button class="tablinks" onclick="unrolltab(event, 'ch4R5')">R code</button>
<button class="tablinks" onclick="unrolltab(event, 'ch4Stata5')">Stata code</button>
<button class="tablinks" onclick="unrolltab(event, 'ch4Hide5')">Hide</button>
::: {#ch4R5 .tabcontent} 
<br />
```{r, highlight = F, eval = F}
dat1$Z2armBlocked <- labelTC(block_ra(blocks=dat1$blockID))
with(dat1, table(blockID, Z2armBlocked))
```
:::
::: {#ch4Stata5 .tabcontent} 
<br />
```{stata, highlight = F, eval = F}
block_ra z2armBlocked, block_var(blockID) replace
label val z2armBlocked tc
table blockID z2armBlocked
capture drop __00* // Clean up block_var temporary var
```
::: 
::: {#ch4Hide5 .tabcontent}
::: 
:::

```{r blockra15, echo = F, eval = T}
dat1$Z2armBlocked <- labelTC(block_ra(blocks=dat1$blockID))
with(dat1, table(blockID, Z2armBlocked))
```

If, say, there were fewer people eligible for treatment in hospital "A" --- or perhaps the intervention was more expensive in that block --- we might choose different treatment probabilities for each block. The code below assigns half of the hospital "A" patients to treatment, but only a quarter of those from hospital "B". Again, we also check that this code worked. This approach is an informal version of one of the best practices for writing code in general, called "unit testing." See the [EGAP Guide to Workflow](https://egap.org/resource/10-things-to-know-about-project-workflow/) for more examples.

::: {.tab} 
<button class="tablinks" onclick="unrolltab(event, 'ch4R6')">R code</button>
<button class="tablinks" onclick="unrolltab(event, 'ch4Stata6')">Stata code</button>
<button class="tablinks" onclick="unrolltab(event, 'ch4Hide6')">Hide</button>
::: {#ch4R6 .tabcontent} 
<br />
```{r, highlight = F, eval = F}
## Blocked assignment, unequal probability
dat1$Z2armBlockedUneqProb <- labelTC(block_ra(blocks=dat1$blockID, block_prob=c(.5,.25)))
with(dat1, table(blockID, Z2armBlockedUneqProb))

## Unit testing
NumTreatedB <- sum(dat1$Z2armBlockedUneqProb=="T" & dat1$blockID=="Block B")
ExpectedNumTreatedB <- sum(dat1$blockID=="Block B")/4
stopifnot(NumTreatedB==ceiling(ExpectedNumTreatedB) | NumTreatedB==floor(ExpectedNumTreatedB))
```
:::
::: {#ch4Stata6 .tabcontent} 
<br />
```{stata, highlight = F, eval = F}
** Blocked assignment, unequal probability
block_ra z2armBlockedUneqProb, block_var(blockID) block_prob(0.5 0.25) replace
label val z2armBlockedUneqProb tc
table blockID z2armBlockedUneqProb
capture drop __00* // Clean up block_var temporary var

** Unit Testing
qui count if z2armBlockedUneqProb == 1 & blockID == 2
global NumTreatedB = `r(N)'
qui count if blockID == 2
global ExpectedNumTreatedB = `r(N)'/4
assert ($NumTreatedB == ceil($ExpectedNumTreatedB)) | ($NumTreatedB == floor($ExpectedNumTreatedB))
```
::: 
::: {#ch4Hide6 .tabcontent}
::: 
:::

```{r blockra2, echo = F, eval = T, results = "hold", collapse=T}
## Blocked assignment, unequal probability
dat1$Z2armBlockedUneqProb <- labelTC(block_ra(blocks=dat1$blockID, block_prob=c(.5,.25)))
with(dat1, table(blockID, Z2armBlockedUneqProb))

## Unit testing
NumTreatedB <- sum(dat1$Z2armBlockedUneqProb=="T" & dat1$blockID=="Block B")
ExpectedNumTreatedB <- sum(dat1$blockID=="Block B")/4
stopifnot(NumTreatedB==ceiling(ExpectedNumTreatedB) | NumTreatedB==floor(ExpectedNumTreatedB))
```

Our team tries to implement block-randomized assignment whenever possible in
order to increase the statistical power of our experiments. We also often find
it useful in cases where different administrative units are implementing the
treatment, or when we expect different groups of people to have different reactions to the treatment.

### Using only a few covariates to create blocks

If we have background information on a few covariates, we can create blocks by hand through a process like the one demonstrated here:

::: {.tab} 
<button class="tablinks" onclick="unrolltab(event, 'ch4R7')">R code</button>
<button class="tablinks" onclick="unrolltab(event, 'ch4Stata7')">Stata code</button>
<button class="tablinks" onclick="unrolltab(event, 'ch4Hide7')">Hide</button>
::: {#ch4R7 .tabcontent} 
<br />
```{r, highlight = F, eval = F}
## For example, make three groups from the cov2 variable
dat1$cov2cat <- with(dat1, cut(cov2, breaks=3))
table(dat1$cov2cat, exclude=c())
with(dat1, tapply(cov2, cov2cat, summary))

## And we can make blocks that are the same on two covariates
dat1$cov1bin <- as.numeric(dat1$cov1>median(dat1$cov1)) # Binarize cov1
dat1$blockV2 <- droplevels(with(dat1, interaction(cov1bin, cov2cat)))
table(dat1$blockV2, exclude=c())

## And then assign within these blocks
set.seed(12345)
dat1$ZblockV2 <- labelTC(block_ra(blocks = dat1$blockV2))
with(dat1, table(blockV2, ZblockV2, exclude=c()))
```
:::
::: {#ch4Stata7 .tabcontent} 
<br />
```{stata, highlight = F, eval = F}
** For example, make three groups from the cov2 variable
egen cov2cat = cut(cov2), group(3) label
table cov2cat
bysort cov2cat : sum cov2 // Divides into intervals differently from R

** And we can make blocks that are the same on two covariates
qui sum cov1, d
gen cov1bin = cond(cov1 > r(p50), 1, 0) // Similar to ifelse() in R
decode cov2cat, generate(string_cov2cat)
gen blockV2 = string(cov1bin) + " " + string_cov2cat
table blockV2

** And then assign within these blocks
set seed 12345
block_ra zblockV2, block_var(blockV2)
label val zblockV2 tc
table blockV2 zblockV2
capture drop __00* // Clean up
```
::: 
::: {#ch4Hide7 .tabcontent}
::: 
:::

```{r cutcovs, eval = T, echo = F, collapse=T, results = "hold"}
## For example, make three groups from the cov2 variable
dat1$cov2cat <- with(dat1, cut(cov2, breaks=3))
table(dat1$cov2cat, exclude=c())
with(dat1, tapply(cov2, cov2cat, summary))

## And we can make blocks that are the same on two covariates
dat1$cov1bin <- as.numeric(dat1$cov1>median(dat1$cov1)) # Binarize cov1
dat1$blockV2 <- droplevels(with(dat1, interaction(cov1bin, cov2cat)))
table(dat1$blockV2, exclude=c())

## And then assign within these blocks
set.seed(12345)
dat1$ZblockV2 <- labelTC(block_ra(blocks = dat1$blockV2))
with(dat1, table(blockV2, ZblockV2, exclude=c()))
```

### Multivariate blocking using many covariates

If instead we have many background variables, we can increase precision by thinking about blocking as a problem of "matching," or creating sets of
units which are as similar as possible across the entire set of covariates [@moore2012multivariate;@moore2016bT063]. Here we show two approaches using R.

Creating pairs:

```{r}
## Using the blockTools package
mvblocks <- block(dat1, id.vars="id", block.vars=c("cov1","cov2"), algorithm="optimal")
dat1$blocksV3 <- createBlockIDs(mvblocks, data=dat1, id.var = "id")
dat1$ZblockV3 <- labelTC(block_ra(blocks = dat1$blocksV3))

## Just show the first ten pairs
with(dat1,table(blocksV3,ZblockV3,exclude=c()))[1:10,]
```

Creating larger blocks:

```{r}
## Using the quickblock package
distmat <- distances(dat1, dist_variables = c("cov1bin", "cov2"), id_variable = "id", normalize="mahalanobiz")
distmat[1:5,1:5]
quantile(as.vector(distmat), seq(0,1,.1))

## The caliper argument helps prevent ill-matched points
mvbigblock <- quickblock(distmat, size_constraint = 6L, caliper = 2.5)

## Look for missing points
table(mvbigblock,exclude=c()) # One point dropped due to caliper
dat1$blocksV4 <- mvbigblock
dat1$notblocked <- is.na(dat1$blocksV4) 
dat1$ZblockV4[dat1$notblocked==F] <- labelTC(block_ra(blocks = dat1$blocksV4))
with(dat1, table(blocksV4, ZblockV4, exclude=c()))[1:10,]
```

It's worth pausing to examine the differences within blocks. We'll focus on the proportion of people in category "1" on the binary covariate (notice that the blocks are homogeneous on this covariate), as well as the difference between the largest and smallest value of the continuous covariate. This table also illustrates that, due to our use of a caliper when calling `quickblock` above, one observation was not included in treatment assignment.

```{r blockingeval}
blockingDescEval <- dat1 %>% 
  group_by(blocksV4) %>%
  summarize(
    cov2diff = max(abs(cov2)) - min(abs(cov2)),
    cov1 = mean(cov1bin),
    count_in_block = n()
    )

blockingDescEval
```

### Disadvantages to consider

Block-randomized assignment and analysis can help reduce both estimation error and non-treatment-related variability in our data. It may also be useful for ensuring equal distribution of treatment arms within less common subgroups in our sample, which can be especially important for preserving our statistical power when estimating heterogenous treatment effects.

However, there are some practical disadvantages to consider. Block-randomized assignment can make treatment administration more complicated, both in terms of implementation by our partners and determining how we should incorporate blocking into our estimation strategy. Especially when a project needs to be rolled out on a short timeline, including a more complex blocking scheme might not be realistic.

It may often be reasonable to conclude that the benefits of blocking are not worth the extra effort it entails. Adjusting for prognostic (i.e., correlated with $Y$) covariates during the analysis stage may provide sufficient improvements in precision, especially in cases where we expect a design to already be reasonably powered under simple or complete random assignment or where we do not expect subgroup analyses (particularly for rare subgroups) to be a key component of an evaluation. But note that in smaller samples without block-randomized assignment, there might be a relatively greater risk of increasing variance by, e.g., adjusting for non-prognostic covariates [@miratrix2013adjusting].

## Cluster random assignment

We often implement a new policy intervention at the level of some group of people --- like a doctor's practice, or a building, or some other administrative unit. Even though we have `r nrow(dat1)` units in our example data, imagine now that they are grouped into 10 buildings, and the policy intervention is at the building level. Below, we assign `r nrow(dat1)/2` of those units to treatment and `r nrow(dat1)/2` to control. Everyone in each building has the same treatment assignment.

::: {.tab} 
<button class="tablinks" onclick="unrolltab(event, 'ch4R8')">R code</button>
<button class="tablinks" onclick="unrolltab(event, 'ch4Stata8')">Stata code</button>
<button class="tablinks" onclick="unrolltab(event, 'ch4Hide8')">Hide</button>
::: {#ch4R8 .tabcontent} 
<br />
```{r, highlight = F, eval = F}
## Make an indicator for cluster membership
ndat1 <- nrow(dat1)
dat1$buildingID <- rep(1:(ndat1/10), length=ndat1)
set.seed(12345)
dat1$Zcluster <- labelTC(cluster_ra(cluster=dat1$buildingID))
with(dat1, table(Zcluster, buildingID))
```
:::
::: {#ch4Stata8 .tabcontent} 
<br />
```{stata, highlight = F, eval = F}
** Make an indicator for cluster membership
qui count
global ndat1 = r(N)
egen buildingID = seq(), from(1) to(10) block(1)
set seed 12345
cluster_ra zcluster, cluster_var(buildingID)
table zcluster buildingID
```
::: 
::: {#ch4Hide8 .tabcontent}
::: 
:::

```{r clusterRA, eval = T, echo = F}
## Make an indicator for cluster membership
ndat1 <- nrow(dat1)
dat1$buildingID <- rep(1:(ndat1/10), length=ndat1)
set.seed(12345)
dat1$Zcluster <- labelTC(cluster_ra(cluster=dat1$buildingID))
with(dat1, table(Zcluster, buildingID))
```

Cluster randomized designs raise new questions about estimation, testing, and statistical power. We describe our approaches to estimation and power
analysis of cluster randomized designs in the chapter on analysis decisions.

## Other randomized designs

In the past, our team has also employed stepped-wedge style designs, saturation designs aimed at discovering whether the effects of the experimental intervention are communicated across people (via some spillover or network mechanism), and
designs where we try to isolate certain experimental units (like buildings)
from each other so that we can focus our learning about the effects of the
intervention in isolation (rather than the effects when people can communicate with each other about the intervention). There are a variety of more specialized randomization designs that may be appropriate for particular projects, and the options discussed above should not be treated as exhaustive. We may expand on some of these other randomization options here in the future.

## Pseudo-random assignment

In some circumstances, we might judge that randomly assigning a treatment of interest would be logistically infeasible. There are a variety of methods we have applied in such cases in the past to ensure assignment to treatment is at least idiosyncratic or arbitrary. What is key here is not really that assignment is random, *per se*. Instead, assignment must be conditionally independent of a unit's potential outcomes [@holland:1986a]. Sometimes, there are available methods of assigning treatment non-randomly that we decide are still likely to satisfy this condition. Of course, relying on any pseudo-random assignment procedure makes it especially important to review evidence of appropriate treatment assignment later (see the next section).

We list a few examples of pseudo-random procedures below, all of which link to Analysis Plans pre-registered on our website. But note that a pseudo-random procedure that satisfies conditional independence in one study will not necessarily satisfy it in another. This needs to be determined on a case-by-case basis. It is usually ideal to layer two arbitrary assignment procedures on top of each other rather than rely on the plausibility of only one.

* Grouping people into partitions based on the last two digits of their SSN and then [rotating each partition's treatment assignment monthly](https://oes.gsa.gov/assets/analysis/1902-analysis-plan.pdf)

* Assigning program applicants to different conditions based on the [interaction of their submission time and submission day](https://oes.gsa.gov/assets/analysis/2310-decreasing-SNAP-denial-rates_analysis-plan.pdf) (is the last digit of each even or odd?)

* Assigning callers to different conditions [based on the last four digits of their phone number](https://oes.gsa.gov/assets/analysis/2309-decreasing-abandonment-of-calls-to-988-analysis-plan.pdf)

## Assessing randomization (balance testing)

If we have covariates, we can evaluate the implementation of a random assignment procedure by exploring covariate differences across treatment arms. The goal is to ensure that the results appear consistent with our intended randomization strategy [@imai2008misunderstandings]. In the absence of covariates, we can at least assess whether the number of units assigned to each arm (conditional on other design features, such as blocking or stratification) is consistent with our intended strategy.

There are a number of ways a researcher might use covariates to provide evidence that randomized assignment was implemented as intended. But it's good to be cautious about methods that rely on statistical significance testing. This is especially true when performing separate significance tests for many covariates, which raises "multiple testing" concerns (see Chapter 5). Briefly, it would be easy to discover one or a few covariates with individual differences-in-means that differ detectably from zero due to random chance (rather than real implementation problems). That is, in a well-operating experiment, we would expect some baseline imbalances for individual covariates---roughly 5 in 100.

Another issue is the relationship between significance test results and sample size. If a sample is too small, meaningful imbalances may still happen to be statistically insignificant. And on the other hand, when working with large samples, truly negligible differences may still happen to be statistically significant. When comparing individual covariates across treatment arms, one strategy for dealing with this is to rely on *equivalence tests*, such as the "Two One-Sided Test" (TOST) procedure [@hartman2018equivalence; @rainey2014arguing].^[This is also useful when interpreting "null effects" for a study's primary outcome measure.] Another strategy is to calculate *statistics that are less sensitive to sample size* like standardized mean differences (e.g., Cohen's $d$ or Hedge's $g$) and variance ratios.^[It's sometimes useful to go further and look at things like Kolmogorov-Smirnov statistics, Q-Q plots, or statistics for various polynomial transformations of continuous covariates.] A common heuristic is that standardized mean differences greater than around 0.1 or 0.2 are more likely to represent a meaningful imbalance. But this is not a hard and fast rule. For either strategy, results need to be judged on a case-by-case basis with a study's policy context and research goals in mind.

That said, there is often still value in judging whether a sample appears sufficiently incompatible with the joint (or "omnibus") null hypothesis of no overall covariate differences between treatment arms. This focus on overall imbalance helps sidestep the multiple testing concerns noted above (but not the sample size concerns). This kind of test is often implemented [in practice](https://blogs.worldbank.org/en/impactevaluations/tools-trade-joint-test-orthogonality-when-testing-balance) by regressing a treatment indicator on a set of covariates and then calculating a p-value based on the model's F-statistic. Failing to find a statistically significant difference in an *omnibus F-test* does not definitively prove that there are no concerning imbalances. But finding a statistically significant difference tells you that a closer look at covariate balance is clearly needed.

There are other ways to perform an omnibus balance test as well, such as using a Wald test or a likelihood ratio (LR) test.^[For an LR test, for instance, the *unrestricted* model would regress treatment on the set of covariates, while the *restricted* model would include only an intercept. This approach may be easier to program than the F-test version when estimating balance on some covariates while adjusting for block fixed effects. In that case, both models should include block fixed effects, and the restricted model would simply drop the covariates (but would no longer be "intercept only").] Additionally, see @hansen_covariate_2008 for an additional omnibus test option called the $d^2$ test, and some evidence that omnibus tests relying on randomization-inference maintain appropriate false positive rates better than other methods we discuss here (the $d^2$ test can be performed under either randomization inference or standard asymptotic-theory inference). 

At a minimum, the Methods Team recommends that all OES projects involving random assignment to treatment perform some form of an omnibus test, and then plan to explore individual covariate imbalances further if this test suggests rejecting the joint null of no covariate differences (using, for example, one of the alternatives discussed above). Time-permitting, more thorough balance checks are always valuable. The Methods Team is happy to consult on what kinds of checks might be most appropriate for particular projects.

We provide an example of an omnibus test below with a binary treatment, a continuous outcome, and 2 covariates. In this case we use the $d^2$ omnibus balance test function `xBalance()` in the package `RItools` [@hansen_covariate_2008; @bowers_ritools_2016]. The overall $p$-value below shows us that this test provides little evidence against the null hypothesis that treatment ($Z$) really was assigned at random---at least in terms of the relationships between treatment assignment and available covariates. You can think of the test statistic here as a summary measure of mean differences across each covariate.

::: {.tab} 
<button class="tablinks" onclick="unrolltab(event, 'ch4R9')">R code</button>
<button class="tablinks" onclick="unrolltab(event, 'ch4Stata9')">Stata code</button>
<button class="tablinks" onclick="unrolltab(event, 'ch4Hide9')">Hide</button>
::: {#ch4R9 .tabcontent} 
<br />
```{stata, highlight = F, eval = F}
randAssessV1 <- balanceTest(Z~cov1+cov2, data=dat1)
randAssessV1$overall[,]
```
:::
::: {#ch4Stata9 .tabcontent} 
<br />
```{stata, highlight = F, eval = F}
* ssc install xbalance.
* See "help xbalance" for additional Stata setup instructions.
* This is calling the RItools R package, so you will need R installed.
* The necessary path to Rterm.exe may look something like this:
global Rterm_path "C:\Program Files\R\R-4.2.1\bin\x64\Rterm.exe"
gen single_block = 1 // To force only unstratified balance testing
label val zblockV2 // Remove value labels first
label val z

xbalance z single_block cov1 cov2
```
::: 
::: {#ch4Hide9 .tabcontent}
::: 
:::

We can assess the implementation of block-randomized assignment using a similar strategy:

::: {.tab} 
<button class="tablinks" onclick="unrolltab(event, 'ch4R10')">R code</button>
<button class="tablinks" onclick="unrolltab(event, 'ch4Stata10')">Stata code</button>
<button class="tablinks" onclick="unrolltab(event, 'ch4Hide10')">Hide</button>
::: {#ch4R10 .tabcontent} 
<br />
```{stata, highlight = F, eval = F}
randAssessV3 <- balanceTest(ZblockV3~cov1+cov2+strata(blocksV3), data=dat1)
randAssessV3$overall[,]
```
:::
::: {#ch4Stata10 .tabcontent} 
<br />
```{stata, highlight = F, eval = F}
* zblockV2 instead of zblockV3
* zblockV3 is generated using blockTools, with no Stata equivalent
xbalance zblockV2 blockV2 cov1 cov2 
```
::: 
::: {#ch4Hide10 .tabcontent}
::: 
:::

Lastly, we could assess randomization under both blocked and clustered random assignment using a regression model like the following, where `strata(blockID)` and `cluster(clusterID)` refer to block and cluster fixed effects (FEs), respectively.

`Z ~ cov1 + cov2 + strata(blockID) + cluster(clusterID)`

Again, a test statistic summarizing the estimated differences across covariates would be used to calculate a p-value using standard methods. This approach, implemented for instance by the `RItools` package, works well for experiments that are not overly small. But as noted above, in a very small experiment (with relatively few clusters), we might have a more appropriately powered balance test if we rely on randomization-based inference methods instead (i.e., the design-based approach to inference discussed in Chapter 3).

### What to do with "failed" randomization assessments?

Observing a $p$-value of less than .05 in an omnibus test like the one above ought to trigger extra scrutiny about implementation and/or how the data were recorded. For example, we might respond by contacting our agency partner to learn more about how random numbers were generated or how random assignment code was used (particularly if we didn't perform the random assignment ourselves). In many circumstances, this follow-up investigation might suggest that random assignment was implemented correctly, and that our understanding of the design or the data was simply incorrect (i.e., the balance test was not performed correctly). But sometimes, a follow-up investigation may not turn up any misunderstandings at all. In those situations, we will need to determine whether our rejection of the null hypothesis of appropriate random assignment is a false positive, or evidence of a more systematic problem.

If our rejection of the null hypothesis appears to be driven by one or more covariates that are substantively important---say, the variable `age` looks very imbalanced between treated and control groups in a health-related randomized trial---then we might present both the unadjusted results and a separate set of results that adjust for the covariate(s) in question (e.g., through a stratified difference-in-means estimator, or by using them as controls in a linear regression). Large differences between the adjusted and unadjusted estimates might help us interpret our findings: estimating separate effects within different age groups, for example, might tell us something useful about the particular context of a study and inform the conclusions we draw.

```{r savedat1, echo=FALSE}
write.csv(dat1,file="dat1_with_designs.csv")
```
