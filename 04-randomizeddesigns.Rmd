```{r setupdat13,  echo=FALSE, include=FALSE}
if(!exists("dat1")){ dat1 <- read.csv(here::here('dat1.csv')) }
```

# Randomization and Design

After working together with our agency partners to translate insights from the social and behavioral sciences into potential policy recommendations, we assess those new ideas by observing differences or changes in real world outcomes (usually measured using existing administrative data).^[We develop the recommendations themselves following [processes](https://oes.gsa.gov/methods/) described in more detail on our website.] In most cases, we design a randomized control trial (an RCT) to ensure that the differences or changes we observe are driven by the policy intervention itself. Here, we show examples of the different methods we consider for randomly assigning units to treatment. These form the core of the different types of RCTs that we use to build evidence about the effectiveness of the new policies.

## Coin flipping randomization versus urn-drawing randomization

Many discussions of RCTs begin by talking about the intervention being assigned to units (people,  schools, offices, districts) "by the flip of a coin," or **simple** random assignment. In other words, each unit's assignment to treatment occurs separately, and there is no *ex ante* guarantee as to exactly what the final number of treated or control units will be. We rarely use this method in practice, even though it is a useful way to introduce the idea that RCTs guarantee fair access to the new policy.

The following code contrasts coin-flipping style random assignment with drawing-from-an-urn style, or **complete** random assignment (where a fixed number of units are randomly chosen for treatment). Coin-flipping based experiments are still valid and tell us about the underlying counterfactuals, but they can have less statistical power, so we often try to avoid them.

Notice that the simple random assignment implemented in the code below results in more observations in the treatment group (group `T`) than in the control group (group `C`). Complete random assignment will always assign 5 units to the treatment, 5 to the control.

```{r completera}
## Start with a small experiment with only 10 units
n <- 10

## Set a random seed for replicability
set.seed(12345)

## Function to add more intuitive labels
labelTC <- function(assignments) {ifelse(assignments == 1, "T", "C")}

## Simulation using functions from the randomizr package
trt_coinflip <- labelTC(simple_ra(n))
trt_urn <- labelTC(complete_ra(n))

## Coin flipping does not guarantee half and half treated and control.
## Drawing from an urn, guarantees half treated and control.
table(trt_coinflip)
table(trt_urn)

## Alternative approach using base R
# set.seed(12345)
# trt_coinflip <- labelTC(rbinom(10, size = 1, prob = .5))
# trt_urn <- labelTC(sample(rep(c(1, 0), n / 2)))
# table(trt_coinflip)
# table(trt_urn)
```

## Urn-drawing or complete randomization into 2 or more groups

We tend to use the `randomizr` R package [@R-randomizr] for simple designs
rather than the base R `sample` function because `randomizr` does some
quality control checks. Notice that we implement a check on our code below with the `stopifnot` command: the code will stop and issue a warning if we didn't actually assign 1/4 of the observations to the treatment condition. Here, we assign the units first to 2 arms with equal probability (`Z2armEqual`). Then, to show how the code works, we assign them to 2 arms where one arm has only a $\frac{1}{4}$ probability of receiving treatment (e.g., imagine a design with an expensive intervention). Last, we assign them based on a design with 4 different arms, each with equal probability (e.g., one control group and three different treatments under consideration). We often use $Z$ to refer to the variable recording our intervention arms.

```{r completera2}
N <- nrow(dat1)
set.seed(12345)

## Two equal arms
dat1$Z2armEqual <- labelTC(complete_ra(N))

## Two unequal arms: .25 chance of treatment (.75 chance of control0
dat1$Z2armUnequalA <- labelTC(complete_ra(N,prob=.25))
stopifnot(sum(dat1$Z2armUnequalA=="T")==N/4)
dat1$Z2armUnequalB <- labelTC(complete_ra(N,m=N/4))

## Four equal arms
dat1$Z4arms <- complete_ra(N,m_each=rep(N/4,4))

table(Z2armEqual=dat1$Z2armEqual)
table(Z2armUnequalA=dat1$Z2armUnequalA)
table(Z2armUnequalB=dat1$Z2armUnequalB)
table(Z4arms=dat1$Z4arms)
```

## Factorial Designs

It's possible to test the effects of more than one intervention without losing much statistical power by randomly assigning multiple treatments independently of each other. The simplest design that we use for this purpose is the $2 \times 2$ **factorial** design. For example, in the next table we see that we have assigned `r N/2` observations to each arm of two separate interventions. Since the randomization of `treatment1` is independent of `treatment2`, we can assess the effects of each treatment separately and pay less of a power penalty (unless one of the treatments dramatically increases the variance of the outcome compared to the experiment with only one treatment assigned).

```{r factdesign2x2}
## Two equal arms, adding a second cross treatment
dat1$Z2armEqual2 <- labelTC(complete_ra(N))
table(treatment1=dat1$Z2armEqual,treatment2=dat1$Z2armEqual2)
```

Although factorial designs allow us to test more than one intervention at the
same time, they tend to provide less statistical power when testing
hypotheses about the *interaction* between the two treatments. If we want to
learn about how two different interventions work together, then the sample
size requirements will be much larger than if we were satisfied with learning about each treatment separately.^[But see @small2011structured, for instance, for a way to increase the power of tests for such questions. This approach is not currently part of our standard practice.]

Note that [recent work](https://direct.mit.edu/rest/article-abstract/doi/10.1162/rest_a_01317/115272/Factorial-Designs-Model-Selection-and-Incorrect?redirectedFrom=fulltext) highlights some important concerns regarding (1) the interpretation of factorial treatment effects and (2) the consequences of omitting interaction terms when estimating separate effects of each treatment [@muralidharan2023factorial]. In regards to (2), even if the interaction term is not of interest, including it in the estimation model may be important for making correct inferences.^[In particular, if the interaction is not truly zero, excluding it from the model could increase the risk of Type I errors (i.e., false positives).] Meanwhile, in regards to (1), these authors emphasize that a factorial treatment effect from a model without an interaction should be interpreted as a weighted average of interactions with other treatments administered across the sample.^[As the authors discuss, this could be framed as an internal or external validity concern.]

## Block Random Assignment

Statistical power depends not only on the size of the experiment and the strength of the treatment effect, but also on the amount of noise, or non-treatment-related variability, in outcomes. Block-randomized designs can help reduce this noise while simultaneously minimizing estimation error -- the amount that our particular experiment's estimate differs from the truth.  

In a **block-randomized**, or stratified, design, we randomly assign units to the policy intervention *within* pre-specified groups.^[Some studies employ post-stratification, where simple or complete randomization might be used to assign treatment, but the data are still analyzed in a stratified manner. See @miratrix2013adjusting, for instance, for more discussion.] 

Suppose we are evaluating whether dedicated navigators can increase the percentage of students living in public housing who complete federal financial aid applications (FAFSA). Our experiment will send navigators to two of four eligible buildings, two of which are large and two of which are small. In a real study we can never know the outcome in all buildings both with and without navigators (the "fundamental problem of causal inference" from the last chapter). But if we could, we might have the data below:

```{r maketab1, echo = F}
tibble(
  Building = c(1:4, "Mean"),
  Size = c(rep.int("Large", 2), rep.int("Small", 2), " "),
  `% FAFSA (No Navigator)` = c(20, 30, 20, 30, 25),
  `% FAFSA (With Navigator)` = c(60, 70, 30, 40, 50)
  ) %>%
  knitr::kable() %>%
  kableExtra::row_spec(4, extra_css = "border-bottom: 1px solid") %>%
  kableExtra::row_spec(5, extra_css = "border-bottom: 1px solid") %>%
  kable_styling(full_width = F)

# This LaTex code was used previously
# \begin{center}
# \begin{tabular}{ccccc}
# &&& FAFSA \% & FAFSA \% \\
# Building & Size & Navigators? & if No Navigators & if Navigators   \\ \hline
# 1 & Large & $\_$ & 20 & 60 \\
# 2 & Large & $\_$ & 30 & 70 \\
# 3 & Small & $\_$ & 20 & 30 \\
# 4 & Small & $\_$ & 30 & 40 \\ \hline
# && Means: & 25 & 50 \\
# \end{tabular}
# \end{center}
```

The true average treatment effect for this sample is the average under treatment (i.e., the average treated potential outcome) minus the average under control (i.e., the average control potential outcome): $\text{ATE} = 50-25=25$ percent more applications per building when a navigator is deployed.

In a real study, we might randomly allocate two buildings to treatment and two buildings to control. If complete random assignment led to us treating the first two buildings, then we might observe:

```{r maketab2, echo = F}
tibble(
  Building = c(1:4, "Mean"),
  Size = c(rep.int("Large", 2), rep.int("Small", 2), " "),
  Treated = c(1, 1, 0, 0, " "),
  `% FAFSA (No Navigator)` = c(" ", " ", 20, 30, 25),
  `% FAFSA (With Navigator)` = c(60, 70, " ", " ", 65)
  ) %>%
  knitr::kable() %>%
  kableExtra::row_spec(4, extra_css = "border-bottom: 1px solid") %>%
  kableExtra::row_spec(5, extra_css = "border-bottom: 1px solid") %>%
  kable_styling(full_width = F)

# This LaTex code was used previously
# \begin{center}
# \begin{tabular}{ccccc}
# &&& FAFSA \% & FAFSA \% \\
# Building & Size & Navigators? & if No Navigators & if Navigators   \\ \hline
# 1 & Large & Yes &  & 60 \\
# 2 & Large & Yes &  & 70 \\
# 3 & Small & No & 20 &  \\
# 4 & Small & No & 30 &  \\ \hline
# && Means: & 25 & 65 \\
# \end{tabular}
# \end{center}
```

This yields a treatment effect estimate of $65-25 = 40$ percent more applications due to the presence of a navigator. This is *larger* than the true value of $25$.

Or, if random assignment led to the other two buildings being treated instead, we might then observe:

```{r maketab3, echo = F}
tibble(
  Building = c(1:4, "Mean"),
  Size = c(rep.int("Large", 2), rep.int("Small", 2), " "),
  Treated = c(0, 0, 1, 1, " "),
  `% FAFSA (No Navigator)` = c(20, 30, " ", " ", 25),
  `% FAFSA (With Navigator)` = c(" ", " ", 30, 40, 35)
  ) %>%
  knitr::kable() %>%
  kableExtra::row_spec(4, extra_css = "border-bottom: 1px solid") %>%
  kableExtra::row_spec(5, extra_css = "border-bottom: 1px solid") %>%
  kable_styling(full_width = F)

# This LaTex code was used previously
# \begin{center}
# \begin{tabular}{ccccc}
# &&& FAFSA \% & FAFSA \% \\
# Building & Size & Navigators? & if No Navigators & if Navigators   \\ \hline
# 1 & Large & $\_$ &  & 60 \\
# 2 & Large & $\_$ & 30 &  \\
# 3 & Small & $\_$ &  & 30 \\
# 4 & Small & $\_$ & 30 &  \\ \hline
# && Means: & 30 & 45 \\
# \end{tabular}
# \end{center}
```

This, in contrast, yields an estimated treatment effect of $35-25 = 10$ percentage point more applications due to the navigators -- now *smaller* than the true value of $25$.

All of the possible (equiprobable) assignments with two treated and two control units, along with their estimated treatment effects, are listed in the table below:

```{r maketab4, echo = F}
tibble(
  Assignments = c("TTCC", "CTCT", "TCCT", "CTTC", "TCTC", "CCTT"),
 `Estimated Effect` = c(40, 35, 25, 25, 15, 10)
  ) %>%
  knitr::kable() %>%
  kable_styling(full_width = F)
# This LaTex code was used previously
# \begin{center}
# \begin{tabular}{cc}
# Assignments & Est TE \\ \hline
# YYNN & 40  \\ 
# NYNY & 35 \\
# YNNY & 25 \\
# NYYN & 25 \\
# YNYN & 15 \\
# NNYY & 10
# \end{tabular}
# \end{center}
```

These possible treatment effect estimates have a mean equal to the true value of 25, illustrating the difference in means is an unbiased estimator. However, some of these estimates are far from the truth, and they have a lot of variability.

To design an experiment that better estimates the true value, and does so with more statistical power (less variability), we can randomly assign units within *blocks*. In general, units should be sorted into different blocks based on their similarity across one or more characteristics that we expect to be correlated with our outcome. Here, blocking implies restricting the possible random assignments to those that have one large and one small building in each treatment group:

```{r maketab5, echo = F}
tibble(
  Assignments = c("CTCT", "TCCT", "CTTC", "TCTC"),
 `Estimated Effect` = c(35, 25, 25, 15)
  ) %>%
  knitr::kable() %>%
  kable_styling(full_width = F)
# This LaTex code was used previously
# \begin{center}
# \begin{tabular}{cc}
# Assignments & Est TE \\ \hline
# NYNY & 35 \\
# YNNY & 25 \\
# NYYN & 25 \\
# YNYN & 15 \\
# \end{tabular}
# \end{center}
```

With this blocked design restricting the random assignments that are possible, we now get an estimate that is no more than 10 percentage points from the truth. Further, our estimates will have less variability (an SD of `r round(sd(c(35, 25, 25, 15)), 2)` rather than `r round(sd(c(35, 25, 25, 15, 40, 10)), 2)`). This improves the statistical power of our design.

For a more realistic example, suppose we are designing an experiment
where the sample includes patients from two different hospitals. We might randomly assign patients to treatment and control *within* each hospital. For instance, we might assign half of the patients in hospital "A" to treatment and half to control, then do the same in hospital "B."^[We use "block" rather than "strata" throughout this document to refer to groups of experimental units that are fixed *before random assignment occurs*.] Below, we have `r nrow(dat1) / 2` units in hospital "A" and `r nrow(dat1) / 2` in hospital "B":

```{r blockra1}
dat1$blockID <- gl(n = 2, k = N/2, labels = c("Block A", "Block B"))
with(dat1,table(blockID=dat1$blockID))
```

We assign half of the units in each hospital to each treatment condition:

```{r blockra15}
dat1$Z2armBlocked <- labelTC(block_ra(blocks=dat1$blockID))
with(dat1, table(blockID, Z2armBlocked))
```

If, say, there were fewer people eligible for treatment in hospital "A" --- or perhaps the intervention was more expensive in that block --- we might choose different treatment probabilities for each block. The code below assigns half of the hospital "A" patients to treatment, but only a quarter of those from hospital "B". Again, we also check that this code worked. This approach is an informal version of one of the best practices for writing code in general, called "unit testing." See the [EGAP Guide to Workflow](https://egap.org/resource/10-things-to-know-about-project-workflow/) for more examples.

```{r blockra2}
## Blocked assignment, unequal probability
dat1$Z2armBlockedUneqProb <- labelTC(block_ra(blocks=dat1$blockID, block_prob=c(.5,.25)))
with(dat1, table(blockID, Z2armBlockedUneqProb))

## Unit testing
NumTreatedB <- sum(dat1$Z2armBlockedUneqProb=="T" & dat1$blockID=="Block B")
ExpectedNumTreatedB <- sum(dat1$blockID=="Block B")/4
stopifnot(NumTreatedB==ceiling(ExpectedNumTreatedB) | NumTreatedB==floor(ExpectedNumTreatedB))
```

Our team tries to implement block-randomized assignment whenever possible in
order to increase the statistical power of our experiments. We also often find
it useful in cases where different administrative units are implementing the
treatment, or when we expect different groups of people to have different reactions to the treatment.

### Using only a few covariates to create blocks

If we have background information on a few covariates, we can create blocks by hand through a process like the one demonstrated here:

```{r cutcovs}
## For example, make three groups from the cov2 variable
dat1$cov2cat <- with(dat1, cut(cov2, breaks=3))
table(dat1$cov2cat, exclude=c())
with(dat1, tapply(cov2, cov2cat, summary))

## And we can make blocks that are the same on two covariates
dat1$cov1bin <- as.numeric(dat1$cov1>median(dat1$cov1)) # Binarize cov1
dat1$blockV2 <- droplevels(with(dat1, interaction(cov1bin, cov2cat)))
table(dat1$blockV2, exclude=c())

## And then assign within these blocks
set.seed(12345)
dat1$ZblockV2 <- labelTC(block_ra(blocks = dat1$blockV2))
with(dat1, table(blockV2, ZblockV2, exclude=c()))
```

### Multivariate blocking using many covariates

If instead we have many background variables, we can increase precision by thinking about the problem of blocking as one of matching, or creating sets of
units which are as similar as possible overall, across the entire set of covariates [@moore2012multivariate;@moore2016bT063]. Here we show two approaches.

Creating pairs:

```{r}
## Using the blockTools package
mvblocks <- block(dat1, id.vars="id", block.vars=c("cov1","cov2"), algorithm="optimal")
dat1$blocksV3 <- createBlockIDs(mvblocks, data=dat1, id.var = "id")
dat1$ZblockV3 <- labelTC(block_ra(blocks = dat1$blocksV3))

## Just show the first ten pairs
with(dat1,table(blocksV3,ZblockV3,exclude=c()))[1:10,]
```

Creating larger blocks:

```{r}
## Using the quickblock package
distmat <- distances(dat1, dist_variables = c("cov1bin", "cov2"), id_variable = "id", normalize="mahalanobiz")
distmat[1:5,1:5]
quantile(as.vector(distmat), seq(0,1,.1))

## The caliper argument helps prevent ill-matched points
mvbigblock <- quickblock(distmat, size_constraint = 6L, caliper = 2.5)

## Look for missing points
table(mvbigblock,exclude=c()) # One point dropped due to caliper
dat1$blocksV4 <- mvbigblock
dat1$notblocked <- is.na(dat1$blocksV4) 
dat1$ZblockV4[dat1$notblocked==F] <- labelTC(block_ra(blocks = dat1$blocksV4))
with(dat1, table(blocksV4, ZblockV4, exclude=c()))[1:10,]
```

It's worth pausing to examine the differences within blocks. We'll focus on the proportion of people in category "1" on the binary covariate (notice that the blocks are homogeneous on this covariate), as well as the difference between the largest and smallest value of the continuous covariate. This table also illustrates that, due to our use of a caliper when calling `quickblock` above, one observation was not included in treatment assignment.

```{r blockingeval}
blockingDescEval <- dat1 %>% 
  group_by(blocksV4) %>%
  summarize(
    cov2diff = max(abs(cov2)) - min(abs(cov2)),
    cov1 = mean(cov1bin),
    count_in_block = n()
    )

blockingDescEval
```

## Cluster random assignment

We often implement a new policy intervention at the level of some group of people --- like a doctor's practice, or a building, or some other administrative unit. Even though we have `r nrow(dat1)` units in our example data, imagine now that they are grouped into 10 buildings, and the policy intervention is at the building level. Below, we assign `r nrow(dat1)/2` of those units to treatment and `r nrow(dat1)/2` to control. Everyone in each building has the same treatment assignment.

```{r clusterRA}
## Make an indicator for cluster membership
ndat1 <- nrow(dat1)
dat1$buildingID <- rep(1:(ndat1/10), length=ndat1)
set.seed(12345)
dat1$Zcluster <- labelTC(cluster_ra(cluster=dat1$buildingID))
with(dat1, table(Zcluster, buildingID))
```

Cluster randomized designs raise new questions about estimation, testing, and statistical power. We describe our approaches to estimation and power
analysis of cluster randomized designs in the next chapter.

## Other designs

Our team has also employed stepped-wedge style designs, saturation designs
aimed at discovering whether the effects of the experimental intervention are
communicated across people (via some spillover or network mechanism), and
designs where we try to isolate certain experimental units (like buildings)
from each other so that we can focus our learning about the effects of the
intervention in isolation (rather than the effects when people can communicate with each other about the intervention). We may discuss some of these in more detail in later versions of this guide.

## Assessing randomization (balance testing)

If we have covariates, we can evaluate the implementation of a random assignment procedure by testing the hypothesis that the treatment-vs-control differences in covariates, or differences across treatment arms, are consistent with our intended randomization strategy. Also, in the absence of covariates, we can at least assess whether the number of units assigned to each arm (conditional on other design features, such as blocking or stratification) is consistent with our intended strategy.

We provide an example below with a binary treatment, a continuous outcome, and 10 covariates. In this case we use the $d^2$ omnibus balance test function `xBalance()` in the package `RItools` [see @hansen_covariate_2008; @bowers_ritools_2016].

The overall $p$-value below shows us that this test provides little evidence against the null hypothesis that treatment ($Z$) really was assigned at random --- at least in terms of the relationships between treatment assignment and the two covariates. The test statistic here is a summary measure of mean differences across each covariate.

Keep in mind that the overall or "omnibus" nature of this test is key. If we had many covariates or many observations, it would easy to discover one or a few covariates with individual differences-in-means that differ detectably from zero due to random chance (rather than real implementation problems). That is, in a well-operating experiment, we would expect some baseline imbalances for individual covariates -- roughly 5 in 100. The value of an omnibus test is that it should not be so easily misled by these chance false positives.

```{r}
randAssessV1 <- balanceTest(Z~cov1+cov2, data=dat1)
randAssessV1$overall[,]
```

We can also assess the implementation of block-randomized assignment:

```{r}
randAssessV3 <- balanceTest(ZblockV3~cov1+cov2+strata(blocksV3), data=dat1)
randAssessV3$overall[,]
```

Lastly, it is possible to assess randomization under both blocked and clustered random assignment using a regression model like the following, where `strata(blockID)` and `cluster(clusterID)` refer to block and cluster fixed effects (FEs), respectively.

`Z ~ cov1 + cov2 + strata(blockID) + cluster(clusterID)`

Again, a test statistic summarizing the estimated differences across covariates would then be used to calculate a p-value. This approach, implemented for instance by the `RItools` package, works well for experiments that are not overly small. In a very small experiment (with relatively few clusters) we might estimate this same regression model, but we would not compare our test statistic to the $\chi^2$ sampling distribution to calculate a p-value. Instead, we would perform a permutation-based test (see the previous chapter).

We sometimes prefer not use $F$-tests or Likelihood Ratio tests to assess randomization. See @hansen_covariate_2008 for some evidence that a test based on randomization-inference (like the $d^2$ test developed in that article) maintains appropriate false positive rates better than the sampling- or likelihood-justified $F$ and likelihood ratio tests.

### What to do with "failed" randomization assessments?

Observing a $p$-value of less than .05 in an omnibus test like the one above ought to trigger extra scrutiny about implementation and/or how the data were recorded. For example, we might respond by contacting our agency partner to learn more about how random numbers were generated or the code was used (particularly if we didn't perform the random assignment ourselves). In many circumstances, this follow-up investigation might indicate that random assignment was implemented correctly, and that our understanding of the design or the data was simply incorrect. But sometimes, the follow-up investigation may not turn up any misunderstandings at all. In those situations, we will tend to assume that our rejection of the null hypothesis of appropriate random assignment was a false positive (consider that we would expect to see about 5 such errors in every 100 experiments).

If our rejection of the null hypothesis appears to be driven by one or more covariates that are substantively important --- say, the variable `age` looks very imbalanced between treated and control groups in a health-related randomized trial --- then we might present both the unadjusted results and a separate set of results that adjust for the covariate(s) in question (e.g., through a stratified difference-in-means estimator, or by using them as controls in a linear regression). Importantly, under a chance rejection of the null (i.e., a false positive) we would not want to treat the adjusted estimate as our primary finding --- in such a situation, the adjusted estimate would be biased while the unadjusted estimate would not. However, large differences between the adjusted and unadjusted estimates might help us interpret our findings: estimating separate effects within different age groups, for example, might tell us something useful about the particular context of a study and inform the conclusions we draw.

### Minimizing the chances of "failed" randomization

Our preference for block-randomization helps us avoid these problems. We can also restrict random assignments in more flexible ways that minimize opportunities for error even more. That said, we prefer not to rely on **re-randomization** --- one of the other possible restricted randomization options --- in the interests of minimizing the complexity of our analyses. But since we adopt a randomization-based approach to the analysis of  experiments, we can easily (in concept) estimate causal effects under a wide variety of random assignment strategies.

```{r savedat1, echo=FALSE}
write.csv(dat1,file="dat1_with_designs.csv")
```

