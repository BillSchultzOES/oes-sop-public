```{r setupdat14, echo=FALSE}
## Read in the dat1 file created from 04-randomizeddesigns.Rmd
dat1 <- read.csv(here::here('dat1_with_designs.csv'))
dat1$rankY <- rank(dat1$Y)
ndat1 <- nrow(dat1)
```

# Analysis choices

<!-- Adds copy code button -->
```{r klippych5, echo=FALSE, include=TRUE}
klippy::klippy(all_precode = T, position = c("top", "right"))
```

<!-- Used (and iteratively updated) in the {oes_code_tab} snippets below. -->
<!-- set chapter number and reset count -->
```{r, include = F, echo = F}
# cnum is modified automatically to iteratively count chunks
# when using the oes_code_tab markdown snippet. each use of
# the snippet adds a value of 1.
ch <- 5
cnum <- 0
```

We've organized our discussion of analysis tactics in this chapter by a study's design. After all, different study designs often require different analyses. But there are a few general tactics that we use to ensure that we can make transparent, valid, and statistically precise statements about the results of our research. We'll start this chapter by discussing those.

First, the nature of the data that we expect to see from a given experiment informs our analysis plans. For example, we may make some choices based on the nature of the outcome --- a binary outcome, a symmetrically distributed continuous outcome, and a heavily skewed continuous outcome each could each call for different analytical approaches.

Second, we tend to ask three different questions in each of our studies, and we answer them with different statistical procedures:

 1. Can we *detect an effect* in our experiment? (We use hypothesis tests to answer this question.)
 2. What is our best guess about the *size of the effect* of the experiment? (We estimate the average treatment effect of our interventions to answer this question.)
 3. *How precise* is our guess? (We report confidence intervals or standard errors to answer this question.)

Finally, in the [Analysis Plans](https://oes.gsa.gov/methodsdetail/#analysis-plans) that we post online before receiving outcome data for a project, we try to anticipate many common decisions involved in data analysis --- how we will treat missing data, how we will rescale, recode, and combine columns of raw data, etc. We touch on some of these topics in more detail below, and will cover others in a future chapter on **Working with Data**.

## Completely or Urn-Draw Randomized Trials

### Two arms

#### Continuous outcomes

In a completely randomized trial where outcomes take on many levels (units
like times, counts of events, dollars, percentages, etc.) we generally assess the *weak null hypothesis* of no average effects, estimate an average treatment effect, and may also assess the *sharp null hypothesis* of no effect for any unit using some test statistic beside a difference-in-means.^[What we call the "weak null" here corresponds to the null hypothesis this is evaluated by default in many software packages (like R and Stata) when implementing common procedures like a difference-in-means test or a linear regression. Meanwhile, the "sharp null" is more often evaluated when employing permutation-based procedures like those discussed in our earlier chapter on design-based inference.] This last assessment allows us to check on whether our choice to focus on mean differences matters for our substantive interpretation of the results.

##### Estimating the average treatment effect and testing the weak null of no average effects

We show the kind of code we use for these purposes here. Below, `Y` is the outcome variable and `Z` is an indicator of the assignment to treatment. 

::: {.tab} 
<button class="tablinks" onclick="unrolltab(event, 'ch5R1')">R code</button>
<button class="tablinks" onclick="unrolltab(event, 'ch5Stata1')">Stata code</button>
<button class="tablinks" onclick="unrolltab(event, 'ch5Hide1')">Hide</button>
::: {#ch5R1 .tabcontent} 
<br />
```{r, highlight = F, eval = F}
## This function comes from the estimatr package
estAndSE1 <- difference_in_means(Y ~ Z,data = dat1)
print(estAndSE1)
```
:::
::: {#ch5Stata1 .tabcontent} 
<br />
```{stata, highlight = F, eval = F}
** Similarly to the R code, estimate the difference
** in means assuming unequal variance across treatment groups.
** There isn't a perfect Stata equivalent providing a
** design-based difference in means estimator.
ttest y, by(z) unequal
```
::: 
::: {#ch5Hide1 .tabcontent}
::: 
:::

```{r contoutcome, eval = T, echo = F}
## This function comes from the estimatr package
estAndSE1 <- difference_in_means(Y ~ Z,data = dat1)
print(estAndSE1)
```

Notice that the standard errors that we use are not the default OLS errors:

::: {.tab} 
<button class="tablinks" onclick="unrolltab(event, 'ch5R2')">R code</button>
<button class="tablinks" onclick="unrolltab(event, 'ch5Stata2')">Stata code</button>
<button class="tablinks" onclick="unrolltab(event, 'ch5Hide2')">Hide</button>
::: {#ch5R2 .tabcontent} 
<br />
```{r, highlight = F, eval = F}
estAndSE1OLS <- lm(Y~Z,data=dat1)
summary(estAndSE1OLS)$coef
```
:::
::: {#ch5Stata2 .tabcontent} 
<br />
```{stata, highlight = F, eval = F}
reg y z
```
::: 
::: {#ch5Hide2 .tabcontent}
::: 
:::

```{r notolsse, eval = T, echo = F}
estAndSE1OLS <- lm(Y~Z,data=dat1)
summary(estAndSE1OLS)$coef
```

The standard errors we prefer reflect repeated randomization from a fixed experimental pool. This is known as the HC2 standard error. @lin_agnostic_2013 and @samii_equivalencies_2012 show that the standard error estimator of an unbiased average treatment effect within a "finite-sample" or design-based framework (i.e., the Neyman standard error; see Chapter 3) is equivalent to the HC2 standard error. These SEs are produced by default by the `estimatr` package's function `difference_in_means()` and the `lmtest` package's functions `coeftest()` and `coefci()`. They can also be produced using the `vcovHC()` function from the `sandwich` package.

Our preference for HC2 errors follows from their design-based justification, but many researchers encounter them as one of several methods of correcting OLS standard errors for *heteroscedasticity*. Essentially, this means that the variance of the regression model's error term is not constant across observations.^[Classical OLS standard errors assume away heteroscedasticity, rendering them potentially inaccurate when heteroscedasticity is present.] When using OLS to analyze data from a two-arm randomized trial, heteroscedasticity might appear because the variance of the outcome is different in the treatment and control groups. This is common in practice.

##### Testing the sharp null of no effects

We may assess the sharp null of no effects via direct permutation as a check on the assumptions underlying the calculations and statistical inferences above (i.e., "randomization inference"). We tend to use a $t$-statistic as our test statistic here to parallel the above tests. But we could use a rank-based test statistic instead if we were concerned about long-tails (i.e., skew) reducing statistical power.

Below, we show how to perform these tests using two different R packages (`coin` and `ri2`), along with several Stata commands (`permute` and `ritest`).  First, the `coin` package [@R-coin] in R and `permute` in Stata:

```{stata RItest1, eval=FALSE, cache=F,include=F, echo=F}
## As of Fall 2023, RItest is only in the randomization-distribution
## development branch of RItools. This code would work if that branch
## were installed. Right now (current renv package versions, as of 9/7/23)
## it is not, and I'm (Bill) hesistant to make the code here reliant
## on a development package that some Fellows may not be able to
## easily install under GSA IT permissions. So, excluding this chunk.

# set.seed(12345)
# test1T <- RItest(y=dat1$Y,z=dat1$Z,test.stat=t.mean.difference,samples=1000)
# print(test1T)
# test1R <- RItest(y=dat1$rankY,z=dat1$Z,test.stat=t.mean.difference,samples=1000)
# print(test1R)
```

::: {.tab} 
<button class="tablinks" onclick="unrolltab(event, 'ch5R3')">R code</button>
<button class="tablinks" onclick="unrolltab(event, 'ch5Stata3')">Stata code</button>
<button class="tablinks" onclick="unrolltab(event, 'ch5Hide3')">Hide</button>
::: {#ch5R3 .tabcontent} 
<br />
```{r, highlight = F, eval = F}
## The coin package
set.seed(12345)

# Compare means
test1coinT <- oneway_test(Y~factor(Z),data=dat1,distribution=approximate(nresample=1000))
test1coinT

# Rank test 1
test1coinR<- oneway_test(rankY~factor(Z),data=dat1,distribution=approximate(nresample=1000))
test1coinR

# Rank test 2
test1coinWR <- wilcox_test(Y~factor(Z),data=dat1,distribution=approximate(nresample=1000))
test1coinWR
```
:::
::: {#ch5Stata3 .tabcontent} 
<br />
```{stata, highlight = F, eval = F}
** The permute command
set seed 12345

* Compare means
permute z z = _b[z], reps(1000) nodots: reg y z // OR: permtest2 y, by(z) simulate runs(1000)

* Rank test 1
egen ranky = rank(y)
permute z z = _b[z], reps(1000) nodots: reg ranky z

* Rank test 2
permute z z = r(z), reps(1000) nodots: ranksum y, by(z)
* OR: ranksum y, by(z) exact // exact, not approximate
```
::: 
::: {#ch5Hide3 .tabcontent}
::: 
:::

```{r otherri, eval = T, echo = F, results = "hold", collapse = T}
## The coin package
set.seed(12345)
test1coinT <- oneway_test(Y~factor(Z),data=dat1,distribution=approximate(nresample=1000))
test1coinT
test1coinR<- oneway_test(rankY~factor(Z),data=dat1,distribution=approximate(nresample=1000))
test1coinR
test1coinWR <- wilcox_test(Y~factor(Z),data=dat1,distribution=approximate(nresample=1000))
test1coinWR
```

Next, the `ri2` R package [@R-ri2] and `ritest` in Stata:

::: {.tab} 
<button class="tablinks" onclick="unrolltab(event, 'ch5R4')">R code</button>
<button class="tablinks" onclick="unrolltab(event, 'ch5Stata4')">Stata code</button>
<button class="tablinks" onclick="unrolltab(event, 'ch5Hide4')">Hide</button>
::: {#ch5R4 .tabcontent} 
<br />
```{r, highlight = F, eval = F}
## The ri2 package
thedesign1 <- randomizr:::declare_ra(N=ndat1,m=sum(dat1$Z))
test1riT <- conduct_ri(Y~Z,declaration=thedesign1,sharp_hypothesis=0,data=dat1,sims=1000)
tidy(test1riT)
test1riR <- conduct_ri(rankY~Z,declaration=thedesign1,sharp_hypothesis=0,data=dat1,sims=1000)
tidy(test1riR)
```
:::
::: {#ch5Stata4 .tabcontent} 
<br />
```{stata, highlight = F, eval = F}
** The ritest command
* ssc install ritest
ritest z z = _b[z], nodots reps(1000): reg y z 
ritest z z = r(z), nodots reps(1000): ranksum y, by(z) 
```
::: 
::: {#ch5Hide4 .tabcontent}
::: 
:::

```{r ri2example, echo = F, eval = T, results = "hold", collapse = T}
## The ri2 package
thedesign1 <- randomizr:::declare_ra(N=ndat1,m=sum(dat1$Z))
thedesign1
test1riT <- conduct_ri(Y ~ Z, declaration = thedesign1,
                       sharp_hypothesis = 0, data = dat1, sims = 1000)
tidy(test1riT)
test1riR <- conduct_ri(rankY ~ Z, declaration = thedesign1,
                       sharp_hypothesis = 0, data = dat1, sims = 1000)
tidy(test1riR)
```

It is also relatively common for OES evaluations to encounter situations where these canned approaches may not be appropriate (e.g., the study design employs an uncommon randomization scheme). In such cases, it is useful to be able to program randomization inference manually. We provide simple templates for how this could be done in the code below, focusing on complete random assignment. This yields similar results to the canned functions above.

::: {.tab} 
<button class="tablinks" onclick="unrolltab(event, 'ch5R6')">R code</button>
<button class="tablinks" onclick="unrolltab(event, 'ch5Stata6')">Stata code</button>
<button class="tablinks" onclick="unrolltab(event, 'ch5Hide6')">Hide</button>
::: {#ch5R6 .tabcontent} 
<br />
```{r, eval = T, echo = F}
## Define a function to re-randomize a single time
## and return the desired test statistic.
ri_draw <- function() {
  
  ## Using randomizr
  dat1$riZ <- complete_ra(N = nrow(dat1), m = sum(dat1$Z))
  
  ## Or base R
  # dat1$riZ <- sample(dat1$Z, length(dat1$Z), replace = F)
  
  ## Return the test statistic of interest.
  ## Simplest is the difference in means itself.
  return( with(dat1, lm(Y ~ riZ))$coefficients["riZ"] )
  
}

## We'll use replicate to repeat this many times.
ri_dist <- replicate(n = 1000, expr = ri_draw(), simplify = T)

## A loop would also work.
# i <- 1
# ri_dist <- matrix(NA, 1000, 1)
# for (i in 1:1000) {
#   ri_dist[i] <- ri_draw()
# }

## Get the real test statistic and calculate the p-value.
# How often do different possible treatment assignments,
# under a sharp null, yield test statistics with a magnitude
# at least as large as our real statistic?
real_stat <- with(dat1, lm(Y ~ Z))$coefficients["Z"]
ri_p_manual <- mean(abs(ri_dist) >= abs(real_stat))

## Compare to test1riT
ri_p_manual
```
:::
::: {#ch5Stata6 .tabcontent} 
<br />
```{stata, highlight = F, eval = F}
** Define a program to re-randomize a single time
** and return a desired test statistic.
capture program drop ri_draw
program define ri_draw, rclass

	** Using randomizr (Stata version)
	capture drop riZ
	qui sum z
	local zsum = r(sum)
	complete_ra riZ, m(`zsum')
	
	** Or manually	
		/*
		gen rand = runiform()
		sort rand
		qui sum z
		gen riZ = 1 in 1/r(sum)
		replace riZ = 0 if missing(riZ)
		drop rand
		*/
	
	** Return the test statistic of interest.
	** Simplest is the difference in means itself.
	qui reg y riZ
	return scalar riZ = _b[riZ]
	
end

** We'll use simulate to repeat this many times.
** Nested within preserve/restore to return to our data after.
preserve

	** Get the real test statistic
	qui reg y z
	local real_stat = _b[z]
	
	** Perform the simulation itself
	simulate ///
	riZ = r(riZ), ///
	reps(1000): ///
	ri_draw
	
	** Calculate the p-value.
	* How often do different possible treatment assignments,
	* under a sharp null, yield test statistics with a magnitude
	* at least as large as our real statistic?
	gen equal_or_greater = abs(riZ) >= abs(`real_stat')
	qui sum equal_or_greater
	local ri_p_manual = r(mean)
	
	** Compare to output from permute or ritest above
	di `ri_p_manual'

restore
```
::: 
::: {#ch5Hide6 .tabcontent}
::: 
:::

```{r ri_doityrself, eval = T, echo = F}
## Define a function to re-randomize a single time
## and return the desired test statistic.
ri_draw <- function() {
  
  ## Using randomizr
  dat1$riZ <- complete_ra(N = nrow(dat1), m = sum(dat1$Z))
  
  ## Or base R
  # dat1$riZ <- sample(dat1$Z, length(dat1$Z), replace = F)
  
  ## Return the test statistic of interest.
  ## Simplest is the difference in means itself.
  return( with(dat1, lm(Y ~ riZ))$coefficients["riZ"] )
  
}

## We'll use replicate to repeat this many times.
ri_dist <- replicate(n = 1000, expr = ri_draw(), simplify = T)

## A loop would also work.
# i <- 1
# ri_dist <- matrix(NA, 1000, 1)
# for (i in 1:1000) {
#   ri_dist[i] <- ri_draw()
# }

## Get the real test statistic and calculate the p-value.
# How often do different possible treatment assignments,
# under a sharp null, yield test statistics with a magnitude
# at least as large as our real statistic?
real_stat <- with(dat1, lm(Y ~ Z))$coefficients["Z"]
ri_p_manual <- mean(abs(ri_dist) >= abs(real_stat))

## Compare to test1riT
ri_p_manual
```

#### Binary outcomes

We tend to focus on differences in percentage points when we are working with binary outcomes, usually estimated via OLS linear regression. A statement like "the effect was a 5 percentage point increase" has made communication with partners easier than a discussion in terms of log odds or odds ratios. In addition to difficulties in interpretation and communication, we also avoid logistic regression coefficients because of the bias problem noticed by @freedman2008randomization in the case of covariance adjustment or more complicated research designs.

##### Estimating the average treatment effect and testing the weak null of no average effects

We can estimate effects (and produce standard errors) for differences of proportions using the same process as above. The average treatment effect estimate here represents the difference in the proportions of positive responses (i.e., $Y=1$) between treatment conditions. The standard error is still valid because it is based on the design of the study and not the distribution of the outcomes.

::: {.tab} 
<button class="tablinks" onclick="unrolltab(event, 'ch5R7')">R code</button>
<button class="tablinks" onclick="unrolltab(event, 'ch5Stata7')">Stata code</button>
<button class="tablinks" onclick="unrolltab(event, 'ch5Hide7')">Hide</button>
::: {#ch5R7 .tabcontent} 
<br />
```{r, highlight = F, eval = F}
## Make some binary outcomes
dat1$u <- runif(ndat1)
dat1$v <- runif(ndat1)
dat1$y0bin <- ifelse(dat1$u>.5, 1, 0) # control potential outcome
dat1$y1bin <- ifelse((dat1$u+dat1$v) >.75, 1, 0) # treated potential outcomes
dat1$Ybin <- with(dat1, Z*y1bin + (1-Z)*y0bin)
truePropDiff <- mean(dat1$y1bin) - mean(dat1$y0bin)

## Estimate and view the difference in proportions
estAndSE2 <- difference_in_means(Ybin~Z,data=dat1)
estAndSE2
```
:::
::: {#ch5Stata7 .tabcontent} 
<br />
```{stata, highlight = F, eval = F}
** Make some binary outcomes
gen u = runiform()
gen v = runiform()
gen uv = u + v
gen y0bin = cond(u > 0.5, 1, 0) // control potential outcome
gen y1bin = cond(uv > 0.75, 1, 0) // treated potential outcome
gen ybin = (z * y1bin) + ((1 - z) * y0bin)
qui sum y1bin, meanonly
local y1mean = r(mean)
qui sum y0bin, meanonly
global truePropDiff = `y1mean' - r(mean)

** Estimate and view the difference in proportions
ttest ybin, by(z) unequal
```
::: 
::: {#ch5Hide7 .tabcontent}
::: 
:::

```{r makebinoutcomes, eval = T, echo = F}
## Make some binary outcomes
dat1$u <- runif(ndat1)
dat1$v <- runif(ndat1)
dat1$y0bin <- ifelse(dat1$u>.5, 1, 0) # control potential outcome
dat1$y1bin <- ifelse((dat1$u+dat1$v) >.75, 1, 0) # treated potential outcomes
dat1$Ybin <- with(dat1, Z*y1bin + (1-Z)*y0bin)
truePropDiff <- mean(dat1$y1bin) - mean(dat1$y0bin)

## Estimate and view the difference in proportions
estAndSE2 <- difference_in_means(Ybin~Z,data=dat1)
estAndSE2
```

When we have an experiment that includes a treatment and control group with binary outcomes, and when we are estimating the ATE, the standard error from a difference in proportions test is the same as the Neyman standard error (and therefore the HC2 error). In contrast, the standard error from a regular OLS regression with a binary outcome --- sometimes called a *linear probability model* --- will be at least slightly incorrect due to inherent heteroscecdasticity [@angrist2009mostly].

To see some logic for this, first consider that difference-in-proportion standard errors are estimated with the following equation:

$$\widehat{SE}_{prop} = \sqrt{\frac{p_{1}(1-p_{1})}{n_{1}}+\frac{p_{2}(1-p_{2})}{n_{2}}}$$

$n_1$ is the size of the group assigned treatment, $n_2$ is the size of the group assigned control, $p_1$ is the proportion of "successes" in the group assigned treatment, and $p_2$iss the proportion of "successes" in the group assigned control. Notice that the numerator in the fractions above represents the variance of the proportion in each treatment group.

Compare this with the Neyman standard error equation [@lin_agnostic_2013]:^[Specifically, the equation for the feasible SE discussed in Chapter 2]

$$\widehat{SE}_{Neyman} = \sqrt{\frac{\var(Y_{t})}{n_1}+\frac{\var(Y_{c})}{n_2}}$$

$Y_c$ is the vector of observed outcomes under control, and $Y_t$ is the vector of observed outcomes under treatment. This equation indicates that we use the observed variances in each treatment group to estimate the Neyman standard error for a difference in means.

The code below compares the various standard error estimators discussed here.

::: {.tab} 
<button class="tablinks" onclick="unrolltab(event, 'ch5R8')">R code</button>
<button class="tablinks" onclick="unrolltab(event, 'ch5Stata8')">Stata code</button>
<button class="tablinks" onclick="unrolltab(event, 'ch5Hide8')">Hide</button>
::: {#ch5R8 .tabcontent} 
<br />
```{r, highlight = F, eval = F}
nt <- sum(dat1$Z)
nc <- sum(1-dat1$Z)

## Find SE for difference of proportions.
p1 <- mean(dat1$Ybin[dat1$Z==1])
p0 <- mean(dat1$Ybin[dat1$Z==0])
se1 <- (p1*(1-p1))/nt
se0 <- (p0*(1-p0))/nc
se_prop <- round(sqrt(se1 + se0), 4)

## Find Neyman SE
varc_s <- var(dat1$Ybin[dat1$Z == 0])
vart_s <- var(dat1$Ybin[dat1$Z == 1])
se_neyman <- round(sqrt((vart_s/nt) + (varc_s/nc)), 4)

## Find OLS SE
simpOLS <- lm(Ybin~Z,dat1)
se_ols <- round(coef(summary(simpOLS))["Z", "Std. Error"], 2)

## Find Neyman SE (which are the HC2 SEs)
se_neyman2 <- coeftest(simpOLS,vcov = vcovHC(simpOLS,type="HC2"))[2,2]
se_neyman3 <- estAndSE2$std.error

## Show SEs
se_compare <- as.data.frame(cbind(se_prop, se_neyman, se_neyman2, se_neyman3, se_ols))
rownames(se_compare) <- "SE(ATE)"
colnames(se_compare) <- c("diff in prop", "neyman1","neyman2","neyman3", "ols")
print(se_compare)
```
:::
::: {#ch5Stata8 .tabcontent} 
<br />
```{stata, highlight = F, eval = F}
qui sum z
global nt = r(sum)
tempvar oneminus
gen `oneminus' = 1 - z
qui sum `oneminus'
global nc = r(sum)

** Find SE for difference of proportions.
qui ttest ybin, by(z)
local p1 = r(mu_2)
local p0 = r(mu_2)
local se1 = (`p1' * (1 - `p1'))/$nt
local se0 = (`p0' * (1 - `p0'))/$nc
global se_prop = round(sqrt(`se1' + `se0'), 0.0001)
local se_list $se_prop // Initialize a running list; used in the matrix below

** Find Neyman SE
qui sum ybin if z == 0
local varc_s = r(sd) * r(sd)
qui sum ybin if z == 1
local vart_s = r(sd) * r(sd)
global se_neyman = round(sqrt((`vart_s'/$nt) + (`varc_s'/$nc)), 0.0001)
local se_list `se_list' $se_neyman

** Find OLS SE
qui reg ybin z
global se_ols = round(_se[z], 0.0001) // See also: r(table) (return list) or e(V) (ereturn list)
local se_list `se_list' $se_ols

** Find Neyman SE (which are the HC2 SEs)
qui reg ybin z, vce(hc2)
global se_neyman2 = round(_se[z], 0.0001) 
qui ttest ybin, by(z) unequal // See: return list
global se_neyman3 = round(r(se), 0.0001)
local se_list `se_list' $se_neyman2 $se_neyman3

** Show SEs
matrix se_compare = J(1, 5, .)
matrix colnames se_compare = "diff in prop" "neyman1" "ols" "neyman2" "neyman3"
local i = 0
foreach l of local se_list {
	local ++i
	matrix se_compare[1, `i'] = `l'
}
matrix list se_compare
```
::: 
::: {#ch5Hide8 .tabcontent}
::: 
:::

```{r, eval = T, echo = F}
nt <- sum(dat1$Z)
nc <- sum(1-dat1$Z)

## Find SE for difference of proportions.
p1 <- mean(dat1$Ybin[dat1$Z==1])
p0 <- mean(dat1$Ybin[dat1$Z==0])
se1 <- (p1*(1-p1))/nt
se0 <- (p0*(1-p0))/nc
se_prop <- round(sqrt(se1 + se0), 4)

## Find Neyman SE
varc_s <- var(dat1$Ybin[dat1$Z == 0])
vart_s <- var(dat1$Ybin[dat1$Z == 1])
se_neyman <- round(sqrt((vart_s/nt) + (varc_s/nc)), 4)

## Find OLS SE
simpOLS <- lm(Ybin~Z,dat1)
se_ols <- round(coef(summary(simpOLS))["Z", "Std. Error"], 2)

## Find Neyman SE (which are the HC2 SEs)
se_neyman2 <- coeftest(simpOLS,vcov = vcovHC(simpOLS,type="HC2"))[2,2]
se_neyman3 <- estAndSE2$std.error

## Show SEs
se_compare <- as.data.frame(cbind(se_prop, se_neyman, se_neyman2, se_neyman3, se_ols))
rownames(se_compare) <- "SE(ATE)"
colnames(se_compare) <- c("diff in prop", "neyman1","neyman2","neyman3", "ols")
print(se_compare)
```

##### Testing the sharp null of no effects

With a binary treatment and a binary outcome, we could test hypothesis that outcomes are totally independent of treatment assignment using what is called *Fisher's exact test*. We can also use the permutation-based approaches above to produce results that do not rely on asymptotic assumptions. Below we show how Fisher's exact test, the Exact Cochran-Mantel-Haenszel test, and the Exact $\chi$-squared test produce the same answers.

::: {.tab} 
<button class="tablinks" onclick="unrolltab(event, 'ch5R9')">R code</button>
<button class="tablinks" onclick="unrolltab(event, 'ch5Stata9')">Stata code</button>
<button class="tablinks" onclick="unrolltab(event, 'ch5Hide9')">Hide</button>
::: {#ch5R9 .tabcontent} 
<br />
```{r, highlight = F, eval = F}
test2fisher <- fisher.test(x=dat1$Z,y=dat1$Ybin)
print(test2fisher)
test2chisq <- chisq_test(factor(Ybin)~factor(Z),data=dat1,distribution=exact())
print(test2chisq)
test2cmh <- cmh_test(factor(Ybin)~factor(Z),data=dat1,distribution=exact())
print(test2cmh)
```
:::
::: {#ch5Stata9 .tabcontent} 
<br />
```{stata, highlight = F, eval = F}
tabulate z ybin, exact
tabulate z ybin, chi2
* search emh
emh z ybin
```
::: 
::: {#ch5Hide9 .tabcontent}
::: 
:::

```{r fisherexact, eval = T, echo = F, results = "hold", collapse = T}
test2fisher <- fisher.test(x=dat1$Z,y=dat1$Ybin)
print(test2fisher)
test2chisq <- chisq_test(factor(Ybin)~factor(Z),data=dat1,distribution=exact())
print(test2chisq)
test2cmh <- cmh_test(factor(Ybin)~factor(Z),data=dat1,distribution=exact())
print(test2cmh)
```

A difference-in-proportions test can also be performed directly (rather than relying on OLS to approximate this). In that case, the null hypothesis is tested while using a binomial distribution (rather than a Normal distribution) to approximate the underlying randomization distribution. In reasonably-sized samples, both approximations perform well.

::: {.tab} 
<button class="tablinks" onclick="unrolltab(event, 'ch5R10')">R code</button>
<button class="tablinks" onclick="unrolltab(event, 'ch5Stata10')">Stata code</button>
<button class="tablinks" onclick="unrolltab(event, 'ch5Hide10')">Hide</button>
::: {#ch5R10 .tabcontent} 
<br />
```{r, highlight = F, eval = F}
mat <- with(dat1,table(Z,Ybin))
matpt <- prop.test(mat[,2:1])
matpt
```
:::
::: {#ch5Stata10 .tabcontent} 
<br />
```{stata, highlight = F, eval = F}
prtest ybin, by(z)
```
::: 
::: {#ch5Hide10 .tabcontent}
::: 
:::

```{r diffprop, eval = T, echo = F}
mat <- with(dat1,table(Z,Ybin))
matpt<-prop.test(mat[,2:1])
matpt
```

### Multiple arms

Multiple treatment arms can be analyzed as above, except that we now have
more than one comparison between a treated group and a control group. Such studies raise both substantive and statistical questions about multiple
testing (or "multiple comparisons"). For example, the `difference_in_means`
function asks which average treatment effect it should estimate, and it
only presents one comparison at a time. We *could* compare the treatment `T2`
with the baseline outcome of `T1`. But we *could also* compare both of `T2` and `T3` with `T1` at the same time, as in the second set of results (`lm_robust` implements the same standard errors as `difference_in_means`, but allows for more flexible model specification).

::: {.tab} 
<button class="tablinks" onclick="unrolltab(event, 'ch5R11')">R code</button>
<button class="tablinks" onclick="unrolltab(event, 'ch5Stata11')">Stata code</button>
<button class="tablinks" onclick="unrolltab(event, 'ch5Hide11')">Hide</button>
::: {#ch5R11 .tabcontent} 
<br />
```{r, highlight = F, eval = F}
## Comparing only conditions 1 and 2
estAndSE3 <- difference_in_means(Y~Z4arms,data=dat1,condition1="T1",condition2="T2")
print(estAndSE3)

## Compare each other arm to T1
estAndSE3multarms <- lm_robust(Y~Z4arms,data=dat1)
print(estAndSE3multarms)
```
:::
::: {#ch5Stata11 .tabcontent} 
<br />
```{stata, highlight = F, eval = F}
** Comparing only conditions 1 and 2
ttest y if inlist(z4arms, "T1", "T2"), by(z4arms) unequal

** Compare each other arm to T1
encode z4arms, gen(z4num)
reg y ib1.z4num, vce(hc2) // Set 1 as the reference category
```
::: 
::: {#ch5Hide11 .tabcontent}
::: 
:::

```{r, eval = T, echo = F, results = "hold", collapse = T}
estAndSE3 <- difference_in_means(Y~Z4arms,data=dat1,condition1="T1",condition2="T2")
print(estAndSE3)
estAndSE3multarms <- lm_robust(Y~Z4arms,data=dat1)
print(estAndSE3multarms)
```

In this case, we could make $((4 \times 4)-4)/2)=6$ different possible comparisons between pairs of treatment groups. Consider that if there were really no effects of any treatment, and if we chose to reject the null at the standard significance threshold of $\alpha=.05$, we would actually claim that there was at least one effect *more than 5% of the time*. $1 - ( 1 - .05)^6 = .27$, or 27% of the time, we would make a false positive error, claiming an effect existed when it did not.

In general, our analyses of studies with multiple arms should reflect the fact that we are making multiple comparisons. Two points are worth emphasizing here. First, the family-wise error rate (FWER) of these tests will differ from the individual error rate of single test. In short, testing more than one hypothesis increases the chance of making at least one Type I error (i.e., incorrectly rejecting a true null hypothesis). Suppose instead of testing a single hypothesis at a conventional significance level of $\alpha = 0.05$ we tested two hypothesis at $\alpha = 0.05$. The probability of retaining both hypotheses is $(1-\alpha)^2 = .9025$  and the probability of rejecting at least one of these hypotheses is $1-(1-\alpha)^2 = .0975$ --- almost double our stated significance threshold of $\alpha = 0.05$.

Second, multiple tests will often be correlated, and corrections for multiple testing should recognize these relationships --- importantly, accounting for this correlation *will penalize multiple testing less*! When we say that tests are "correlated," we mean that there is some relationship between the test statistics (e.g., a student's t-statistic, or a $\chi^{2}$ statistic) used to perform statistical inference in each case. In other words, it means that the test statistics are jointly distributed --- when one test statistic is higher, the other will tend to be higher as well.^[Imagine testing two hypotheses with $\alpha = .05$, but the assumed reference distributions for the test statistics were identical: by accident, ran the same exact code twice. In that case, we are really just doing one test and so we haven't changed our probability of rejecting a true null hypothesis for either test. If the two tests were instead correlated at .99, we would have changed this probability but only very slightly, since both tests would basically still be the same.]

That issue in mind, our default recommendations for multi-arm trials are as follows:

 - First, decide on a focal, **confirmatory** comparison for the entire evaluation: say, control/status quo *versus* receiving any version of the treatment. Such a test would likely have more statistical power than a test that evaluates each arm separately, and would also have a correctly controlled false positive rate. This would then serve as the primary confirmatory finding we report.

 - Next, we perform the rest of the comparisons as **exploratory** analyses without multiple testing adjustment --- i.e., as analyses that may inform future projects and give hints about where we might be seeing more or less of an effect, but which cannot serve as a foundation for overall conclusions on their own.
 
 - Alternatively, we might perform confirmatory comparisons that adjust for the collective false positive rate.^[We generally seek to control the FWER, but sometimes consider controlling the false discovery rate, or FDR, instead. Holding the FDR at 20%, for example, implies trying to ensure that only 20% of reported test results (or 1 in 5) are a false positive. This contrasts with adjustments to control the FWER, which focus on the probability of observing one or more false positives across tests.] Other options we consider are using the Tukey HSD procedure for pairwise comparisons, or testing the hypotheses in a particular order to preserve statistical power [@rosenbaum2008a].

#### Adjusting p-values and confidence intervals for multiple comparisons

Here is an illustration of different methods of adjusting for multiple comparisons in R.

To reflect that fact that we are making multiple comparisons, we could adjust
$p$-values from (uncorrelated) tests to control the familywise error rate at $\alpha$ through either a single step procedure (e.g. [Bonferroni correction](https://en.wikipedia.org/wiki/Bonferroni_correction)) or a stepwise stepwise procedure (such as the [Holm correction](https://en.wikipedia.org/wiki/Holm%E2%80%93Bonferroni_method)). We might also control the false discovery rate (e.g., using the [Benjamini-Hochberg correction](https://en.wikipedia.org/wiki/False_discovery_rate#Benjamini.E2.80.93Hochberg_procedure)).

Our standard practice is to adjust the FWER for uncorrelated tests using Holm adjustment. For more on such adjustments and multiple comparisons see EGAP's [10 Things you need to know about multiple comparisons](https://egap.org/resource/10-things-to-know-about-multiple-comparisons/).

<!-- Explain
here about what constitutes a family and how we choose. Also why Holm. And
when FDR.-->

::: {.tab} 
<button class="tablinks" onclick="unrolltab(event, 'ch5R12')">R code</button>
<button class="tablinks" onclick="unrolltab(event, 'ch5Stata12')">Stata code</button>
<button class="tablinks" onclick="unrolltab(event, 'ch5Hide12')">Hide</button>
::: {#ch5R12 .tabcontent} 
<br />
```{r, highlight = F, eval = F}
## Get p-values but exclude intercept
pvals <- summary(estAndSE3multarms)$coef[2:4,4]

## Illustrate different corrections (or lack thereof)

# None
round(p.adjust(pvals, "none"), 3)

# Bonferroni
round(p.adjust(pvals, "bonferroni"), 3)

# Holm
round(p.adjust(pvals, "holm"), 3)

# Hochberg
round(p.adjust(pvals, "hochberg"), 3) # FDR instead of FWER
```
:::
::: {#ch5Stata12 .tabcontent} 
<br />
```{stata, highlight = F, eval = F}
** Get p-values but exclude intercept
* See also: search parmest
matrix pvals = r(table)["pvalue", 2..4] // save in a matrix
matrix pvalst = pvals' // transpose
svmat pvalst, names(col) // add matrix as data in memory

** Illustrate different corrections (or lack thereof)

* None
replace pvalue = round(pvalue, 0.0001)
list pvalue if !missing(pvalue)

* Bonferroni
* ssc install qqvalue
qqvalue pvalue if !missing(pvalue), method(bonferroni) qvalue(adj_p_bonf)
replace adj_p_bonf = round(adj_p_bonf, 0.0001)
list adj_p_bonf if !missing(pvalue)

* Holm
qqvalue pvalue if !missing(pvalue), method(holm) qvalue(adj_p_holm)
replace adj_p_holm = round(adj_p_holm, 0.0001)
list adj_p_holm if !missing(pvalue)

* Hochberg
qqvalue pvalue if !missing(pvalue), method(hochberg) qvalue(adj_p_hoch)
replace adj_p_hoch = round(adj_p_hoch, 0.0001)
list adj_p_hoch if !missing(pvalue) // FDR instead of FWER
```
::: 
::: {#ch5Hide12 .tabcontent}
::: 
:::

```{r, eval = T, echo = F, results = "hold", collapse = T}
## Get p-values but exclude intercept
pvals <- summary(estAndSE3multarms)$coef[2:4,4]

## Illustrate different corrections (or lack thereof)

# None
print("None")
round(p.adjust(pvals, "none"), 3)

# Bonferroni
print("Bonferroni")
round(p.adjust(pvals, "bonferroni"), 3)

# Holm
print("Holm")
round(p.adjust(pvals, "holm"), 3)

# Hochberg
print("Hochberg")
round(p.adjust(pvals, "hochberg"), 3) # FDR instead of FWER
```

Simply adjusting $p$-values from this linear model, however, ignores the fact that we may be interested in other pairwise comparisons, such as the difference in effects between receiving `T3` vs `T4`. It also ignores potential correlations in the distribution of test statistics (i.e., we are leaving statistical power "on the table").

Instead of the above, and instead of employing simulation to control the FWER (step 7 on [this page](https://egap.org/resource/10-things-to-know-about-multiple-comparisons/)), we can also implement a [Tukey Honestly Signficant Differences (HSD) test](https://en.wikipedia.org/wiki/Tukey%27s_range_test). The Tukey HSD test (sometimes called a Tukey range test or just a
Tukey test) calculates multiple-comparison-adjusted $p$-values and simultaneous confidence intervals *for all pairwise comparisons* in a model, while taking into account possible correlations between test statistics. It is similar to a two-sample t-test, but with built in adjustment for multiple comparisons. The test statistic for any comparison between two equally-sized groups $i$ and $j$ is:

$$ t_{ij} = \frac{\bar{y_i}-\bar{y_j}}{s\sqrt{\frac{2}{n}}} $$

$\bar{y_i}$ and $\bar{y_j}$ are the means in groups $i$ and
$j$, respectively. $s$ is the pooled standard deviation of the outcome, and $n$ is the common sample size. A critical value is then chosen for $t_{ij}$ given the desired significance level, $\alpha$, the number of groups being compared, $k$, and the degrees of freedom, $n-k$. We'll represent this critical value with $t_{\alpha,k,n}$.

The confidence interval for any difference between equally-sized groups is then:^[We focus on the classic Tukey HSD procedure for this illustration, which assumes equally sized groups. There are modifications to apply this procedure in other settings, such as the Tukey-Kramer test.]

$$\left[\bar{y_i}-\bar{y_j}-t_{\alpha,k,n}s\sqrt{\frac{2}{n}}\hspace{2mm},\hspace{2mm} \bar{y_i}-\bar{y_j}+t_{\alpha,k,n}s\sqrt{\frac{2}{n}}\right]$$

We present an R implementation of the Tukey HSD test using the `glht()` function from the `multcomp` package, which offers more flexiblity than the
`TukeyHSD` in the base `stats` package (at the price of a slightly more complicated syntax). We also illustrate a few options in Stata. But to perform this post-hoc test, we first need to fit an applicable model.

::: {.tab} 
<button class="tablinks" onclick="unrolltab(event, 'ch5R13')">R code</button>
<button class="tablinks" onclick="unrolltab(event, 'ch5Stata13')">Stata code</button>
<button class="tablinks" onclick="unrolltab(event, 'ch5Hide13')">Hide</button>
::: {#ch5R13 .tabcontent} 
<br />
```{r, highlight = F, eval = F}
## We can use aov() or lm()
dat1$Z4factor <- as.factor(dat1$Z4arms)
aovmod <- aov(Y~Z4factor, dat1)
##lmmod <- lm(Y~Z4arms, dat1)
```
:::
::: {#ch5Stata13 .tabcontent} 
<br />
```{stata, highlight = F, eval = F}
anova y z4num
```
::: 
::: {#ch5Hide13 .tabcontent}
::: 
:::

```{r, eval = T, echo = F}
## We can use aov() or lm()
dat1$Z4factor <- as.factor(dat1$Z4arms)
aovmod <- aov(Y~Z4factor, dat1)
##lmmod <- lm(Y~Z4arms, dat1)
```

In R, using the `glht()` function's `linfcnt` argument, we tell the function to conduct a Tukey test of all pairwise comparisons for our treatment indicator, $Z$. In Stata, we can do this using the `tukeyhsd` command or `pwcompare` commands.

::: {.tab} 
<button class="tablinks" onclick="unrolltab(event, 'ch5R14')">R code</button>
<button class="tablinks" onclick="unrolltab(event, 'ch5Stata14')">Stata code</button>
<button class="tablinks" onclick="unrolltab(event, 'ch5Hide14')">Hide</button>
::: {#ch5R14 .tabcontent} 
<br />
```{r, highlight = F, eval = F}
tukey_mc <- glht(aovmod, linfct = mcp(Z4factor = "Tukey"))
summary(tukey_mc)
```
:::
::: {#ch5Stata14 .tabcontent} 
<br />
```{stata, highlight = F, eval = F}
* search tukeyhsd
* search qsturng
tukeyhsd z4num
* Or: pwcompare z4arms, mcompare(tukey) effects
```
::: 
::: {#ch5Hide14 .tabcontent}
::: 
:::

```{r, eval = T, echo = F}
tukey_mc <- glht(aovmod, linfct = mcp(Z4factor = "Tukey"))
summary(tukey_mc)
```

Focusing on the R results, we can then plot the 95% family wise confidence intervals for these comparisons.

```{r tukeyplot}
## Save dfault ploting parameters
op <- par()
## Add space to left-hand outer margin
par(oma = c(1, 3, 0, 0))
plot(tukey_mc)
```

We can also obtain simultaneous confidence intervals at other levels of statistical significance using the `confint()` function.

```{r plot_tukey_ci, warnings=FALSE}
## Generate and plot 90% confidence intervals
tukey_mc_90ci <- confint(tukey_mc, level = .90)
plot(tukey_mc_90ci)
## Restore plotting defaults, now
par(op)
```

See also, in R: `pairwise.prop.test` for binary outcomes.

### Multiple Outcomes

Our studies often involve more than one outcome measure. Assessing the effect of even a simple two-arm treatment on 10 different outcomes raises the same kinds of questions that come up in the context of multi-arm trials, generally requiring applications of the methods discussed above.

<!-- Insert examples here? Or not?-->

## Covariance adjustment (the use of background information to increase precision)

When we have background or baseline information about experimental units, we
can use this to increase the precision with which we estimate our treatment
effects (i.e., increase the statistical power of our tests). We prefer to use this information during the design phase to create block randomized designs. But we may only have access to such background information after the study has been fielded, and so we will pre-specify use of this information to increase our statistical power.

We sometimes avoid the practice of adjusting for covariates (or fixed effect dummies) in a linear and additive fashion. This estimator of the average treatment effect can be subject to small-sample bias and may be --- counterintuitively --- *less* efficient [@freedman2008rae].^[See our discussion below for more context.] In contrast, an approach to covariate adjustment that we call the "Lin estimator" performs better [@lin_agnostic_2013]. To be clear, the bias/precision-loss attributable to linear covariance adjustment estimator may often be quite small, especially when sample sizes are large. Yet, because it is frequently costless to use the Lin estimator in sufficiently large samples, this is our default recommendation (see [this page](https://declaredesign.org/blog/2018-09-11-controlling-pretreatment-covariates.html) as well). That said, a number of our projects encounter situations where linear, additive covariate adjustment is preferred.^[For instance, as the number of parameters to be estimated becomes large relative to a study's sample size, we may judge that the loss of degrees of freedom due to Lin adjustment in a particular case is not worth it's potential benefits.]

### Possible bias in the least squares ATE estimator with covariates

When we estimate the average treatment effect using least squares we tend to say that we "regress" some outcome for each unit $i$, $Y_i$, on (often binary) treatment assignment, $Z_i$, where $Z_i=1$ if a unit is assigned to treatment and 0 if assigned to control. And we write a linear model relating $Z$ and $Y$ as below, where $\beta_1$ represents the difference in means of $Y$ between units with $Z=1$ and $Z=0$:

$$
\begin{equation}
Y_i = \beta_0 + \beta_1 Z_i + e_i
\end{equation}
$$

This is a common practice because we know that the formula to estimate $\beta_1$ in Equation (1) is the same as the difference-in-means when comparing $Y$ across the treatment and control groups:

$$
\begin{equation}
\hat{\beta}_1 = \overline{Y}_{Z=1} - \overline{Y}_{Z=0} = \frac{\cov(Y,Z)}{\var(Z)}.
\end{equation}
$$
This last term, expressed with covariances and variances, is the expression for the slope coefficient in a bivariate OLS regression model. This estimator of the average treatment effect has no systematic error (i.e., it is unbiased), so we can write $E_R(\hat{\beta}_1)=\beta_1 \equiv \text{ATE}$, where $E_R(\hat{\beta}_1)$ refers to the expectation of $\hat{\beta}_{1}$ across randomizations consistent with the experimental design.

Sometimes we have an additional (pre-treatment) covariate, $X_i$, commonly included in the analysis as follows:

$$
\begin{equation}
Y_i = \beta_0 + \beta_1 Z_i + \beta_2 X_i + e_i
\end{equation}
$$

What is $\beta_1$ in this case? The matrix representation here is: $(\beta X^{T}\beta X)^{-1}\beta X^{T}\beta y$. But it will be more useful to examine the scalar formula:

$$ \hat{\beta}_1 = \frac{\var(X)\cov(Z,Y) - \cov(X,Z)\cov(X,Y)}{\var(Z)\var(X) - \cov(Z,X)^2} $$

In very large experiments $\cov(X,Z) \approx 0$ because $Z$ is randomly
assigned and is thus independent of background variables like $X$. However in any given finite sized experiment $\cov(X,Z) \ne 0$, so this does not reduce to an unbiased estimator as it does in the bivariate case. Thus, @freedman2008rae showed that there is a small amount of bias in using the equation above to estimate the average treatment effect.

To address this small-sample bias, and to address the asymptotic inefficiency issue with linear covariance adjustment noted by @freedman2008rae, @lin_agnostic_2013 suggests the following least squares approach --- regressing the outcome on binary treatment assignment $Z_i$ and its interaction with mean-centered covariates:^[@lin_agnostic_2013 notes that the asymptotic inefficiency problem appears when a 2-arm design is sufficiently imbalanced (many more observations in one arm) or when a covariate has a stronger relationship with a unit's treatment effect than with it's expected outcome. This inefficiency may also appear in a 3+ arm design, regardless of balance.]

$$
\begin{equation}
Y_i = \beta_0 + \beta_1 Z_i + \beta_2 ( X_i - \bar{X} ) + \beta_3 Z_i (X_i - \bar{X}) +  e_i
\end{equation}
$$

When implementing this covariance adjustment strategy, remember that every covariate, *including binary indicators used to estimate fixed effects or to control for categorical covariates*, should be mean-centered and interacted with treatment.

For instance, imagine a design with two treatment arms (treatment vs control) and three covariates. Linear adjustment for these covariates would involve fitting an OLS regression with *four* slope coefficients (one for the treatment group, and one for each covariate). @lin_agnostic_2013 adjustment for these covariates would instead involve fitting a regression with *seven* slope coefficients (one for the treatment group, one for each mean-centered covariate, and one for each interaction of treatment with a mean-centered covariate).^[The covariates outside the treatment interactions do not actually need to be mean-centered, but the version of each covariate in the interaction term does. E.g., in Equation 4, $\beta_2 (X_i - \bar{X})$ could be replaced by $\beta_2 X_i$, but $Z_i (X_i - \bar{X})$ must be kept as-is. It is often easier to simply only use mean-centered covariates in Lin adjustment here to avoid confusion.]

See the [Green-Lin-Coppock SOP](https://alexandercoppock.com/Green-Lab-SOP/Green_Lab_SOP.html#using-covariates-in-analysis) for more examples of this approach to covariance adjustment.

### Illustrating the Lin Approach to Covariance Adjustment

Here, we show how covariance adjustment can create bias in estimation of the average treatment effect --- and how to reduce this bias while using the Lin procedure as well as by increasing the size of the experiment. In this case, we compare an experiment with 20 units to an experiement with 100 units, in each case with half of the units assigned to treatment by complete random assignment.

We'll use the [DeclareDesign](https://declaredesign.org/) package in the R code to make this process of writing a simulation to assess bias easier.^[For more, see also [this accompanying book](https://book.declaredesign.org/introduction/preface.html).] Much of the R code that follows is providing instructions to the `diagnose_design` command, which repeats the design of the experiment many times, each time estimating an average treatment effect, and comparing the mean of those estimate to the truth (labeled "Mean Estimand" below). We also provide code illustrating how you could accomplish something similar in Stata using manually defined programs and the `simulate` command.

The true potential outcomes in these example data (`y1` and `y0`) were generated using one covariate, called `cov2`, with no treatment effect. In what follows, we compare the performance of (1) the simple estimator using OLS to (2) estimators that use Lin's procedure involving just the correct covariate, and also to (3) estimators that use incorrect covariates (since we rarely know exactly the covariates that help generate any given behavioral outcome).

We'll break this code up into sections to help with legibility. First, in R, we prepare `design` objects (a class used by `DeclareDesign`) for the $n=20$ and $n=100$ designs. Meanwhile, in Stata, we write a program to randomly generate a sample dataset; this program will then be iterated below. In both the R and Stata simulations, we follow a design-based philosophy in which randomness in our estimates across simulations comes only from variation in treatment assignment.

::: {.tab} 
<button class="tablinks" onclick="unrolltab(event, 'ch5R15')">R code</button>
<button class="tablinks" onclick="unrolltab(event, 'ch5Stata15')">Stata code</button>
<button class="tablinks" onclick="unrolltab(event, 'ch5Hide15')">Hide</button>
::: {#ch5R15 .tabcontent} 
<br />
```{r, highlight = F, eval = F}
## Keep a dataframe of select variables
wrkdat1 <- dat1 %>% dplyr::select(id,y1,y0,contains("cov"))

## Declare this as our larger population
## (an experimental sample of 100 units)
popbigdat1 <- declare_population(wrkdat1)

## A dataset to represent a smaller experiment,
## or a cluster randomized experiment with few clusters
## (an experimental sample of 20 units)
set.seed(12345)
smalldat1 <- dat1 %>% dplyr::select(id,y1,y0,contains("cov")) %>% sample_n(20)

## Now declare the different inputs for DeclareDesign
## (declare the smaller population, and assign treatment in each)
popsmalldat1 <- declare_population(smalldat1)
assignsmalldat1 <- declare_assignment(Znew=complete_ra(N,prob=0.5))
assignbigdat1 <- declare_assignment(Znew=complete_ra(N,prob=0.5))

## No additional treatment effects
## (potential outcomes)
po_functionNull <- function(data){
	data$Y_Znew_0 <- data$y0
	data$Y_Znew_1 <- data$y1
	data
}

## A few additional declare design settings
ysdat1  <- declare_potential_outcomes(handler = po_functionNull)
theestimanddat1 <- declare_inquiry(ATE = mean(Y_Znew_1 - Y_Znew_0))
theobsidentdat1 <- declare_reveal(Y, Znew)

## The smaller sample design
thedesignsmalldat1 <- popsmalldat1 + assignsmalldat1 + ysdat1 + theestimanddat1 + theobsidentdat1

## The larger sample design
thedesignbigdat1 <- popbigdat1 + assignbigdat1 + ysdat1 + theestimanddat1 + theobsidentdat1
```
:::
::: {#ch5Stata15 .tabcontent} 
<br />
```{stata, highlight = F, eval = F}
** Preserve the running dataset used so far.
save main.dta, replace

** Keep a dataframe of select variables.
keep id y1 y0 cov*

** A dataset to represent a smaller experiment,
** or a cluster randomized experiment with few clusters
** (an experimental sample of 20 units).
** Only done for illustration here. In practice, we'll use the data
** generated in the R code for the sake of comparison.
set seed 12345
gen rand = runiform()
sort rand
keep in 1/20
drop rand

** Define a program to load one of these datasets
** and then randomly re-assign treatment.
capture program drop sample_from
program define sample_from, rclass

	syntax[ , smaller ///
		propsimtreat(real 0.5) ]
	
	* Which dataset to use?
	if "`smaller'" != "" import delimited using "smalldat1.csv", clear
	else import delimited using "popbigdat1.csv", clear
	
	* Make sure propsimtreat is a proportion
	if `propsimtreat' > 1 | `propsimtreat' < 0 {
		di as error "Check input: propsimtreat"
		exit
	}
	
	* Re-assign (simulated) treatment in this draw of the data
	complete_ra znew, prob(`propsimtreat')
	
	* No additional treatment effects
	* (assigning new potential outcomes)
	gen y_znew_0 = y0
	gen y_znew_1 = y1
	
	* Save the true ATE
	gen true_effect = y_znew_1 - y_znew_0
	qui sum true_effect, meanonly
	return scalar ATE = r(mean)
	
	* Get the revealed outcome
	gen ynew = (znew * y_znew_1) + ((1 - znew) * y_znew_0)

end
```
::: 
::: {#ch5Hide15 .tabcontent}
::: 
:::

```{r lianestimatorexample, eval = T, echo = F, results="hide"}
## Keep a dataframe of select variables
wrkdat1 <- dat1 %>% dplyr::select(id,y1,y0,contains("cov"))

## Declare this as our larger population
## (an experimental sample of 100 units)
popbigdat1 <- declare_population(wrkdat1)

## A dataset to represent a smaller experiment,
## or a cluster randomized experiment with few clusters
## (an experimental sample of 20 units)
set.seed(12345)
smalldat1 <- dat1 %>% dplyr::select(id,y1,y0,contains("cov")) %>% sample_n(20)

## Now declare the different inputs for DeclareDesign
## (declare the smaller population, and assign treatment in each)
popsmalldat1 <- declare_population(smalldat1)
assignsmalldat1 <- declare_assignment(Znew=complete_ra(N,prob=0.5))
assignbigdat1 <- declare_assignment(Znew=complete_ra(N,prob=0.5))

## No additional treatment effects
## (potential outcomes)
po_functionNull <- function(data){
	data$Y_Znew_0 <- data$y0
	data$Y_Znew_1 <- data$y1
	data
}

## A few additional declare design settings
ysdat1  <- declare_potential_outcomes(handler = po_functionNull)
theestimanddat1 <- declare_inquiry(ATE = mean(Y_Znew_1 - Y_Znew_0))
theobsidentdat1 <- declare_reveal(Y, Znew)

## The smaller sample design
thedesignsmalldat1 <- popsmalldat1 + assignsmalldat1 + ysdat1 + theestimanddat1 + theobsidentdat1

## The larger sample design
thedesignbigdat1 <- popbigdat1 + assignbigdat1 + ysdat1 + theestimanddat1 + theobsidentdat1
```

```{r, eval = F, echo = F, include = F}
# To use in the stata code
write.csv(wrkdat1, "popbigdat1.csv", row.names = F)
write.csv(smalldat1, "smalldat1.csv", row.names = F)
```

Next, in R, we'll prepare a list of estimators (i.e., models) we want to compare. These include different numbers of (potentially incorrect) covariates, with and without Lin (2013) adjustment. Similarly, in Stata, we'll write another program that generates a single dataset and then applies the same list of estimators to it, saving key results from each.

::: {.tab} 
<button class="tablinks" onclick="unrolltab(event, 'ch5R16')">R code</button>
<button class="tablinks" onclick="unrolltab(event, 'ch5Stata16')">Stata code</button>
<button class="tablinks" onclick="unrolltab(event, 'ch5Hide16')">Hide</button>
::: {#ch5R16 .tabcontent} 
<br />
```{r, highlight = F, eval = F}
## Declare a selection of different estimation strategies
estCov0 <- declare_estimator(Y~Znew, inquiry=theestimanddat1, .method=lm_robust, label="CovAdj0: Lm, No Covariates")
estCov1 <- declare_estimator(Y~Znew+cov2, inquiry=theestimanddat1, .method=lm_robust, label="CovAdj1: Lm, Correct Covariate")
estCov2 <- declare_estimator(Y~Znew+cov1+cov2+cov3+cov4+cov5+cov6+cov7+cov8, inquiry=theestimanddat1, .method=lm_robust, label="CovAdj2: Lm, Mixed Covariates")
estCov3 <- declare_estimator(Y~Znew+cov1+cov3+cov4+cov5+cov6, inquiry=theestimanddat1, .method=lm_robust, label="CovAdj3: Lm, Wrong Covariates")
estCov4 <- declare_estimator(Y~Znew,covariates=~cov1+cov2+cov3+cov4+cov5+cov6+cov7+cov8, inquiry=theestimanddat1, .method=lm_lin, label="CovAdj4: Lin, Mixed Covariates")
estCov5 <- declare_estimator(Y~Znew,covariates=~cov2, inquiry=theestimanddat1, .method=lm_lin, label="CovAdj5: Lin, Correct Covariate")

## List them all together
all_estimators <- estCov0 + estCov1 + estCov2 + estCov3 + estCov4 + estCov5
```
:::
::: {#ch5Stata16 .tabcontent} 
<br />
```{stata, highlight = F, eval = F}
** Define a program to apply various estimation strategies
** to a dataset drawn using the program defined above.
capture program drop apply_estimators
program define apply_estimators, rclass

	** Same arguments as above
	syntax[, smaller ///
		propsimtreat(real 0.5) ]
		
	** Call the program above
	sample_from, `smaller' propsimtreat(`propsimtreat')
	return scalar ATE = r(ATE)

	** CovAdj0: Lm, No Covariates
	qui reg ynew znew, vce(hc2)
	return scalar CovAdj0_est = _b[znew]
	return scalar CovAdj0_p = r(table)["pvalue", "znew"]
	
	** CovAdj1: Lm, Correct Covariate
	qui reg ynew znew cov2, vce(hc2)
	return scalar CovAdj1_est = _b[znew]
	return scalar CovAdj1_p = r(table)["pvalue", "znew"]
	
	** CovAdj2: Lm, Mixed Covariates
	qui reg ynew znew cov1-cov8, vce(hc2)
	return scalar CovAdj2_est = _b[znew]
	return scalar CovAdj2_p = r(table)["pvalue", "znew"]
	
	** CovAdj3: Lm, Wrong Covariates
	qui reg ynew znew cov1 cov3-cov6, vce(hc2)
	return scalar CovAdj3_est = _b[znew]
	return scalar CovAdj3_p = r(table)["pvalue", "znew"]
	
	** CovAdj4: Lin, Mixed Covariates
	qui reg ynew znew cov1-cov8 // to make a sample indicator
	gen samp = e(sample) // ensure correct obs are used in mean-centering
	foreach var of varlist cov1-cov8 {
		qui sum `var' if samp == 1, meanonly
		qui gen mc_`var' = `var' - `r(mean)' if samp == 1
	}
	qui reg ynew i.znew##c.(mc_*), vce(hc2)
	return scalar CovAdj4_est = _b[1.znew]
	return scalar CovAdj4_p = r(table)["pvalue", "1.znew"]
	drop mc_* samp
	
	** CovAdj5: Lin, Correct Covariate
	qui reg ynew znew cov2
	gen samp = e(sample)
	foreach var of varlist cov2 {
		qui sum `var' if samp == 1, meanonly
		qui gen mc_`var' = `var' - `r(mean)' if samp == 1
	}
	qui reg ynew i.znew##c.(mc_*), vce(hc2)
	return scalar CovAdj5_est = _b[1.znew]
	return scalar CovAdj5_p = r(table)["pvalue", "1.znew"]
	drop mc_* samp

end
```
::: 
::: {#ch5Hide16 .tabcontent}
::: 
:::

```{r covadjestimators, eval = T, echo = F}
## Declare a selection of different estimation strategies
estCov0 <- declare_estimator(Y~Znew, inquiry=theestimanddat1, .method=lm_robust, label="CovAdj0: Lm, No Covariates")
estCov1 <- declare_estimator(Y~Znew+cov2, inquiry=theestimanddat1, .method=lm_robust, label="CovAdj1: Lm, Correct Covariate")
estCov2 <- declare_estimator(Y~Znew+cov1+cov2+cov3+cov4+cov5+cov6+cov7+cov8, inquiry=theestimanddat1, .method=lm_robust, label="CovAdj2: Lm, Mixed Covariates")
estCov3 <- declare_estimator(Y~Znew+cov1+cov3+cov4+cov5+cov6, inquiry=theestimanddat1, .method=lm_robust, label="CovAdj3: Lm, Wrong Covariates")
estCov4 <- declare_estimator(Y~Znew,covariates=~cov1+cov2+cov3+cov4+cov5+cov6+cov7+cov8, inquiry=theestimanddat1, .method=lm_lin, label="CovAdj4: Lin, Mixed Covariates")
estCov5 <- declare_estimator(Y~Znew,covariates=~cov2, inquiry=theestimanddat1, .method=lm_lin, label="CovAdj5: Lin, Correct Covariate")

## List them all together
all_estimators <- estCov0 + estCov1 + estCov2 + estCov3 + estCov4 + estCov5
```

```{r testestimators, eval = F, echo=FALSE, results = FALSE}
## Testing, hidden from book
blah <- draw_data(thedesignsmalldat1)
blahBig <- draw_data(thedesignbigdat1)
estCov0(blah)
estCov1(blah)
estCov2(blah)
estCov3(blah)
estCov4(blah)
estCov5(blah)
```

After this, as a last step in R, we'll add those estimators to our `design` class objects.

```{r setup_design_plus_est_objects}
## Smaller sample
thedesignsmalldat1PlusEstimators <- thedesignsmalldat1 + all_estimators

## Larger sample
thedesignbigdat1PlusEstimators <- thedesignbigdat1 + all_estimators
```

Now, let's simulate each design 200 times and evaluate their performance. First, the smaller sample:

::: {.tab} 
<button class="tablinks" onclick="unrolltab(event, 'ch5R17')">R code</button>
<button class="tablinks" onclick="unrolltab(event, 'ch5Stata17')">Stata code</button>
<button class="tablinks" onclick="unrolltab(event, 'ch5Hide17')">Hide</button>
::: {#ch5R17 .tabcontent} 
<br />
```{r, highlight = F, eval = F}
## Summarize characteristics of the smaller-sample designs
sims <- 200
set.seed(12345)
thediagnosisCovAdj1 <- diagnose_design(
  thedesignsmalldat1PlusEstimators,
  sims = sims,
  bootstrap_sims = 0
  )
```
:::
::: {#ch5Stata17 .tabcontent} 
<br />
```{stata, highlight = F, eval = F}
** Call the program once just to make a list of scalars to save
apply_estimators, smaller
local rscalars: r(scalars) // Save all scalars in r() to a local
local to_store "" // Update them in a loop to work properly in simulate
foreach item of local rscalars {
  local to_store "`to_store' `item' = r(`item')"
}

** Summarize characteristics of the smaller-sample designs
set seed 12345
simulate ///
`to_store', ///
reps(200): ///
apply_estimators, smaller

** Create summary matrix
qui des, short // saves number of columns to r()
matrix diagnosands = J((r(k) - 1)/2, 6, .)
matrix rownames diagnosands = "Lm, No Covariates" "Lm, Correct Covariate" "Lm, Mixed Covariates" "Lm, Wrong Covariates" "Lin, Mixed Covariates" "Lin, Correct Covariate"
matrix colnames diagnosands = "Mean estimand" "Mean estimate" "Bias" "SD Estimate" "RMSE" "Power"

** Calculate quantities to include
** (https://declaredesign.org/r/declaredesign/reference/declare_diagnosands.html)
local row = 0
forvalues i = 0/5 {
	
	local ++row
	
	* Estimand
	qui sum ATE, meanonly
	matrix diagnosands[`row', 1] = r(mean)
	
	* Estimate
	qui sum CovAdj`i'_est, meanonly
	matrix diagnosands[`row', 2] = r(mean)
	
	* Bias
	qui gen biascalc = CovAdj`i'_est - ATE
	qui sum biascalc, meanonly
	matrix diagnosands[`row', 3] = r(mean)
	drop biascalc
	
	* SD estimate (based on population variance, no n-1 in the variance denom.)
	qui sum CovAdj`i'_est
	qui gen sdcalc = (CovAdj`i'_est - r(mean))^2
	qui sum sdcalc
	matrix diagnosands[`row', 4] = sqrt(r(sum)/r(N))
	drop sdcalc
	
	* RMSE
	gen atediff = (CovAdj`i'_est - ATE)^2
	qui sum atediff, meanonly
	matrix diagnosands[`row', 5] = sqrt(r(mean))
	drop atediff
	
	* Power
	gen rejectnull = CovAdj`i'_p <= 0.05
	qui sum rejectnull, meanonly
	matrix diagnosands[`row', 6] = r(mean)
	drop rejectnull
	
}

* View the results
matrix list diagnosands
```
::: 
::: {#ch5Hide17 .tabcontent}
::: 
:::

```{r diagnosisCovAdj1, cache=TRUE, results="hide", eval = T, echo = F}
## Summarize characteristics of the smaller-sample designs
sims <- 200
set.seed(12345)
thediagnosisCovAdj1 <- diagnose_design(
  thedesignsmalldat1PlusEstimators,
  sims = sims,
  bootstrap_sims = 0
  )
```

Second, the larger sample: 

::: {.tab} 
<button class="tablinks" onclick="unrolltab(event, 'ch5R18')">R code</button>
<button class="tablinks" onclick="unrolltab(event, 'ch5Stata18')">Stata code</button>
<button class="tablinks" onclick="unrolltab(event, 'ch5Hide18')">Hide</button>
::: {#ch5R18 .tabcontent} 
<br />
```{r, highlight = F, eval = F}
## Summarize characteristics of the larger-sample designs
set.seed(12345)
thediagnosisCovAdj2 <- diagnose_design(
  thedesignbigdat1PlusEstimators,
  sims = sims,
  bootstrap_sims = 0
  )
```
:::
::: {#ch5Stata18 .tabcontent} 
<br />
```{stata, highlight = F, eval = F}
** Summarize characteristics of the large-sample designs
apply_estimators
local rscalars : r(scalars)
local to_store ""
foreach item of local rscalars {
  local to_store "`to_store' `item' = r(`item')"
}
simulate ///
`to_store', ///
reps(200): ///
apply_estimators

** Create summary matrix
qui des, short // get number of columns in r()
matrix diagnosands = J((r(k) - 1)/2, 6, .)
matrix rownames diagnosands = "Lm, No Covariates" "Lm, Correct Covariate" "Lm, Mixed Covariates" "Lm, Wrong Covariates" "Lin, Mixed Covariates" "Lin, Correct Covariate"
matrix colnames diagnosands = "Mean estimand" "Mean estimate" "Bias" "SD Estimate" "RMSE" "Power"

** Calculate quantities to include
local row = 0
forvalues i = 0/5 {
	
	local ++row
	
	* Estimand
	qui sum ATE, meanonly
	matrix diagnosands[`row', 1] = r(mean)
	
	* Estimate
	qui sum CovAdj`i'_est, meanonly
	matrix diagnosands[`row', 2] = r(mean)
	
	* Bias
	qui gen biascalc = CovAdj`i'_est - ATE
	qui sum biascalc, meanonly
	matrix diagnosands[`row', 3] = r(mean)
	drop biascalc
	
	* SD estimate (based on population variance, no n-1 in the variance denom.)
	qui sum CovAdj`i'_est
	qui gen sdcalc = (CovAdj`i'_est - r(mean))^2
	qui sum sdcalc
	matrix diagnosands[`row', 4] = sqrt(r(sum)/r(N))
	drop sdcalc
	
	* RMSE
	gen atediff = (CovAdj`i'_est - ATE)^2
	qui sum atediff, meanonly
	matrix diagnosands[`row', 5] = sqrt(r(mean))
	drop atediff
	
	* Power
	gen rejectnull = CovAdj`i'_p <= 0.05
	qui sum rejectnull, meanonly
	matrix diagnosands[`row', 6] = r(mean)
	drop rejectnull
	
}

* View the results
matrix list diagnosands
```
::: 
::: {#ch5Hide18 .tabcontent}
::: 
:::

```{r diagnosisCovAdj2, cache=TRUE, results="hide", eval = T, echo = F}
## Summarize characteristics of the larger-sample designs
set.seed(12345)
thediagnosisCovAdj2 <- diagnose_design(
  thedesignbigdat1PlusEstimators,
  sims = sims,
  bootstrap_sims = 0
  )
```

From the small sample simulation (N=20), we can see that "CovAdj1: Lm, Correct Covariate" shows fairly large bias compared to the estimator using no covariates at all. The Lin approach using only the correct covariate reduces the bias, but does not erase it. However, the unadjusted estimator has fairly low power where as the Lin approach with the correct covariate "CovAdj5: Lin, Correct Covariate" has excellent power to detect the 1 SD effect built into this experiment. One interesting result here is that the Lin approach is worst (in terms of power) when a mixture of correct and incorrect covariates are added to the linear model following the interaction-and-centering based approach.

```{r diagnostCovAdj1, echo = F, eval = T}
diagcols <- c(3,5,6,7,8,9,10,11)
kable(reshape_diagnosis(thediagnosisCovAdj1)[,diagcols] )
```

The experiment with $N=100$ shows much smaller bias than the small experiment above. Since all estimators allow us to detect the 1 SD effect (Power=1), we can look to the RMSE (Root Mean Squared Error) column to learn about the precision of these estimators. Again, the unadjusted approach has low bias, but it now has the largest standard error. While the Lin approach with a mixture of correct and incorrect covariates has low bias, it shows slightly worse precision than one that gets the choice of covariates correct.

```{r diagnostCovAdj2, echo = F, eval = T}
kable(reshape_diagnosis(thediagnosisCovAdj2)[,diagcols] )
```

```{r setupgraph, eval = F, echo = F}
## Bill: not sure what the intended takeaway is here.
## I feel like the table already communicates everything.
## And this code (and the chunk below) was already left out of the SOP.
## Past authors had trouble getting this to run? To potentially revisit.
## Leaving out for now (see chunk options)
simdesignsCovAdj1 <- get_simulations(thediagnosisCovAdj1)
trueATE1covadj <- with(dat1,mean(y1-y0))
## simdesigns <- simulate_design(thedesign,sims=sims)
simmeansCovAdj1 <- simdesignsCovAdj1 %>% group_by(estimator) %>% summarize(expest=mean(estimate))
```

```{r plotcovadjsims, eval=FALSE, echo=FALSE}
g <- ggplot(data=simdesignsCovAdj1,aes(x=estimate,color=estimator)) +
	geom_density(size=2) +
	geom_vline(xintercept=trueATE1covadj) +
	geom_point(data=simmeansCovAdj1,aes(x=expest,y=rep(0,6)),shape=17,size=6) +
	theme_bw() +
	theme(legend.position = c(.9,.8)) +
scale_x_continuous(limits = c(-10, 20))

print(g)
```

The Lin approach works well when covariates are few and sample sizes
are large, but these simulations show where the approach is weaker: when there are many covariates (relative to the number of observations), some of which may not actually be correlated with the outcome. In this case, the estimator involving both correct and irrelevant covariates included 18 terms. Fitting a model with 18 terms using a small sample, e.g. $N=20$, allows nearly any observation to exert undue influence, increases the risk of serious multicollinearity, and leads to overfitting problems in general.

Our team does not often run into such an extreme version of this problem because our studies have tended to involve many thousands of units and relatively few covariates. However, when we do encounter situations where the number of covariates (or fixed effects) we'd like to account for is large, we consider a few alternative approaches: (1) standard linear covariate adjustment; (2) collapsing the covariates into fewer dimensions (e.g., using principal component analysis); or (3) working with a residualized version of the outcome, as described below.

### The Rosenbaum Approach to Covariance Adjustment

When we have many covariates, Lin adjustment may not be appropriate if the number of parameters to estimate becomes too large relative to a study's sample size. @rosenbaum:2002a suggests an alternative approach in which the outcomes are regressed on covariates, ignoring treatment assignment, and then the residuals from that first regression are used as the outcome in a second regression to estimate an average treatment effect. Again, we evaluate this approach through simulation.

First, code we use to generate Rosenbaum (2002) estimators:

::: {.tab} 
<button class="tablinks" onclick="unrolltab(event, 'ch5R19')">R code</button>
<button class="tablinks" onclick="unrolltab(event, 'ch5Stata19')">Stata code</button>
<button class="tablinks" onclick="unrolltab(event, 'ch5Hide19')">Hide</button>
::: {#ch5R19 .tabcontent} 
<br />
```{r, highlight = F, eval = F}
## The argument covs is a character vector of names of covariates.
## The function below generates a separate estimation command 
## that performs Rosenbaum (2002) adjustment for the specified
## covariates, given a sample of data. This new function can
## then by used as an estimator in simulations below. This code
## takes advantage of the concept of "function factories."
## See, e.g.: https://adv-r.hadley.nz/function-factories.html.
make_est_fun<-function(covs){
  
  force(covs)
  
  ## Generate a formula from the character vector of covariates
  covfmla <- reformulate(covs,response="Y")
  
  ## What the new function to-be-generated will do,
  ## given the model generated above:
  function(data){
    data$e_y <- residuals(lm(covfmla,data=data))
    obj <- lm_robust(e_y~Znew,data=data)
    res <- tidy(obj) %>% filter(term=="Znew")
    return(res)
    }
  
  }

## Make Rosenbaum (2002) estimators for different groups of covariates
est_fun_correct <- make_est_fun("cov2")
est_fun_mixed<- make_est_fun(c("cov1","cov2","cov3","cov4","cov5","cov6","cov7","cov8"))
est_fun_incorrect <- make_est_fun(c("cov1","cov3","cov4","cov5","cov6"))

## Declare additional estimators, to add to the designs above
estCov6 <- declare_estimator(handler = label_estimator(est_fun_correct), inquiry=theestimanddat1, label="CovAdj6: Resid, Correct")
estCov7 <- declare_estimator(handler = label_estimator(est_fun_mixed), inquiry=theestimanddat1, label="CovAdj7: Resid, Mixed")
estCov8 <- declare_estimator(handler = label_estimator(est_fun_incorrect), inquiry=theestimanddat1, label="CovAdj8: Resid, Incorrect")

## Add the additional estimators
thedesignsmalldat1PlusRoseEstimators <- thedesignsmalldat1 + all_estimators + estCov6 + estCov7 + estCov8
thedesignbigdat1PlusRoseEstimators <- thedesignbigdat1 + all_estimators + estCov6 + estCov7 + estCov8
```
:::
::: {#ch5Stata19 .tabcontent} 
<br />
```{stata, highlight = F, eval = F}
** Define a program to perform Rosenbaum estimation
* (with all variables specified in a varlist as in regress).
capture program drop rosenbaum_est
program define rosenbaum_est, rclass

	* Assumed structure of varlist: outcome treatment list_of_covariates.
	syntax varlist
	* Alternative 1: syntax, outcome(varname) treatment(varname) covar(varlist)
	* Alternative 2: syntax varlist, outcome(varname) treatment(varname)
	
	* Pull out first var of varlist as outcome
	tokenize `varlist'
	local outcome `1'
	macro shift
	
	* Pull out second var of varlist as treatment
	local treat_cov `*'
	tokenize `treat_cov'
	local treatment `1'
	macro shift
	
	* The rest are treated as covariates
	local covar `*'
	
	* Estimation
	reg `outcome' `covar'
	tempvar yhat
	predict `yhat', xb
	tempvar resid
	gen `resid' = `outcome' - `yhat'
	reg `resid' `treatment', vce(hc2)
	
	* Output
	return scalar out_est = _b[`treatment']
	return scalar out_p = r(table)["pvalue", "`treatment'"]

end

** Program to apply this estimator to the generated data:
capture program drop apply_estimators_2
program define apply_estimators_2, rclass

	syntax[, smaller ///
		propsimtreat(real 0.5) ]
		
	** Call the sample generation / random assignment program
	sample_from, `smaller' propsimtreat(`propsimtreat')
	return scalar ATE = r(ATE)

	** CovAdj6: Resid, Correct
	qui rosenbaum_est ynew znew cov2
	return scalar CovAdj6_est = r(out_est)
	return scalar CovAdj6_p = r(out_p)
	
	** CovAdj7: Resid, Mixed
	qui rosenbaum_est ynew znew cov1-cov8
	return scalar CovAdj7_est = r(out_est)
	return scalar CovAdj7_p = r(out_p)
	
	** CovAdj8: Resid, Incorrect
	qui rosenbaum_est ynew znew cov1 cov3-cov6
	return scalar CovAdj8_est = r(out_est)
	return scalar CovAdj8_p = r(out_p)

end
```
::: 
::: {#ch5Hide19 .tabcontent}
::: 
:::

```{r rosenbaumstylecov, eval = T, echo = F}
## The argument covs is a character vector of names of covariates.
## The function below generates a separate estimation command 
## that performs Rosenbaum (2002) adjustment for the specified
## covariates, given a sample of data. This new function can
## then by used as an estimator in simulations below. This code
## takes advantage of the concept of "function factories."
## See, e.g.: https://adv-r.hadley.nz/function-factories.html.
make_est_fun<-function(covs){
  
  force(covs)
  
  ## Generate a formula from the character vector of covariates
  covfmla <- reformulate(covs,response="Y")
  
  ## What the new function to-be-generated will do,
  ## given the model generated above:
  function(data){
    data$e_y <- residuals(lm(covfmla,data=data))
    obj <- lm_robust(e_y~Znew,data=data)
    res <- tidy(obj) %>% filter(term=="Znew")
    return(res)
    }
  
  }

## Make Rosenbaum (2002) estimators for different groups of covariates
est_fun_correct <- make_est_fun("cov2")
est_fun_mixed<- make_est_fun(c("cov1","cov2","cov3","cov4","cov5","cov6","cov7","cov8"))
est_fun_incorrect <- make_est_fun(c("cov1","cov3","cov4","cov5","cov6"))

## Declare additional estimators, to add to the designs above
estCov6 <- declare_estimator(handler = label_estimator(est_fun_correct), inquiry=theestimanddat1, label="CovAdj6: Resid, Correct")
estCov7 <- declare_estimator(handler = label_estimator(est_fun_mixed), inquiry=theestimanddat1, label="CovAdj7: Resid, Mixed")
estCov8 <- declare_estimator(handler = label_estimator(est_fun_incorrect), inquiry=theestimanddat1, label="CovAdj8: Resid, Incorrect")

## Add the additional estimators
thedesignsmalldat1PlusRoseEstimators <- thedesignsmalldat1 + all_estimators + estCov6 + estCov7 + estCov8
thedesignbigdat1PlusRoseEstimators <- thedesignbigdat1 + all_estimators + estCov6 + estCov7 + estCov8
```

Second, we evaluate a design that applies those estimators to our smaller sample:

::: {.tab} 
<button class="tablinks" onclick="unrolltab(event, 'ch5R20')">R code</button>
<button class="tablinks" onclick="unrolltab(event, 'ch5Stata20')">Stata code</button>
<button class="tablinks" onclick="unrolltab(event, 'ch5Hide20')">Hide</button>
::: {#ch5R20 .tabcontent} 
<br />
```{r, highlight = F, eval = F}
set.seed(12345)
thediagnosisCovAdj3 <- diagnose_design(
  thedesignsmalldat1PlusRoseEstimators,
  sims = sims,
  bootstrap_sims = 0
  )
```
:::
::: {#ch5Stata20 .tabcontent} 
<br />
```{stata, highlight = F, eval = F}
** Summarize characteristics of the smaller-sample designs
apply_estimators_2, smaller // Looking only at the rosenbaum estimators now
local rscalars : r(scalars)
local to_store ""
foreach item of local rscalars {
  local to_store "`to_store' `item' = r(`item')"
}
simulate ///
`to_store', ///
reps(200): ///
apply_estimators_2, smaller

** Create summary matrix
qui des, short
matrix diagnosands = J((`r(k)' - 1)/2, 6, .)
matrix rownames diagnosands = "Resid, Correct" "Resid, Mixed" "Resid, Incorrect"
matrix colnames diagnosands = "Mean estimand" "Mean estimate" "Bias" "SD Estimate" "RMSE" "Power"

** Calculate quantities to include
local row = 0
forvalues i = 6/8 {
	
	local ++row
	
	* Estimand
	qui sum ATE, meanonly
	matrix diagnosands[`row', 1] = r(mean)
	
	* Estimate
	qui sum CovAdj`i'_est, meanonly
	matrix diagnosands[`row', 2] = r(mean)
	
	* Bias
	qui gen biascalc = CovAdj`i'_est - ATE
	qui sum biascalc, meanonly
	matrix diagnosands[`row', 3] = r(mean)
	drop biascalc
	
	* SD estimate (based on population variance, no n-1 in the variance denom.)
	qui sum CovAdj`i'_est
	qui gen sdcalc = (CovAdj`i'_est - r(mean))^2
	qui sum sdcalc
	matrix diagnosands[`row', 4] = sqrt(r(sum)/r(N))
	drop sdcalc
	
	* RMSE
	gen atediff = (CovAdj`i'_est - ATE)^2
	qui sum atediff, meanonly
	matrix diagnosands[`row', 5] = sqrt(r(mean))
	drop atediff
	
	* Power
	gen rejectnull = CovAdj`i'_p <= 0.05
	qui sum rejectnull, meanonly
	matrix diagnosands[`row', 6] = r(mean)
	drop rejectnull
	
}

* View the results
matrix list diagnosands
```
::: 
::: {#ch5Hide20 .tabcontent}
::: 
:::

```{r diagnosisCovAdj3, cache=TRUE, results="hide", eval = T, echo = F}
set.seed(12345)
thediagnosisCovAdj3 <- diagnose_design(
  thedesignsmalldat1PlusRoseEstimators,
  sims = sims,
  bootstrap_sims = 0
  )
```

Finally, we evaluate a design that applies those estimators (alongside the others above) to our larger sample:

::: {.tab} 
<button class="tablinks" onclick="unrolltab(event, 'ch5R21')">R code</button>
<button class="tablinks" onclick="unrolltab(event, 'ch5Stata21')">Stata code</button>
<button class="tablinks" onclick="unrolltab(event, 'ch5Hide21')">Hide</button>
::: {#ch5R21 .tabcontent} 
<br />
```{r, highlight = F, eval = F}
set.seed(12345)
thediagnosisCovAdj4 <- diagnose_design(
  thedesignbigdat1PlusRoseEstimators,
  sims = sims,
  bootstrap_sims = 0
  )
```
:::
::: {#ch5Stata21 .tabcontent} 
<br />
```{stata, highlight = F, eval = F}
** Summarize characteristics of the larger-sample designs
apply_estimators_2 // Looking only at the rosenbaum estimators now
local rscalars : r(scalars)
local to_store ""
foreach item of local rscalars {
  local to_store "`to_store' `item' = r(`item')"
}
di "`to_store'"
simulate ///
`to_store', ///
reps(200): ///
apply_estimators_2

** Create summary matrix
qui des, short
matrix diagnosands = J((`r(k)' - 1)/2, 6, .)
matrix rownames diagnosands = "Resid, Correct" "Resid, Mixed" "Resid, Incorrect"
matrix colnames diagnosands = "Mean estimand" "Mean estimate" "Bias" "SD Estimate" "RMSE" "Power"

** Calculate quantities to include
local row = 0
forvalues i = 6/8 {
	
	local ++row
	
	* Estimand
	qui sum ATE, meanonly
	matrix diagnosands[`row', 1] = r(mean)
	
	* Estimate
	qui sum CovAdj`i'_est, meanonly
	matrix diagnosands[`row', 2] = r(mean)
	
	* Bias
	qui gen biascalc = CovAdj`i'_est - ATE
	qui sum biascalc, meanonly
	matrix diagnosands[`row', 3] = r(mean)
	drop biascalc
	
	* SD estimate (based on population variance, no n-1 in the variance denom.)
	qui sum CovAdj`i'_est
	qui gen sdcalc = (CovAdj`i'_est - r(mean))^2
	qui sum sdcalc
	matrix diagnosands[`row', 4] = sqrt(r(sum)/r(N))
	drop sdcalc
	
	* RMSE
	gen atediff = (CovAdj`i'_est - ATE)^2
	qui sum atediff, meanonly
	matrix diagnosands[`row', 5] = sqrt(r(mean))
	drop atediff
	
	* Power
	gen rejectnull = CovAdj`i'_p <= 0.05
	qui sum rejectnull, meanonly
	matrix diagnosands[`row', 6] = r(mean)
	drop rejectnull
	
}

* View the results
matrix list diagnosands

* Return to main data from before the covariance adjustment simulations
use main.dta, clear
erase main.dta
```
::: 
::: {#ch5Hide21 .tabcontent}
::: 
:::

```{r diagnosisCovAdj4, cache=TRUE, results="hide", eval = T, echo = F}
set.seed(12345)
thediagnosisCovAdj4 <- diagnose_design(
  thedesignbigdat1PlusRoseEstimators,
  sims = sims,
  bootstrap_sims = 0
  )
```

With a small sample (N=20), the Rosenbaum-style approach yields very little
bias and quite high power using the correct covariate ("CovAdj6: Resid,
Correct"), but performs poorly in terms of bias and precision with incorrect
covariates.

```{r diagnostCovAdj3, echo = F, eval = T}
kable(reshape_diagnosis(thediagnosisCovAdj3)[7:9,diagcols])
```

With a larger experiment, the bias goes down, but coverage is still relatively worse when incorrect covariates are included. We speculate that performance might improve if we fit covariance adjustment models that produce residuals separately for the treated and control groups.

```{r diagnostCovAdj4, echo = F, eval = T}
kable(reshape_diagnosis(thediagnosisCovAdj4)[7:9,diagcols])
```

## How to choose covariates for covariance adjustment?

Our analysis plans commonly specify a few covariates based on what we know about the mechanisms and context of the study. In general, if we have a measurement of the outcome *before the treatment was assigned* --- the baseline outcome --- we try to use it as a part of a blocking or covariance adjustment strategy.

When we have access to many covariates, we may sometimes use simple machine
learning methods to select variables that strongly predict the outcome, such as a lasso model. In particular, we tend to prefer the *adaptive lasso* rather than a simple lasso because it has better theoretical properties [@zou2006adaptive], but also because the adaptive lasso tends to produce sparser results --- and the bias from covariance adjustment can sometimes be significance if we include too many covariates.

<!-- #### Example of using the adaptive lasso for variable selection -->

<!-- Here, we show how we use *baseline data*, or data collected before treatment was assigned or a new policy was implemented, to choose covariates to include later in blocking or covariance adjustment. -->

<!-- We tend to use the adaptive lasso rather than the simple lasso because the adaptive lasso has better theoretical properties [@zou2006adaptive] but also because the adaptive lasso tends to produce sparser results --- and the bias from covariance adjustment can be severe if we add many many covariates to a covriance adjustment procedure. -->

<!-- **TO DO** -->

## Block-randomized trials {#blockrandanalysis}

We design block-randomized trials by splitting units into groups based on predefined characteristics --- covariates that cannot be changed by the experimental treatment --- and then randomly assigning treatment within each
group. We use this procedure when we want to increase our ability to detect
signal from noise. It assumes that the noise, or random variation in the outcome measure, is associated with the variables that we use to assign blocks. For example, if we imagine that patterns of energy use will tend to differ according to size of family, we may create blocks or strata of different family sizes and randomly assign an energy saving intervention separately within those blocks.

We also design block-randomized experiments when we want to assess effects within and across subgroups (for example, if we want to ensure that we have enough statistical power to detect a *difference in effects* between veterans and non-veterans). If we have complete random assignment, it is likely that the proportion of veterans assigned treatment will not be exactly same as the proportion of non-veterans receiving treatment. However, if we stratify or block the group on military status, and randomly assign treatment and control within each group, we can then ensure that equal proportions (or numbers) or veterans and non-veterans receive the treatment and control.

Most of the general ideas that we demonstrated in the context of completely randomized trials have direct analogues in the case of block randomized trials. The only additional question that arises with block randomized trials is about how to weight the contributions of each individual block when calculating an overall average treatment effect or testing an overall hypothesis about treatment effects. We begin with the simple case of testing the sharp null of no effects when we have a binary outcome --- in the case of a Cochran-Mantel-Haenszel test the weighting of different blocks is handled automatically.

### Testing binary outcomes under block randomization: Cochran-Mantel-Haenszel (CMH) test for K X 2 X 2 tables

For block-randomized trials with a binary outcome, we might use the CMH test to evaluate the null of no effect.^[As discussed above, by "binary outcome," we mean that there are only two possible values, and we have recorded one or the other. Usually, we use indicator variables (0, 1) to record these outcomes.] This is one way of keeping the outcomes for each strata separate rather than pooling them together --- under blocking, we are effectively repeating the same experiment separately within each stratum. The CMH test tells us if the odds ratios across strata support an association between the outcome and treatment [@cochran_methods_1954;@mantel_statistical_1959].

To set up the CMH test, we need *k* sets of 2x2 contingency tables. Suppose
the table below represents outcomes from stratum *i* where A, B, C, and D are
counts of observations:

Assignment| Response  | No response | Total
--------- |---------- |----------   |----------
Treatment | A         | B           | A+B
Control   | C         | D           | C+D
Total     | A+C       | B+D         | A+B+C+D = T

The CMH test statistic takes the sum of the deviations between observed and expected outcomes within each stratum ($O_{i}$ and $E_{i}$, respectively), squares this sum, and compares it to the sum of the variance within each strata:

$$
CMH = \frac{\big[\sum_{i=1}^{k} (O_{i} -
E_{i})\big]^{2}}{\sum_{i=1}^{k}{\mathrm{\var}[O_{i}]}}
$$

where $$O_{i} = \frac{(A_i+B_i)(A_i+C_i)}{T_i}$$

and $$\var[O_{i}] =
\frac{(A_i+B_i)(A_i+C_i)(B_i+D_i)(C_i+D_i)}{{T_i}^2(T_i-1)}$$

In large-enough samples, if there are no associations between treatment and
outcomes across strata, we would expect to see an odds ratio equal to 1. Across randomizations of large-enough samples, this test statistic follows an asymptotic $\chi^2$ distribution with degrees of freedom = 1.

For a standard two-arm trial, the odds ratio for a given stratum would be:

$$OR = \frac{A/B}{C/D} = \frac{AD}{BC}$$

Across strata, could then find an overall common odds ratio as follows:

$$
\begin{equation}
 OR_{CMH} = \frac{\sum_{i=1}^{k} (A_{i}D_{i}/T_{i})}{\sum_{i=1}^{k}{B_{i}C_{i}}{T_{i}}}
\end{equation}
$$
If this common odds ratio is substantially greater than 1, then we should suspect that there is an association between the outcome and treatment across strata, and the CMH test statistic would be large. If $OR_{CMH} = 1$, this would instead support the null hypothesis that there is no association between treatment and the outcome, and the CMH test statistic would be small.

We could also use the CMH test to compare odds ratios between experiments, rather than comparing against the null that the common odds ratio = 1.

### Estimating an overall average treatment effect {#blockrandate}

Whether or not we randomly assign a policy intervention within blocks or strata, our team nearly always reports a single estimate of the average treatment effect. To review, we define the overall ATE (the theoretical estimand) as a simple average of the individual treatment effects. For two treatments, we might write this individual-level causal effect as $\tau_i = y_{i,0} - y_{i,1}$. We'd therefore express the overall (true) ATE across our sample as as $\bar{\tau}=(1/n) \sum_{i=1}^n \tau_i$. Unfortunately, this quantity is unobservable.

In blocked designs, we have randomly assigned the intervention within each block independently. As we note above, this (1) increases precision and (2) supports more credible subgroup analysis. How might we "analyze as we have randomized" to learn about $\bar{\tau}$ using observable data? Our approach is to build up from the block-level. See @gerber_field_2012 for more context.

Say, for example, that the unobserved ATE within a given block, $b$, was $\text{ATE}_{b}=\bar{\tau}_b=(1/n_b)\sum_{i=1}^{n_b} \tau_{i}$. Here, we are averaging the individual level treatment effects ($\tau_{i}$) across all $n_b$ people in block $b$. Now, imagine that we had an experiment with blocks of different sizes (and perhaps with different proportions of units assigned to treatment --- e.g., certain blocks may be more expensive places in which to run an experiment). We can learn about $\bar{\tau}$ by first reframing it as the block-size weighted average of the true within-block treatment effects:^[We illustrate the equivalence between $\bar{\tau}$ and $\bar{\tau}_{\text{nbwt}$ in our code below: the comparison between `trueATE1` and `trueATE2`.] $$\bar{\tau} = \bar{\tau}_{\text{nbwt}} = (1/B) \sum_{b=1}^B (n_b/n) \bar{\tau}_b$$

We can then estimate $\bar{\tau}_{\text{nbwt}}$ based on its observable analogue, just as we have with a completely randomized experiment. Blocks are essentially their own randomized experiments, so we can estimate each $\bar{\tau}_b$ using the following unbiased estimator, where $i \in t$ means "for $i$ in the treatment group" and where $m_b$ is the number of units assigned to treatment in block $b$: $$\hat{\tau}_b=\sum_{i \in t} Y_{ib}/m_b - \sum_{i \in c} Y_{ib}/(n_b - m_b)$$

We would then plug the $\hat{\tau}_b$ estimates into the expression for $\bar{\tau}_{\text{nbwt}}$ above, yielding $\hat{\bar{\tau}}_{\text{nbwt}}$. *Note that many people do not use this unbiased estimator because its precision is worse that those of another, slightly biased estimator.* We illustrate both methods below, the block-size weighted estimator and what we call the "precision-weighted" estimator. We also offer some reflections on when a biased estimator that tends to produce answers closer to the truth might be preferred over an unbiased estimator where any given estimate may be farther from the truth.

The precision-weighted estimator uses *harmonic-weights*. We have also tended to call it a "precision-weighted average." The weights combine block size, $n_b$, and the proportion of the block assigned to treatment, $p_b = (1/n_b) \sum_{i=1}^{n_b} Z_{ib}$, for a binary treatment, $Z_{ib}$. The weight within each block is $h_b = n_b p_b (1 - p_b)$, and the theoretical estimand is: $$\bar{\tau}_{\text{hbwt}}= (1/B) \sum_{b=1}^B (1/h_b) \bar{\tau}_b$$

This approach may sound unfamiliar. But using precisions weights to average a set of within-block treatment effect estimates is equivalent to the more common practice of regression adjustment for linear, additive blocked fixed effects (and we show an example of this in our code below).

Two more points are worth emphasizing. First, by weighting blocks proportionally to their contributions to overall sample size, $\bar{\tau}_{\text{nbwt}}$ effectively treats all units equally in terms of their contributions to the ATE. In contrast, $\bar{\tau}_{\text{hbwt}}$ weights blocks (and therefore the individuals within them) relatively more if they have a treatment probability closer to 0.5.^[A block of 100 units would have a harmonic weight of 25 with a treatment probability of 0.5 $(100 \times 0.5 \times 0.5)$, but a harmonic weight of 18.75 under an alternative treatment probability of 0.25 $(100 \times 0.25 \times 0.75)$.] In effect, the precision benefits come from giving slightly more explanatory power to units in blocks with more balanced treatment assignment, at the expense of recovering $\bar{\tau}$ as closely as possible.

Second, it should be clear from the expression for $h_b$ above that the precision-weighted approach will give no weight to units in blocks where the probability of treatment is 0 or 1 (i.e., all units are eiter treated or untreated, due perhaps to treatment administration issues). What may not be obvious yet is that the unbiased, block-size weighted approach suffers from the same problem. Treatment probabilities may not factor into $n_b/n$, but these are block-level weights that we can't apply without a within-block treatment effect estimate (which is not possible if there's no within-block variation in treatment). Moreover, the unit level expression for these unbiased weights *is* a function of the within-block treated proportion $(p_{ib})$:

$$nbwt_{i} = Z_{i}/p_{ib} + (1-Z_{i})/(1 - p_{ib})$$

For reference, an expression for unit-level precision-weights is:

$$hbwt_{i} = nbwt_{i} \times p_{ib} \times (1 - p_{ib})$$

Now, we'll show multiple approaches to applying these estimators. We'll also demonstrate (1) that ignoring the blocks when estimating treatment effects can produce problems in both estimation and testing, and (2) that the block-size weighted approaches are unbiased but possibly less precise than the precision weighted approaches.

::: {.tab} 
<button class="tablinks" onclick="unrolltab(event, 'ch5R22')">R code</button>
<button class="tablinks" onclick="unrolltab(event, 'ch5Stata22')">Stata code</button>
<button class="tablinks" onclick="unrolltab(event, 'ch5Hide22')">Hide</button>
::: {#ch5R22 .tabcontent} 
<br />
```{r, highlight = F, eval = F}
## Create block sizes and create block weights
B <- 10 # Number of blocks
dat <- data.frame(b=rep(1:B,c(8,20,30,40,50,60,70,80,100,800)))
dat$bF <- factor(dat$b)

## x1 is a covariate that strongly predicts the outcome without treatment
set.seed(2201)
dat <- group_by(dat,b) %>%
  mutate(
    nb=n(),
    x1=rpois(n = nb,lambda=runif(1,min=1,max=2000))
    )

## The treatment effect varies by block size (sqrt(nb) because nb has such a large range.)
dat <- group_by(dat,b) %>% 
  mutate(
    y0=sd(x1)*x1+rchisq(n=nb,df=1),
    y0=y0*(y0>quantile(y0,.05)),
    tauib = -(sd(y0))*sqrt(nb) + rnorm(n(),mean=0,sd=sd(y0)),
    y1=y0+tauib,
    y1=y1*(y1>0)
    )
blockpredpower <- summary(lm(y0~bF,data=dat))$r.squared
```
:::
::: {#ch5Stata22 .tabcontent} 
<br />
```{stata, highlight = F, eval = F}
** Data simulation provided for illustration.
** In practice, will use data generated by the R code,
** for the sake of comparison.

** Create block sizes and create block weights
clear
local sizes 8 20 30 40 50 60 70 80 100 800 // Block sample sizes
set obs 10 // Number of blocks
qui gen bf = . // Categorical block var
qui gen nb = . // Number in block
local i = 0
foreach size of local sizes {
	local ++i
	replace nb = `size' if _n == `i'
	replace bf = `i' if _n == `i'
}
gen lambda = runiform(1, 2000) // For poisson generation below
expand nb
sort bf

** x1 is a covariate that strongly predicts the outcome without treatment
set seed 2201
gen x1 = rpoisson(lambda)

** The treatment effect varies by block size (sqrt(nb) because nb has such a large range.)
bysort bf: egen sd_x1 = sd(x1) 
gen y0 = (sd_x1 * x1) + rchi2(1)
bysort bf: egen p5 = pctile(y0), p(5)
replace y0 = 0 if y0 <= p5
bysort bf: egen sd_y0 = sd(y0)
gen tauib = -(sd_y0)*sqrt(nb) + rnormal(0, sd_y0)
gen y1 = y0 + tauib
replace y1 = 0 if y1 <= 0
qui reg y0 i.bf
di round(e(r2), 0.0001)
drop sd_y0 sd_x1 p5
```
::: 
::: {#ch5Hide22 .tabcontent}
::: 
:::

```{r newblockeddat, echo = F, eval = T}
## Create block sizes and create block weights
B <- 10 # Number of blocks
dat <- data.frame(b=rep(1:B,c(8,20,30,40,50,60,70,80,100,800)))
dat$bF <- factor(dat$b)

## x1 is a covariate that strongly predicts the outcome without treatment
set.seed(2201)
dat <- group_by(dat,b) %>%
  mutate(
    nb=n(),
    x1=rpois(n = nb,lambda=runif(1,min=1,max=2000))
    )

## The treatment effect varies by block size (sqrt(nb) because nb has such a large range.)
dat <- group_by(dat,b) %>% 
  mutate(
    y0=sd(x1)*x1+rchisq(n=nb,df=1),
    y0=y0*(y0>quantile(y0,.05)),
    tauib = -(sd(y0))*sqrt(nb) + rnorm(n(),mean=0,sd=sd(y0)),
    y1=y0+tauib,
    y1=y1*(y1>0)
    )
blockpredpower <- summary(lm(y0~bF,data=dat))$r.squared
```

```{r, echo = F, eval = T, include = F}
## Again, exporting for use in Stata example code.
write.csv(dat, "blocksimdat.csv", row.names = F)
```

To make the differences between these estimation approaches more vivid,
we've create a dataset with blocks of widely varying sizes. Half of the blocks have half of the units assigned to treatment and the other half 10% of the units assigned to treatment. Notice also that the baseline outcomes are strongly predicted by the blocks ($R^2$ around $`r round(blockpredpower,2)`$).

We'll use `DeclareDesign` (in R) to assess bias, coverage and power (or precision) of several different approaches to applying each estimator. We'll also provide a parallel Stata example. The next code block sets up the simulation and also demonstrates different approaches to estimating an ATE (as usual, we can only do this because we are using simulation to compare these different statistical techniques in a setting where we know the true data generating process).

::: {.tab} 
<button class="tablinks" onclick="unrolltab(event, 'ch5R23')">R code</button>
<button class="tablinks" onclick="unrolltab(event, 'ch5Stata23')">Stata code</button>
<button class="tablinks" onclick="unrolltab(event, 'ch5Hide23')">Hide</button>
::: {#ch5R23 .tabcontent} 
<br />
```{r, highlight = F, eval = F}
## Using the Declare Design Machinery to ensure that 
## the data here and the simulations below match

## Declare the population
thepop <- declare_population(dat)

## Represent the potential outcomes with a function
po_function <- function(data){
	data$Y_Z_0 <- data$y0
	data$Y_Z_1 <- data$y1
	data
}

## Use this function to declare the potential outcomes
theys <- declare_potential_outcomes(handler = po_function)

## Declare the desired inquiry
theestimand <- declare_inquiry(ATE = mean(Y_Z_1 - Y_Z_0))

## Declare treatment assignment
numtreated <- sort(unique(dat$nb))/rep(c(2,10),B/2)
theassign <- declare_assignment(Z = block_ra(blocks = bF, block_m = numtreated))

## Declare revealed data
theobsident <- declare_reveal(Y, Z)

## Combine these design elements together
thedesign <- thepop + theys + theestimand + theassign + theobsident

# Draw data matching this design
set.seed(2201)
dat2 <- draw_data(thedesign)

## Now add individual-level weights to the data
## (under complete_ra within blocks, these will
## be the same across re-randomizations; OK to
## not recalculate within the simulation).
dat2 <- dat2 %>% 
  group_by(b) %>% 
  mutate(
    nb = n(), # Size of block
    pib = mean(Z), # Prob. of treatment assignment
    nTb = sum(Z), # Number treated
    nCb = nb - nTb, # Number control
    nbwt = (Z/pib) + ((1-Z)/(1-pib)), # Unbiased regression weight
    hbwt = nbwt * (pib * (1 - pib)) # Precision regression weight
    )

## Declare a new population, and generate another design
thepop2 <- declare_population(dat2)
thedesign2 <- thepop2 + theys + theestimand + theassign + theobsident

## Create the block level dataset, with block level weights.
datB <- group_by(dat2,b) %>%
  summarize(
    taub = mean(Y[Z==1]) - mean(Y[Z==0]), # Effect (variance below)
    truetaub = mean(y1) - mean(y0), # True effect
    nb = n(), # Number in block
    nTb = sum(Z), # Number treated
    nCb = nb - nTb, # Number control
    estvartaub = (nb/(nb-1)) * (var(Y[Z==1])/nTb) + (var(Y[Z==0])/nCb),
    pb = mean(Z), # Proportion treated
    nbwt = unique(nb/nrow(dat2)), # Block-size weight
    pbwt = pb * ( 1 - pb),
    hbwt2 = nbwt * pbwt, # Precision weights
    hbwt3 = pbwt * nb,
    hbwt = (2*(nCb * nTb)/(nTb + nCb))
    )
datB$greenlabrule <- 20*datB$hbwt3/sum(datB$hbwt3)

## All of these expressions of the harmonic mean weight are the same
datB$hbwt01 <- datB$hbwt/sum(datB$hbwt)
datB$hbwt02 <- datB$hbwt2/sum(datB$hbwt2)
datB$hbwt03 <- datB$hbwt3/sum(datB$hbwt3)
stopifnot(all.equal(datB$hbwt01, datB$hbwt02))
stopifnot(all.equal(datB$hbwt01, datB$hbwt03))

## What is the "true" ATE?
trueATE1 <- with(dat2, mean(y1) - mean(y0))
trueATE2 <- with(datB, sum(truetaub*nbwt))
stopifnot(all.equal(trueATE1, trueATE2))
## We could define the following as an estimand, too.
## trueATE3 <- with(datB, sum(truetaub*hbwt01))
## c(trueATE1,trueATE2,trueATE3)

## We can get the same answer using R's weighted.mean command
trueATE2b <- weighted.mean(datB$truetaub, w=datB$nbwt)
stopifnot(all.equal(trueATE2b, trueATE2))
```
:::
::: {#ch5Stata23 .tabcontent} 
<br />
```{stata, highlight = F, eval = F}
** Define a program to load base dataset and randomly re-assign treatment
capture program drop sample_from
program define sample_from, rclass
	
	* Load simulated data from R
	import delimited using "blocksimdat.csv", clear
	
	* Get # treated for each block
	levelsof nb, local(nblevels)
	local numtreat
	local i = 0
	foreach l of local nblevels {
		
		local ++i // block indices (1, 2, 3,... 10)
		
		* even block indices (2nd, 4th, etc.) get 10% treated
		if mod(`i',2) == 0 {
			local temp = `l'/10 
			local numtreat `numtreat' `temp'
		}
		
		* odd get 50% treated
		else {
			local temp = `l'/2
			local numtreat `numtreat' `temp'
		}
		
	}
	
	* Re-assign (simulated) treatment in this draw of the data
	capture drop __0*
	block_ra znew, block_var(bf) block_m(`numtreat') replace
	
	* No additional treatment effects
	* (assigning new potential outcomes)
	gen y_znew_0 = y0
	gen y_znew_1 = y1

	* Get true ATE in r()
	gen true_effect = y_znew_1 - y_znew_0
	sum true_effect, meanonly
	return scalar ATE = r(mean)
	
	* Revealed outcome
	gen ynew = (znew * y_znew_1) + ((1 - znew) * y_znew_0)
	
	* Now add individual-level weights to the data
	* (under complete_ra within blocks, these will
	* be the same across re-randomizations).
	bysort bf: egen pib = mean(znew) // Prob. of treatment assignment
	bysort bf: egen nTb = total(znew) // Number treated
	gen nCb = nb - nTb // Number control
	gen nbwt = (znew/pib) + ((1-znew)/(1-pib)) // Unbiased regression weight
	gen hbwt = nbwt * (pib * (1 - pib)) // Precision regression weight

end

** Draw data using this program once and save the true ATE
set seed 2201
sample_from
global trueATE1 = round(r(ATE), 0.01)

** For illustration: how to prepare block-level data as in the R code.
** In practice, we'll use data from the R code for comparison.

** Prepare to create a block level dataset, with block level weights.
bysort bf znew: egen treatmean = mean(ynew) if znew == 1 // Treat and control means
bysort bf znew: egen controlmean = mean(ynew) if znew == 0
bysort bf znew: egen treatsd = sd(ynew) if znew == 1 // Treat and control SD
bysort bf znew: egen controlsd = sd(ynew) if znew == 0
qui count
gen total_n = r(N)

** Collapse to block-level
collapse ///
(mean) treatmean controlmean treatsd controlsd y1 y0 pb = znew ///
(first) nb nTb nCb total_n, ///
by(bf)

** Additional variable preparation
gen taub = treatmean - controlmean // Observed effect (variance below)
gen truetaub = y1 - y0 // True effect
gen treatvar = treatsd * treatsd
gen controlvar = controlsd * controlsd
gen estvartaub = (nb/(nb-1)) * (treatvar/nTb) + (controlvar/nCb)
gen nbwt = nb / total_n // Block size weight
gen pbwt = pb * (1 - pb) // pb is proportion treated
gen hbwt2 = nbwt * pbwt // Precision weights
gen hbwt3 = pbwt * nb
gen hbwt = (2*(nCb * nTb)/(nTb + nCb))
qui sum hbwt3
gen greenlabrule = 20 * hbwt3 / r(sum)

** All of these expressions of the harmonic mean weight are the same
qui sum hbwt
gen double hbwt01 = round(hbwt / r(sum), 0.000001) // Precision issues
qui sum hbwt2
gen double hbwt02 = round(hbwt2 / r(sum), 0.000001)
qui sum hbwt3
gen double hbwt03 = round(hbwt3 / r(sum), 0.000001)
assert hbwt01 == hbwt02
assert hbwt01 == hbwt03

** What is the "true" ATE?
gen truetaubweighted = truetaub * nbwt
qui sum truetaubweighted
global trueATE2 = round(r(sum), 0.01)
assert $trueATE1 == $trueATE2 // After rounding
** We could define the following as an estimand, too.
* gen truetaubweighted2 = truetaub * hbwt01
* qui sum truetaubweighted2
* global trueATE3 = round(r(sum), 0.01)
```
::: 
::: {#ch5Hide23 .tabcontent}
::: 
:::

```{r setup_dd_blockrand, echo = F, eval = T}
## Using the Declare Design Machinery to ensure that 
## the data here and the simulations below match

## Declare the population
thepop <- declare_population(dat)

## Represent the potential outcomes with a function
po_function <- function(data){
	data$Y_Z_0 <- data$y0
	data$Y_Z_1 <- data$y1
	data
}

## Use this function to declare the potential outcomes
theys <- declare_potential_outcomes(handler = po_function)

## Declare the desired inquiry
theestimand <- declare_inquiry(ATE = mean(Y_Z_1 - Y_Z_0))

## Declare treatment assignment
numtreated <- sort(unique(dat$nb))/rep(c(2,10),B/2)
theassign <- declare_assignment(Z = block_ra(blocks = bF, block_m = numtreated))

## Declare revealed data
theobsident <- declare_reveal(Y, Z)

## Combine these design elements together
thedesign <- thepop + theys + theestimand + theassign + theobsident

# Draw data matching this design
set.seed(2201)
dat2 <- draw_data(thedesign)

## Now add individual-level weights to the data
## (under complete_ra within blocks, these will
## be the same across re-randomizations; OK to
## not recalculate within the simulation).
dat2 <- dat2 %>% 
  group_by(b) %>% 
  mutate(
    nb = n(), # Size of block
    pib = mean(Z), # Prob. of treatment assignment
    nTb = sum(Z), # Number treated
    nCb = nb - nTb, # Number control
    nbwt = (Z/pib) + ((1-Z)/(1-pib)), # Unbiased regression weight
    hbwt = nbwt * (pib * (1 - pib)) # Precision regression weight
    )

## Declare a new population, and generate another design
thepop2 <- declare_population(dat2)
thedesign2 <- thepop2 + theys + theestimand + theassign + theobsident

## Create the block level dataset, with block level weights.
datB <- group_by(dat2,b) %>%
  summarize(
    taub = mean(Y[Z==1]) - mean(Y[Z==0]), # Effect (variance below)
    truetaub = mean(y1) - mean(y0), # True effect
    nb = n(), # Number in block
    nTb = sum(Z), # Number treated
    nCb = nb - nTb, # Number control
    estvartaub = (nb/(nb-1)) * (var(Y[Z==1])/nTb) + (var(Y[Z==0])/nCb),
    pb = mean(Z), # Proportion treated
    nbwt = unique(nb/nrow(dat2)), # Block-size weight
    pbwt = pb * ( 1 - pb),
    hbwt2 = nbwt * pbwt, # Precision weights
    hbwt3 = pbwt * nb,
    hbwt = (2*(nCb * nTb)/(nTb + nCb))
    )
datB$greenlabrule <- 20*datB$hbwt3/sum(datB$hbwt3)

## All of these expressions of the harmonic mean weight are the same
datB$hbwt01 <- datB$hbwt/sum(datB$hbwt)
datB$hbwt02 <- datB$hbwt2/sum(datB$hbwt2)
datB$hbwt03 <- datB$hbwt3/sum(datB$hbwt3)
stopifnot(all.equal(datB$hbwt01, datB$hbwt02))
stopifnot(all.equal(datB$hbwt01, datB$hbwt03))

## What is the "true" ATE?
trueATE1 <- with(dat2, mean(y1) - mean(y0))
trueATE2 <- with(datB, sum(truetaub*nbwt))
stopifnot(all.equal(trueATE1, trueATE2))
## We could define the following as an estimand, too.
## trueATE3 <- with(datB, sum(truetaub*hbwt01))
## c(trueATE1,trueATE2,trueATE3)

## We can get the same answer using R's weighted.mean command
trueATE2b <- weighted.mean(datB$truetaub, w=datB$nbwt)
stopifnot(all.equal(trueATE2b, trueATE2))
```

```{r, echo = F, eval = T, include = F}
## Again, exporting for use in Stata example code.
write.csv(dat2, "blocksimdat2.csv", row.names = F)
write.csv(datB, "datB.csv", row.names = F)
```

We can now review the design:

```{r blockdesign, echo = F}
with(dat2, table(treatment=Z,blocknumber=b))
```

With those simulations prepared, we'll start by comparing multiple approaches to getting an unbiased block-size-weighted average treatment effect estimate, $\bar{\tau}_{\text{nbwt}}$. Notice that we do not use linear, additive fixed effects adjustment in any of these.^[I.e., we are not employing the least squares dummy variable, or LSDV, method of including fixed effects in a linear regression. For our purposes here, including fixed effects in the regression via a *within-estimator* instead would yield the same results [@cameron2005microeconometrics].] Two of these approaches do include fixed effects dummies, but they are mean-centered and interacted with treatment assignment --- in other words, we include the fixed effects via @lin_agnostic_2013 adjustment.^[See also @miratrix2013adjusting]

1. `simple_block` refers to calculating mean differences within blocks and then taking the block-size weighted average of them
2. `diffmeans` uses the `difference_in_means` function from the `estimatr`
package [@R-estimatr]
3. `lmlin` applies Lin covariance adjustment *via* block indicators, using the `lm_lin` function
4. `lmlinbyhand` verifies that function using matrix operations; see [this entry](https://alexandercoppock.com/Green-Lab-SOP/Green_Lab_SOP.html#taking-block-randomization-into-account-in-ses-and-cis) from the Green lab's SOP.
5. `regwts` uses the basic OLS function from R `lm` with appropriate weights.

Let's compare the estimates themselves using a single dataset before running our simulation:

::: {.tab} 
<button class="tablinks" onclick="unrolltab(event, 'ch5R24')">R code</button>
<button class="tablinks" onclick="unrolltab(event, 'ch5Stata24')">Stata code</button>
<button class="tablinks" onclick="unrolltab(event, 'ch5Hide24')">Hide</button>
::: {#ch5R24 .tabcontent} 
<br />
```{r, highlight = F, eval = F}
## simple_block
ate_nbwt1 <- with(datB,sum(taub*nbwt))

## diffmeans
ate_nbwt2 <- difference_in_means(Y~Z,blocks=b,data=dat2)

## lmlin
ate_nbwt3 <- lm_lin(Y~Z,covariates=~bF,data=dat2)

## regwts
ate_nbwt5 <- lm_robust(Y~Z,data=dat2,weights=nbwt)
ate_nbwt5a <- lm(Y~Z,data=dat2,weights=nbwt)
ate_nbwt5ase <- coeftest(ate_nbwt5a,vcov=vcovHC(ate_nbwt5a,type="HC2"))

## lmlinbyhand
X <- model.matrix(~bF-1,data=dat2)
barX <- colMeans(X)
Xmd <- sweep(X,2,barX)
stopifnot(all.equal((X[,3]-mean(X[,3])), Xmd[,3]))
ZXmd <- sweep(Xmd,1,dat2$Z,FUN="*")
stopifnot(all.equal(dat2$Z*Xmd[,3],ZXmd[,3]))
bigX <- cbind(Intercept=1,Z=dat2$Z,Xmd[,-1],ZXmd[,-1])
bigXdf <- data.frame(bigX,Y=dat2$Y)
ate_nbwt4 <-lm(Y~.-1,data=bigXdf)
ate_nbwt4se <- coeftest(ate_nbwt4,vcov.=vcovHC(ate_nbwt4,type="HC2"))

## List all
nbwtates<-c(
  simple_block=ate_nbwt1,
  diffmeans=ate_nbwt2$coefficients[["Z"]],
  lmlin=ate_nbwt3$coefficients[["Z"]],
  lmlinbyhand=ate_nbwt4$coefficients[["Z"]],
  regwts=ate_nbwt5$coefficients[["Z"]]
  )

nbwtates
```
:::
::: {#ch5Stata24 .tabcontent} 
<br />
```{stata, highlight = F, eval = F}
** Load block-level data from the R code
import delimited using "datB.csv", clear

** simple_block
gen taubweighted = taub * nbwt
qui sum taubweighted
global ate_nbwt1 = r(sum)
tempvar calc_se
gen `calc_se' = nbwt^2 * estvartaub
qui sum `calc_se'
global ate_nbwt1se = sqrt(r(sum))

** design-based diffmeans estimator (return to observation level data)
* See the DeclareDesign package mathematical notes.
import delimited using "blocksimdat2.csv", clear // dat2 in the R code
rename (y z ntb ncb) (ynew znew nTb nCb) // Adjust names to match illustrative code above
egen blockmean = mean(ynew), by(bf znew)
egen blocksd = sd(ynew), by(bf znew)
bysort bf (znew): gen diffmean = blockmean[_N] - blockmean[1] // with sorting, treat - control
bysort bf (znew): gen variance = ((blocksd[_N]^2)/nTb[_N]) + ((blocksd[1]^2)/nCb[1])
qui sum diffmean, meanonly
global ate_nbwt2 = r(mean)
qui count
gen forblockedse = (nb/r(N))^2 * variance
bysort bf: replace forblockedse = . if _n != 1
qui sum forblockedse
global ate_nbwt2se = sqrt(r(sum))

** lmlinbyhand
tabulate bf, gen(block_)
drop block_1
qui reg ynew znew i.bf
gen samp = e(sample) // Ensure correct sample used
foreach var of varlist block_* {
	qui sum `var' if samp == 1, meanonly
	gen mc_`var' = `var' - `r(mean)'
}
qui reg ynew i.znew##c.(mc_block_*), vce(hc2)
global ate_nbwt3 = _b[1.znew]
global ate_nbwt3se = _se[1.znew]

** regwts
qui reg ynew znew [aw = nbwt], vce(hc2)
global ate_nbwt5 = _b[znew]
global ate_nbwt5se = _se[znew]

** List all
di "simple_block = $ate_nbwt1"
di "diffmeans = $ate_nbwt2"
di "lmlinbyhand = $ate_nbwt3"
di "regwts = $ate_nbwt5"
```
::: 
::: {#ch5Hide24 .tabcontent}
::: 
:::

```{r blockrandests, eval = T, echo=F}
## simple_block
ate_nbwt1 <- with(datB,sum(taub*nbwt))

## diffmeans
ate_nbwt2 <- difference_in_means(Y~Z,blocks=b,data=dat2)

## lmlin
ate_nbwt3 <- lm_lin(Y~Z,covariates=~bF,data=dat2)

## regwts
ate_nbwt5 <- lm_robust(Y~Z,data=dat2,weights=nbwt)
ate_nbwt5a <- lm(Y~Z,data=dat2,weights=nbwt)
ate_nbwt5ase <- coeftest(ate_nbwt5a,vcov=vcovHC(ate_nbwt5a,type="HC2"))

## lmlinbyhand
X <- model.matrix(~bF-1,data=dat2)
barX <- colMeans(X)
Xmd <- sweep(X,2,barX)
stopifnot(all.equal((X[,3]-mean(X[,3])), Xmd[,3]))
ZXmd <- sweep(Xmd,1,dat2$Z,FUN="*")
stopifnot(all.equal(dat2$Z*Xmd[,3],ZXmd[,3]))
bigX <- cbind(Intercept=1,Z=dat2$Z,Xmd[,-1],ZXmd[,-1])
bigXdf <- data.frame(bigX,Y=dat2$Y)
ate_nbwt4 <-lm(Y~.-1,data=bigXdf)
ate_nbwt4se <- coeftest(ate_nbwt4,vcov.=vcovHC(ate_nbwt4,type="HC2"))

## List all
nbwtates<-c(
  simple_block=ate_nbwt1,
  diffmeans=ate_nbwt2$coefficients[["Z"]],
  lmlin=ate_nbwt3$coefficients[["Z"]],
  lmlinbyhand=ate_nbwt4$coefficients[["Z"]],
  regwts=ate_nbwt5$coefficients[["Z"]]
  )

nbwtates
```

Let's also quickly review their standard errors:

::: {.tab} 
<button class="tablinks" onclick="unrolltab(event, 'ch5R25')">R code</button>
<button class="tablinks" onclick="unrolltab(event, 'ch5Stata25')">Stata code</button>
<button class="tablinks" onclick="unrolltab(event, 'ch5Hide25')">Hide</button>
::: {#ch5R25 .tabcontent} 
<br />
```{r, highlight = F, eval = F}
## Comparing the Standard Errors
ate_nbwt1se <- sqrt(sum(datB$nbwt^2 * datB$estvartaub))
nbwtses <- c(
  simple_block=ate_nbwt1se,
  diffmeans=ate_nbwt2$std.error,
	lmlin=ate_nbwt3$std.error[["Z"]],
	lmlinbyhand=ate_nbwt4se["Z","Std. Error"],
	regwts=ate_nbwt5$std.error[["Z"]]
  )

nbwtses
```
:::
::: {#ch5Stata25 .tabcontent} 
<br />
```{stata, highlight = F, eval = F}
** Comparing the Standard Errors
di "simple_block = $ate_nbwt1se"
di "diffmeans = $ate_nbwt2se"
di "lmlinbyhand = $ate_nbwt3se"
di "regwts = $ate_nbwt5se"
```
::: 
::: {#ch5Hide25 .tabcontent}
::: 
:::

```{r blockrandSEests,echo=F, eval = T}
## Comparing the Standard Errors
ate_nbwt1se <- sqrt(sum(datB$nbwt^2 * datB$estvartaub))
nbwtses <- c(
  simple_block=ate_nbwt1se,
  diffmeans=ate_nbwt2$std.error,
	lmlin=ate_nbwt3$std.error[["Z"]],
	lmlinbyhand=ate_nbwt4se["Z","Std. Error"],
	regwts=ate_nbwt5$std.error[["Z"]]
  )

nbwtses
```

As noted above, weighting by block size allows us to define the average treatment effect in a way that treats each unit equally. And we have just illustrated six different ways to estimate this effect. If we want to calculate standard errors for these estimators (e.g., to produce confidence intervals), we will, in general, be leaving statistical power on the table in exchange for an easier to interpret estimate, and an estimator that relates to its underlying target in an unbiased manner.

Next, we show an approach to blocking adjustment that is optimal from the perspective of statistical power, or narrow confidence intervals. As discussed above, we call this the "precision-weighted" average treatment effect.^[See Section 5 of @hansen_covariate_2008 for proof that this kind of weighting is optimal from the perspective of precision.] In some literatures, it's typical to use OLS regression machinery to calculate an ATE that is weighted to account for blocking --- i.e., a "Least Squared Dummy Variables" (LSDV) approach to including block fixed effects in a regression. We show here that this is simply another version of the precision-weighted ATE estimator.

1. `simple_block` calculates a simple differences of means within blocks and
then takes a weighted average of those differences, using precision
weights
2. `lm_fixed_effects1` uses `lm_robust` with binary indicators for blocks
3. `lm_fixed_effects2` uses `lm_robust` with the `fixed_effects` option including a factor variable recording block membership
4. `direct_wts` uses `lm_robust` without block-indicators but with precision weights
4. `demeaned` regresses a block-centered version of the outcome on a block-centered version of the treatment indicator (i.e., including block fixed effects through a *within estimator*).

Let's compare the estimates themselves using a single dataset first:

::: {.tab} 
<button class="tablinks" onclick="unrolltab(event, 'ch5R26')">R code</button>
<button class="tablinks" onclick="unrolltab(event, 'ch5Stata26')">Stata code</button>
<button class="tablinks" onclick="unrolltab(event, 'ch5Hide26')">Hide</button>
::: {#ch5R26 .tabcontent} 
<br />
```{r, highlight = F, eval = F}
## simple_block
ate_hbwt1 <- with(datB, sum(taub*hbwt01))

## lm_fixed_effects1
ate_hbwt2 <- lm_robust(Y~Z+bF,data=dat2)

## lm_fixed_effects2
ate_hbwt3 <- lm_robust(Y~Z,fixed_effects=~bF,data=dat2)

## direct_wts
ate_hbwt4 <- lm_robust(Y~Z,data=dat2,weights=hbwt)

## demeaned
ate_hbwt5 <- lm_robust(I(Y-ave(Y,b))~I(Z-ave(Z,b)),data=dat2)

## List all
hbwtates <- c(
  simple_block=ate_hbwt1,
	lm_fixed_effects1=ate_hbwt2$coefficients[["Z"]],
	lm_fixed_effects2=ate_hbwt3$coefficients[["Z"]],
	direct_wts=ate_hbwt4$coefficients[["Z"]],
	demeaned=ate_hbwt5$coefficient[[2]]
  )

hbwtates
```
:::
::: {#ch5Stata26 .tabcontent} 
<br />
```{stata, highlight = F, eval = F}
** simple_block (return to block level data)
import delimited using "datB.csv", clear
capture drop taubweighted_prec
gen taubweighted_prec = taub * hbwt01
qui sum taubweighted_prec
global ate_hbwt1 = r(sum)
capture drop __0*
tempvar calc_se2
gen `calc_se2' = hbwt01^2 * estvartaub
qui sum `calc_se2'
global ate_hbwt1se = sqrt(r(sum))

** lm_fixed_effects1 (return to observation level data)
import delimited using "blocksimdat2.csv", clear // dat2 in the R code
rename (y z ntb ncb) (ynew znew nTb nCb) // Adjust names to match illustrative code above
qui reg ynew znew i.bf, vce(hc2)
global ate_hbwt2 = _b[znew]
global ate_hbwt2se = _se[znew]

** lm_fixed_effects2 (SEs differ from R variant of this)
qui areg ynew znew, absorb(bf) vce(hc2)
global ate_hbwt3 = _b[znew]
global ate_hbwt3se = _se[znew]

** direct_wts
qui reg ynew znew [aw = hbwt], vce(hc2)
global ate_hbwt4 = _b[znew]
global ate_hbwt4se = _se[znew]

** demeaned
bysort bf: egen meanz = mean(znew)
bysort bf: egen meany = mean(ynew)
gen demeany = ynew - meany
gen demeanz = znew - meanz
qui reg demeany demeanz, vce(hc2)
global ate_hbwt5 = _b[demeanz]
global ate_hbwt5se = _se[demeanz]

** List all
di "simple_block = $ate_hbwt1"
di "lm_fixed_effects1 = $ate_hbwt2"
di "lm_fixed_effects2 = $ate_hbwt3"
di "direct_wts = $ate_hbwt4"
di "demeaned = $ate_hbwt5"
```
::: 
::: {#ch5Hide26 .tabcontent}
::: 
:::

```{r, eval = T, echo = F}
## simple_block
ate_hbwt1 <- with(datB, sum(taub*hbwt01))

## lm_fixed_effects1
ate_hbwt2 <- lm_robust(Y~Z+bF,data=dat2)

## lm_fixed_effects2
ate_hbwt3 <- lm_robust(Y~Z,fixed_effects=~bF,data=dat2)

## direct_wts
ate_hbwt4 <- lm_robust(Y~Z,data=dat2,weights=hbwt)

## demeaned
ate_hbwt5 <- lm_robust(I(Y-ave(Y,b))~I(Z-ave(Z,b)),data=dat2)

## List all
hbwtates <- c(
  simple_block=ate_hbwt1,
	lm_fixed_effects1=ate_hbwt2$coefficients[["Z"]],
	lm_fixed_effects2=ate_hbwt3$coefficients[["Z"]],
	direct_wts=ate_hbwt4$coefficients[["Z"]],
	demeaned=ate_hbwt5$coefficient[[2]]
  )

hbwtates
```

Let's also quickly review their standard errors:

::: {.tab} 
<button class="tablinks" onclick="unrolltab(event, 'ch5R27')">R code</button>
<button class="tablinks" onclick="unrolltab(event, 'ch5Stata27')">Stata code</button>
<button class="tablinks" onclick="unrolltab(event, 'ch5Hide27')">Hide</button>
::: {#ch5R27 .tabcontent} 
<br />
```{r, highlight = F, eval = F}
## Comparing the Standard Errors
ate_hbwt1se <- sqrt(sum(datB$hbwt01^2 * datB$estvartaub))
hbwtses <- c(
  simple_block=ate_hbwt1se,
  lm_fixed_effects1=ate_hbwt2$std.error[["Z"]],
	lm_fixed_effects2=ate_hbwt3$std.error[["Z"]],
	direct_wts=ate_hbwt4$std.error[["Z"]],
	demeaned=ate_hbwt5$std.error[[2]]
  )

hbwtses
```
:::
::: {#ch5Stata27 .tabcontent} 
<br />
```{stata, highlight = F, eval = F}
** Comparing the Standard Errors
di "simple_block = $ate_hbwt1se"
di "lm_fixed_effects1 = $ate_hbwt2se"
di "lm_fixed_effects2 = $ate_hbwt3se"
di "direct_wts = $ate_hbwt4se"
di "demeaned = $ate_hbwt5se"
```
::: 
::: {#ch5Hide27 .tabcontent}
::: 
:::

```{r, echo=F, eval=T}
## Comparing the Standard Errors
ate_hbwt1se <- sqrt(sum(datB$hbwt01^2 * datB$estvartaub))
hbwtses <- c(
  simple_block=ate_hbwt1se,
  lm_fixed_effects1=ate_hbwt2$std.error[["Z"]],
	lm_fixed_effects2=ate_hbwt3$std.error[["Z"]],
	direct_wts=ate_hbwt4$std.error[["Z"]],
	demeaned=ate_hbwt5$std.error[[2]]
  )

hbwtses
```

Now, we claimed that the block size weighted estimator is unbiased but perhaps less precise than the precision-weighted estimator. In R, we'll use `DeclareDesign` to compare the performance of those estimators and illustrate our point. We implement those estimators as functions passed to the `diagnose_design` function in the next code block. In Stata, we use the same approach as above, passing manually-written programs to `simulate`.

::: {.tab} 
<button class="tablinks" onclick="unrolltab(event, 'ch5R28')">R code</button>
<button class="tablinks" onclick="unrolltab(event, 'ch5Stata28')">Stata code</button>
<button class="tablinks" onclick="unrolltab(event, 'ch5Hide28')">Hide</button>
::: {#ch5R28 .tabcontent} 
<br />
```{r, highlight = F, eval = F}
## Define estimator functions employed in the simulation below

# Two options that don't account for blocking
estnowtHC2 <- declare_estimator(Y~Z, inquiry=theestimand, .method=lm_robust, label="E1: Ignores Blocks, Design (HC2) SE")
estnowtIID <- declare_estimator(Y~Z, inquiry=theestimand, .method=lm, label="E0: Ignores Blocks, OLS SE")

# Two options applying block-size weights
estnbwt1 <- declare_estimator(Y~Z, inquiry=theestimand, .method=difference_in_means, blocks=b, label="E2: Diff Means Block Size Weights, Design SE")
estnbwt2 <- declare_estimator(Y~Z, inquiry=theestimand, .method=lm_lin, covariates=~bF, label="E3: Treatment Interaction with Block Indicators, Design SE")

# Another block-size-weighted option (via regression weights)
nbwt_est_fun <- function(data){
	data$newnbwt <- with(data, (Z/pib) + ((1-Z)/(1-pib)))
	obj <- lm_robust(Y~Z,data=data, weights=newnbwt)
	res <- tidy(obj) %>% filter(term=="Z")
	return(res)
}
estnbwt3 <- declare_estimator(handler=label_estimator(nbwt_est_fun), inquiry=theestimand, label="E4: Least Squares with Block Size Weights, Design SE")

# Two precision-weighted options
esthbwt1 <- declare_estimator(Y~Z+bF, inquiry=theestimand, .method=lm_robust, label="E5: Precision Weights via Fixed Effects, Design SE")
esthbwt2 <- declare_estimator(Y~Z, inquiry=theestimand, .method=lm_robust, fixed_effects=~bF, label="E6: Precision Weights via Demeaning, Design SE")

# Another precision-weighted option (regression weights)
hbwt_est_fun <- function(data){
	data$newnbwt <- with(data, (Z/pib) + ((1-Z)/(1-pib)))
	data$newhbwt <- with(data, newnbwt * (pib * (1 - pib)))
	obj <- lm_robust(Y~Z,data=data, weights=newhbwt)
	res <- tidy(obj) %>% filter(term=="Z")
	return(res)
}
esthbwt3 <- declare_estimator(handler=label_estimator(hbwt_est_fun), inquiry=theestimand, label="E7: Direct Precision Weights, Design SE")

# A final precision-weighted option (demeaning)
direct_demean_fun <- function(data){
	data$Y <- with(data, Y - ave(Y,b))
	data$Z <- with(data, Z - ave(Z,b))
	obj <- lm_robust(Y~Z, data=data)
	data.frame(term = "Z" ,
		   estimate = obj$coefficients[[2]],
		   std.error = obj$std.error[[2]],
		   statistic = obj$statistic[[2]],
		   p.value=obj$p.value[[2]],
		   conf.low=obj$conf.low[[2]],
		   conf.high=obj$conf.high[[2]],
		   df=obj$df[[2]],
		   outcome="Y")
}
esthbwt4 <- declare_estimator(handler=label_estimator(direct_demean_fun), inquiry=theestimand, label="E8: Direct Demeaning, Design SE")

## Vector of all the estimator function names just created
## (via regular expression)
theestimators <- ls(patt="^est.*?wt")
theestimators

## Get results for each estimator
checkest <- sapply(
  theestimators,
  function(x){
    get(x)(as.data.frame(dat2))[c("estimate","std.error")]
    }
  )

## Combine all of these design objects
thedesignPlusEstimators <- thedesign2 +
	estnowtHC2 + estnowtIID + estnbwt1 + estnbwt2 + estnbwt3 + 
	esthbwt1 + esthbwt2 + esthbwt3 + esthbwt4

## Update the default diagnosands for this simulation
diagnosands <- declare_diagnosands(
   mean_estimand = mean(estimand),
   mean_estimate = mean(estimate),
   bias = mean(estimate - estimand),
   sd_estimate = sqrt(pop.var(estimate)),
   mean_se = mean(std.error),
   rmse = sqrt(mean((estimate - estimand) ^ 2)),
   power = mean(p.value <= 0.05),
   coverage = mean(estimand <= conf.high & estimand >= conf.low)
   )

## Perform the simulation
sims <- 200
set.seed(12345)
thediagnosis <- diagnose_design(thedesignPlusEstimators, sims=sims, bootstrap_sims = 0, diagnosands = diagnosands)
```
:::
::: {#ch5Stata28 .tabcontent} 
<br />
```{stata, highlight = F, eval = F}
** Define a program to apply various estimation strategies
** to a dataset drawn via sample_from.
capture program drop apply_estimators
program define apply_estimators, rclass
		
	** Call the data generation program defined above
	sample_from
	return scalar ATE = r(ATE)
	
	** E0: Ignores Blocks, OLS SE
	qui reg ynew znew
	return scalar estnowtIID_est = _b[znew]
	return scalar estnowtIID_se = _se[znew]
	return scalar estnowtIID_p = r(table)["pvalue", "znew"]
	
	** E1: Ignores Blocks, Design (HC2) SE
	qui reg ynew znew, vce(hc2)
	return scalar estnowtHC2_est = _b[znew]
	return scalar estnowtHC2_se = _se[znew]
	return scalar estnowtHC2_p = r(table)["pvalue", "znew"]
	
	** E2: Design-based difference in means, Design SE
	egen blockmean = mean(ynew), by(bf znew)
	egen blocksd = sd(ynew), by(bf znew)
	bysort bf (znew): gen diffmean = blockmean[_N] - blockmean[1] // with sorting, treat - control
	bysort bf (znew): gen variance = ((blocksd[_N]^2)/nTb[_N]) + ((blocksd[1]^2)/nTb[1])
	qui sum diffmean, meanonly
	local est = r(mean)
	return scalar estnbwt1_est = `est'
	qui count
	local N = r(N)
	gen forblockedse = (nb/`N')^2 * variance
	bysort bf: replace forblockedse = . if _n != 1
	qui sum forblockedse
	local se = sqrt(r(sum))
	return scalar estnbwt1_se = `se'
	qui levelsof bf, local(b)
	local J: word count `b'
	local df = `N' - (2*`J')
	return scalar estnbwt1_p = (2 * ttail(`df', abs(`est'/`se')))
	
	** E3: Treatment Interaction with Block Indicators, Design SE
	qui tabulate bf, gen(block_)
	drop block_1
	qui reg ynew znew i.bf
	gen samp = e(sample)
	foreach var of varlist block_* {
		qui sum `var' if samp == 1, meanonly
		gen mc_`var' = `var' - r(mean)
	}
	qui reg ynew i.znew##c.(mc_block_*), vce(hc2)
	return scalar estnbwt2_est = _b[1.znew]
	return scalar estnbwt2_se = _se[1.znew]
	return scalar estnbwt2_p = r(table)["pvalue", "1.znew"]
	
	** E4: Least Squares with Block Size Weights, Design SE
	qui reg ynew znew [aw = nbwt], vce(hc2)
	return scalar estnbwt4_est = _b[znew]
	return scalar estnbwt4_se = _se[znew]
	return scalar estnbwt4_p = r(table)["pvalue", "znew"]

	** E5: Precision Weights via Fixed Effects, Design SE
	qui reg ynew znew i.bf, vce(hc2)
	return scalar esthbwt1_est = _b[znew]
	return scalar esthbwt1_se = _se[znew]
	return scalar esthbwt1_p = r(table)["pvalue", "znew"]
	
	** E6: Precision Weights via Demeaning, Design SE
	qui areg ynew znew, absorb(bf) vce(hc2)
	return scalar esthbwt2_est = _b[znew]
	return scalar esthbwt2_se = _se[znew]
	return scalar esthbwt2_p = r(table)["pvalue", "znew"]
	
	** E7: Direct Precision Weights, Design SE
	qui reg ynew znew [aw = hbwt], vce(hc2)
	return scalar esthbwt3_est = _b[znew]
	return scalar esthbwt3_se = _se[znew]
	return scalar esthbwt3_p = r(table)["pvalue", "znew"]
	
	** E8: Direct Demeaning, Design SE
	bysort bf: egen meanz = mean(znew)
	bysort bf: egen meany = mean(ynew)
	gen demeany = ynew - meany
	gen demeanz = znew - meanz
	qui reg demeany demeanz, vce(hc2)
	return scalar esthbwt4_est = _b[demeanz]
	return scalar esthbwt4_se = _se[demeanz]
	return scalar esthbwt4_p = r(table)["pvalue", "demeanz"]
	
end

** Perform the simulation
apply_estimators
local rscalars : r(scalars)
local to_store ""
foreach item of local rscalars {
  local to_store "`to_store' `item' = r(`item')"
}
simulate ///
`to_store', ///
reps(200): ///
apply_estimators

** Create summary matrix
qui des, short
di (`r(k)' - 1)/3
matrix diagnosands = J((`r(k)' - 1)/3, 6, .)
matrix rownames diagnosands = "E0: Ignores Blocks, OLS SE" "E1: Ignores Blocks, Design (HC2) SE" "E2: Diff Means BW, Design SE" "E3: Lin BW, Design SE" "E4: Direct BW, Design SE" "E5: FE PW, Design SE" "E6: Demean PW, Design SE" "E7: Direct PW, Design SE" "E8: Direct Demeaning, Design SE"
matrix colnames diagnosands = "Mean estimand" "Mean estimate" "Bias" "SD Estimate" "MeanSE" "Coverage"

** Get variable name roots to loop through
local roots estnowtIID estnowtHC2 estnbwt1 estnbwt2 estnbwt4 esthbwt1 esthbwt2 esthbwt3 esthbwt4
 
** Calculate quantities to include
local row = 0
foreach l of local roots {
	
	local ++row
	
	* Estimand
	qui sum ATE, meanonly
	matrix diagnosands[`row', 1] = `r(mean)'
	
	* Estimate
	qui sum `l'_est, meanonly
	matrix diagnosands[`row', 2] = `r(mean)'
	
	* Bias
	qui gen biascalc = `l'_est - ATE
	qui sum biascalc, meanonly
	matrix diagnosands[`row', 3] = r(mean)
	drop biascalc
	
	* SD estimate (based on population variance, no n-1 in the variance denom.)
	qui sum `l'_est
	qui gen sdcalc = (`l'_est - r(mean))^2
	qui sum sdcalc
	matrix diagnosands[`row', 4] = sqrt(r(sum)/r(N))
	drop sdcalc
	
	* Mean SE
	qui sum `l'_se
	matrix diagnosands[`row', 5] = `r(mean)'
	
	* Coverage (z-stat CIs to limit estimates stored above)
	qui gen conflow = `l'_est - (1.96 * `l'_se)
	qui gen confhigh = `l'_est + (1.96 * `l'_se)
	qui gen inrange = ATE <= confhigh & ATE >= conflow
	qui sum inrange
	matrix diagnosands[`row', 6] = r(mean)
	drop conf* inrange
	
}

* View the results
matrix list diagnosands
```
::: 
::: {#ch5Hide28 .tabcontent}
::: 
:::

```{r defineestimators1, results="hide", cache = T, echo = F, eval = T}
## Define estimator functions employed in the simulation below

# Two options that don't account for blocking
estnowtHC2 <- declare_estimator(Y~Z, inquiry=theestimand, .method=lm_robust, label="E1: Ignores Blocks, Design (HC2) SE")
estnowtIID <- declare_estimator(Y~Z, inquiry=theestimand, .method=lm, label="E0: Ignores Blocks, OLS SE")

# Two options applying block-size weights
estnbwt1 <- declare_estimator(Y~Z, inquiry=theestimand, .method=difference_in_means, blocks=b, label="E2: Diff Means Block Size Weights, Design SE")
estnbwt2 <- declare_estimator(Y~Z, inquiry=theestimand, .method=lm_lin, covariates=~bF, label="E3: Treatment Interaction with Block Indicators, Design SE")

# Another block-size-weighted option (via regression weights)
nbwt_est_fun <- function(data){
	data$newnbwt <- with(data, (Z/pib) + ((1-Z)/(1-pib)))
	obj <- lm_robust(Y~Z,data=data, weights=newnbwt)
	res <- tidy(obj) %>% filter(term=="Z")
	return(res)
}
estnbwt3 <- declare_estimator(handler=label_estimator(nbwt_est_fun), inquiry=theestimand, label="E4: Least Squares with Block Size Weights, Design SE")

# Two precision-weighted options
esthbwt1 <- declare_estimator(Y~Z+bF, inquiry=theestimand, .method=lm_robust, label="E5: Precision Weights via Fixed Effects, Design SE")
esthbwt2 <- declare_estimator(Y~Z, inquiry=theestimand, .method=lm_robust, fixed_effects=~bF, label="E6: Precision Weights via Demeaning, Design SE")

# Another precision-weighted option (regression weights)
hbwt_est_fun <- function(data){
	data$newnbwt <- with(data, (Z/pib) + ((1-Z)/(1-pib)))
	data$newhbwt <- with(data, newnbwt * (pib * (1 - pib)))
	obj <- lm_robust(Y~Z,data=data, weights=newhbwt)
	res <- tidy(obj) %>% filter(term=="Z")
	return(res)
}
esthbwt3 <- declare_estimator(handler=label_estimator(hbwt_est_fun), inquiry=theestimand, label="E7: Direct Precision Weights, Design SE")

# A final precision-weighted option (demeaning)
direct_demean_fun <- function(data){
	data$Y <- with(data, Y - ave(Y,b))
	data$Z <- with(data, Z - ave(Z,b))
	obj <- lm_robust(Y~Z, data=data)
	data.frame(term = "Z" ,
		   estimate = obj$coefficients[[2]],
		   std.error = obj$std.error[[2]],
		   statistic = obj$statistic[[2]],
		   p.value=obj$p.value[[2]],
		   conf.low=obj$conf.low[[2]],
		   conf.high=obj$conf.high[[2]],
		   df=obj$df[[2]],
		   outcome="Y")
}
esthbwt4 <- declare_estimator(handler=label_estimator(direct_demean_fun), inquiry=theestimand, label="E8: Direct Demeaning, Design SE")

## Vector of all the estimator function names just created
## (via regular expression)
theestimators <- ls(patt="^est.*?wt")
theestimators

## Get results for each estimator
checkest <- sapply(
  theestimators,
  function(x){
    get(x)(as.data.frame(dat2))[c("estimate","std.error")]
    }
  )

## Combine all of these design objects
thedesignPlusEstimators <- thedesign2 +
	estnowtHC2 + estnowtIID + estnbwt1 + estnbwt2 + estnbwt3 + 
	esthbwt1 + esthbwt2 + esthbwt3 + esthbwt4

## Update the default diagnosands for this simulation
diagnosands <- declare_diagnosands(
   mean_estimand = mean(estimand),
   mean_estimate = mean(estimate),
   bias = mean(estimate - estimand),
   sd_estimate = sqrt(pop.var(estimate)),
   mean_se = mean(std.error),
   rmse = sqrt(mean((estimate - estimand) ^ 2)),
   power = mean(p.value <= 0.05),
   coverage = mean(estimand <= conf.high & estimand >= conf.low)
   )

## Perform the simulation
sims <- 200
set.seed(12345)
thediagnosis <- diagnose_design(thedesignPlusEstimators, sims=sims, bootstrap_sims = 0, diagnosands = diagnosands)
```

We can see in the diagnostic output below that the estimators using block-size weights (E2, E3, E4, or E5) all eliminate bias (within simulation error).^[We've omitted power, since all the estimators in this example have a power of approximately 100%] The estimators ignoring blocks (E0 and E1), are biased, and in this particular simulation the precision weighed estimators (E6--E9) are also highly biased --- with some of them also producing poor "coverage" or false positive rates (E7 and E9).

This output also shows us the "SD Estimate" (which is a good estimate of the standard error of the estimate) and the "Mean SE" (which is the average of the analytic estimates of the standard error). In the case of E2, E3, E4 or E5 the Mean SE is larger than the SD Estimate --- this is good in that it means that our analytic standard errors will be conservative. However, we also would prefer that our analytic standard errors not be *too* conservative.

```{r diagnosisblocks, echo = F}
kable(reshape_diagnosis(thediagnosis)[, c(3, 6:10, 13)])
```

```{r setupblockdiaggraph, eval=FALSE, echo = F}
## For a visualization below currently not included in the SOP
simdesigns <- get_simulations(thediagnosis)
## simdesigns <- simulate_design(thedesign,sims=sims)
simmeans <- simdesigns %>%
  group_by(estimator) %>%
  summarize(expest=mean(estimate))
```

```{r graphblockdiag, eval=FALSE, include=F}
## Now compare to better behaved outcomes.
## 
g <- ggplot(data=simdesigns,aes(x=estimate,color=estimator)) +
	geom_density() +
	geom_vline(xintercept=trueATE1) +
	geom_point(data=simmeans,aes(x=expest,y=rep(0,10)),shape=17,size=6) +
	theme_bw() +
	theme(legend.position = c(.9,.8))
print(g)
```

The simulated outcome data we used here is highly skewed by design. This helps illustrate that the biases of precision-weighted estimates could be exacerbated by common problems in administrative data like skewed outcomes or zero-inflation (i.e., many 0s). In this case, the skew even prevents us from illustrating the precision benefits of harmonic weights.

One solution we could apply in this case is rank-transforming the outcome measure. This transformed outcome nearly erases the bias from the block-size weighted estimators, and the precision-weighted approaches also perform as expected. However, ignoring the blocked design is still a problem --- E0 and E1 continue to show substantial bias. Moreover, notice here and above that although using regression weights alone to apply these blocking adjustments (rather than, e.g., an appropriate fixed effects specification) produces the right estimates, it does so at the cost of a larger standard error.

::: {.tab} 
<button class="tablinks" onclick="unrolltab(event, 'ch5R29')">R code</button>
<button class="tablinks" onclick="unrolltab(event, 'ch5Stata29')">Stata code</button>
<button class="tablinks" onclick="unrolltab(event, 'ch5Hide29')">Hide</button>
::: {#ch5R29 .tabcontent} 
<br />
```{r, highlight = F, eval = F}
## Define a function to rank-transform the potential outcomes
po_functionNorm <- function(data){
	data <- data %>%
	  group_by(b) %>%
	  mutate(
	    Y_Z_0=rank(y0),
	    Y_Z_1=rank(y1)
	    )
	return(as.data.frame(data))
}

## Redefine relevant DeclareDesign objects
theysNorm <- declare_potential_outcomes(handler = po_functionNorm)
thedesignNorm <- thepop2 + theysNorm + theestimand + theassign + theobsident
datNorm <- draw_data(thedesignNorm)
thedesignPlusEstimatorsNorm <- thedesignNorm +
  estnowtHC2 + estnowtIID + estnbwt1 +
  estnbwt2 + estnbwt3 + estnbwt4 +
  esthbwt1 + esthbwt2 + esthbwt3 + 
  esthbwt4

## Perform a new simulation
sims <- 200
thediagnosisNorm <- diagnose_design(thedesignPlusEstimatorsNorm, sims = sims, bootstrap_sims = 0, diagnosands = diagnosands)
```
:::
::: {#ch5Stata29 .tabcontent} 
<br />
```{stata, highlight = F, eval = F}
** Perform this simulation again with rank-transformed potential outcomes

** Redefine sampling program to consider ranked potential outcomes
capture program drop sample_from
program define sample_from, rclass
	
	* Load simulated data from R
	import delimited using "blocksimdat.csv", clear
	
	* Get # treated for each block
	levelsof nb, local(nblevels)
	local numtreat
	local i = 0
	foreach l of local nblevels {
		
		local ++i // block indices (1, 2, 3,... 10)
		
		* even block indices (2nd, 4th, etc.) get 10% treated
		if mod(`i',2) == 0 {
			local temp = `l'/10 
			local numtreat `numtreat' `temp'
		}
		
		* odd get 50% treated
		else {
			local temp = `l'/2
			local numtreat `numtreat' `temp'
		}
		
	}
	
	* Re-assign (simulated) treatment in this draw of the data
	capture drop __0*
	block_ra znew, block_var(bf) block_m(`numtreat') replace
	
	* No additional treatment effects, but rank POs
	egen y_znew_0 = rank(y0), unique // Different than how ties are handled in R.
	egen y_znew_1 = rank(y1), unique // R rank command averages by default.
	
	* Get true ATE in r()
	gen true_effect = y_znew_1 - y_znew_0
	sum true_effect, meanonly
	return scalar ATE = r(mean)
	
	* Revealed outcome
	gen ynew = (znew * y_znew_1) + ((1 - znew) * y_znew_0)
	
	* Now add individual-level weights to the data
	* (under complete_ra within blocks, these will
	* be the same across re-randomizations).
	bysort bf: egen pib = mean(znew) // Prob. of treatment assignment
	bysort bf: egen nTb = total(znew) // Number treated
	gen nCb = nb - nTb // Number control
	gen nbwt = (znew/pib) + ((1-znew)/(1-pib)) // Unbiased regression weight
	gen hbwt = nbwt * (pib * (1 - pib)) // Precision regression weight

end

** Estimators program above can then be re-used
apply_estimators
local rscalars : r(scalars)
local to_store ""
foreach item of local rscalars {
  local to_store "`to_store' `item' = r(`item')"
}
simulate ///
`to_store', ///
reps(200): ///
apply_estimators

** Create summary matrix
qui des, short
di (`r(k)' - 1)/3
matrix diagnosands = J((`r(k)' - 1)/3, 6, .)
matrix rownames diagnosands = "E0: Ignores Blocks, OLS SE" "E1: Ignores Blocks, Design (HC2) SE" "E2: Diff Means BW, Design SE" "E3: Lin BW, Design SE" "E5: Direct BW, Design SE" "E6: FE PW, Design SE" "E7: Demean PW, Design SE" "E8: Direct PW, Design SE" "E9: Direct Demeaning, Design SE"
matrix colnames diagnosands = "Mean estimand" "Mean estimate" "Bias" "SD Estimate" "MeanSE" "Coverage"

** Get variable name roots to loop through
local roots estnowtIID estnowtHC2 estnbwt1 estnbwt2 estnbwt4 esthbwt1 esthbwt2 esthbwt3 esthbwt4
 
** Calculate quantities to include
local row = 0
foreach l of local roots {
	
	local ++row
	
	* Estimand
	qui sum ATE, meanonly
	matrix diagnosands[`row', 1] = `r(mean)'
	
	* Estimate
	qui sum `l'_est, meanonly
	matrix diagnosands[`row', 2] = `r(mean)'
	
	* Bias
	qui gen biascalc = `l'_est - ATE
	qui sum biascalc, meanonly
	matrix diagnosands[`row', 3] = r(mean)
	drop biascalc
	
	* SD estimate (based on population variance, no n-1 in the variance denom.)
	qui sum `l'_est
	qui gen sdcalc = (`l'_est - r(mean))^2
	qui sum sdcalc
	matrix diagnosands[`row', 4] = sqrt(r(sum)/r(N))
	drop sdcalc
	
	* Mean SE
	qui sum `l'_se
	matrix diagnosands[`row', 5] = `r(mean)'
	
	* Coverage (z-stat CIs to limit estimates stored above)
	qui gen conflow = `l'_est - (1.96 * `l'_se)
	qui gen confhigh = `l'_est + (1.96 * `l'_se)
	qui gen inrange = ATE <= confhigh & ATE >= conflow
	qui sum inrange
	matrix diagnosands[`row', 6] = r(mean)
	drop conf* inrange
	
}

* View the results
matrix list diagnosands
```
::: 
::: {#ch5Hide29 .tabcontent}
::: 
:::

```{r ranktransblock, cache = T, eval = T, echo = F}
## Define a function to rank-transform the potential outcomes
po_functionNorm <- function(data){
	data <- data %>%
	  group_by(b) %>%
	  mutate(
	    Y_Z_0=rank(y0),
	    Y_Z_1=rank(y1)
	    )
	return(as.data.frame(data))
}

## Redefine relevant DeclareDesign objects
theysNorm <- declare_potential_outcomes(handler = po_functionNorm)
thedesignNorm <- thepop2 + theysNorm + theestimand + theassign + theobsident
datNorm <- draw_data(thedesignNorm)
thedesignPlusEstimatorsNorm <- thedesignNorm +
  estnowtHC2 + estnowtIID + estnbwt1 +
  estnbwt2 + estnbwt3 + estnbwt4 +
  esthbwt1 + esthbwt2 + esthbwt3 + 
  esthbwt4

## Perform a new simulation
sims <- 200
thediagnosisNorm <- diagnose_design(thedesignPlusEstimatorsNorm, sims = sims, bootstrap_sims = 0, diagnosands = diagnosands)
```

```{r, echo = F, eval = T}
## View results
kable(reshape_diagnosis(thediagnosisNorm)[, c(3, 6:10, 13)])
```

```{r setupblockdiaggraph2s, eval=FALSE, echo = F}
## Again, leaving this out of SOP for now
simdesignsNorm <- get_simulations(thediagnosisNorm)
## simdesigns <- simulate_design(thedesign,sims=sims)
simmeansNorm <- simdesignsNorm %>% group_by(estimator) %>% summarize(expest=mean(estimate))
```

```{r plotthediagnosisNorm, eval=FALSE, echo = F}
## Again, leaving this out of SOP for now
g2 <- ggplot(data=simdesignsNorm,aes(x=estimate,color=estimator)) +
	geom_density() +
	geom_vline(xintercept=trueATE1) +
	geom_point(data=simmeansNorm,aes(x=expest,y=rep(0,10)),shape=17,size=6) +
	theme_bw() +
	theme(legend.position = c(.9,.8))
print(g2)
```

In a pair-randomized design, for instance, we know that bias should not arise from ignoring the blocking structure. But we could still improve precision by taking the pairing into account [@bowers2011mem]. This final simulation changes the design to still have unequal sized blocks, but with a uniform probability of treatment assignment in each. Here, only two estimators show appreciable bias (E5 and E8). However, ignoring the blocks leads to overly conservative standard errors.

::: {.tab} 
<button class="tablinks" onclick="unrolltab(event, 'ch5R36')">R code</button>
<button class="tablinks" onclick="unrolltab(event, 'ch5Stata36')">Stata code</button>
<button class="tablinks" onclick="unrolltab(event, 'ch5Hide36')">Hide</button>
::: {#ch5R36 .tabcontent} 
<br />
```{r, highlight = F, eval = F}
## Re-define relevant declare design objects
theassignEqual <- declare_assignment(Z=block_ra(blocks = bF))
thedesignNormEqual <- thepop2 + theysNorm + theestimand + 
  theassignEqual + theobsident
datNormEqual <- draw_data(thedesignNormEqual)
thedesignPlusEstimatorsNormEqual <- thedesignNormEqual +
  estnowtHC2 + estnowtIID + estnbwt1 + 
  estnbwt2 + estnbwt3 + estnbwt4 +
  esthbwt1 + esthbwt2 + esthbwt3 + esthbwt4

## Perform a new simulation
sims <- 200
set.seed(12345)
thediagnosisNormEqual <- diagnose_design(thedesignPlusEstimatorsNormEqual, sims = sims, bootstrap_sims = 0, diagnosands = diagnosands)
```
:::
::: {#ch5Stata36 .tabcontent} 
<br />
```{stata, highlight = F, eval = F}
** Perform this simulation again with equal assignment probabilities.

** Redefine sampling program once more.
capture program drop sample_from
program define sample_from, rclass
	
	* Load simulated data from R
	import delimited using "blocksimdat.csv", clear

	* Re-assign (simulated) treatment in this draw of the data.
	capture drop __0*
	block_ra znew, block_var(bf) replace
	
	* No additional treatment effects, but rank POs
	egen y_znew_0 = rank(y0), unique // Different than how ties are handled in R.
	egen y_znew_1 = rank(y1), unique // R rank command averages by default.
	
	* Get true ATE in r()
	gen true_effect = y_znew_1 - y_znew_0
	sum true_effect, meanonly
	return scalar ATE = r(mean)
	
	* Revealed outcome
	gen ynew = (znew * y_znew_1) + ((1 - znew) * y_znew_0)
	
	* Now add individual-level weights to the data
	* (under complete_ra within blocks, these will
	* be the same across re-randomizations).
	bysort bf: egen pib = mean(znew) // Prob. of treatment assignment
	bysort bf: egen nTb = total(znew) // Number treated
	gen nCb = nb - nTb // Number control
	gen nbwt = (znew/pib) + ((1-znew)/(1-pib)) // Unbiased regression weight
	gen hbwt = nbwt * (pib * (1 - pib)) // Precision regression weight

end

** Estimators program above can then be re-used
apply_estimators
local rscalars : r(scalars)
local to_store ""
foreach item of local rscalars {
  local to_store "`to_store' `item' = r(`item')"
}
simulate ///
`to_store', ///
reps(200): ///
apply_estimators

** Create summary matrix
qui des, short
di (`r(k)' - 1)/3
matrix diagnosands = J((`r(k)' - 1)/3, 6, .)
matrix rownames diagnosands = "E0: Ignores Blocks, OLS SE" "E1: Ignores Blocks, Design (HC2) SE" "E2: Diff Means BW, Design SE" "E3: Lin BW, Design SE" "E5: Direct BW, Design SE" "E6: FE PW, Design SE" "E7: Demean PW, Design SE" "E8: Direct PW, Design SE" "E9: Direct Demeaning, Design SE"
matrix colnames diagnosands = "Mean estimand" "Mean estimate" "Bias" "SD Estimate" "MeanSE" "Coverage"

** Get variable name roots to loop through
local roots estnowtIID estnowtHC2 estnbwt1 estnbwt2 estnbwt4 esthbwt1 esthbwt2 esthbwt3 esthbwt4
 
** Calculate quantities to include
local row = 0
foreach l of local roots {
	
	local ++row
	
	* Estimand
	qui sum ATE, meanonly
	matrix diagnosands[`row', 1] = `r(mean)'
	
	* Estimate
	qui sum `l'_est, meanonly
	matrix diagnosands[`row', 2] = `r(mean)'
	
	* Bias
	qui gen biascalc = `l'_est - ATE
	qui sum biascalc, meanonly
	matrix diagnosands[`row', 3] = r(mean)
	drop biascalc
	
	* SD estimate (based on population variance, no n-1 in the variance denom.)
	qui sum `l'_est
	qui gen sdcalc = (`l'_est - r(mean))^2
	qui sum sdcalc
	matrix diagnosands[`row', 4] = sqrt(r(sum)/r(N))
	drop sdcalc
	
	* Mean SE
	qui sum `l'_se
	matrix diagnosands[`row', 5] = `r(mean)'
	
	* Coverage (z-stat CIs to limit estimates stored above)
	qui gen conflow = `l'_est - (1.96 * `l'_se)
	qui gen confhigh = `l'_est + (1.96 * `l'_se)
	qui gen inrange = ATE <= confhigh & ATE >= conflow
	qui sum inrange
	matrix diagnosands[`row', 6] = r(mean)
	drop conf* inrange
	
}

* View the results
matrix list diagnosands
```
::: 
::: {#ch5Hide36 .tabcontent}
::: 
:::

```{r equalprobblocks, cache = T, eval = T, echo = F}
## Re-define relevant declare design objects
theassignEqual <- declare_assignment(Z=block_ra(blocks = bF))
thedesignNormEqual <- thepop2 + theysNorm + theestimand + 
  theassignEqual + theobsident
datNormEqual <- draw_data(thedesignNormEqual)
thedesignPlusEstimatorsNormEqual <- thedesignNormEqual +
  estnowtHC2 + estnowtIID + estnbwt1 + 
  estnbwt2 + estnbwt3 + estnbwt4 +
  esthbwt1 + esthbwt2 + esthbwt3 + esthbwt4

## Perform a new simulation
sims <- 200
set.seed(12345)
thediagnosisNormEqual <- diagnose_design(thedesignPlusEstimatorsNormEqual, sims = sims, bootstrap_sims = 0, diagnosands = diagnosands)
```

```{r, echo = F, eval = T}
## View results
kable(reshape_diagnosis(thediagnosisNormEqual)[, c(3, 6:10, 13)])
```

```{r setupblockdiaggraph3, eval=FALSE, echo=F}
## Again, leaving this out of SOP for now
simdesignsNormEqual <- get_simulations(thediagnosisNorm)
simmeansNormEqual <- simdesignsNormEqual %>% group_by(estimator) %>% summarize(expest=mean(estimate))
```

```{r graphblockdiag3, eval=FALSE, echo=F}
## Again, leaving this out of SOP for now
## Now compare to better behaved outcomes.
g3 <- ggplot(data=simdesignsNormEqual,aes(x=estimate,color=estimator)) +
	geom_density() +
	geom_vline(xintercept=trueATE1) +
	geom_point(data=simmeansNormEqual,aes(x=expest,y=rep(0,10)),shape=17,size=6) +
	theme_bw() +
	theme(legend.position = c(.9,.8))
print(g3)
```

#### Summary of Approaches to the Analysis of Block Randomized Trials

Our team prefers block randomized trials because of their potential to increase statistical power, as well as their ability to let us focus on subgroup effects. Our approach has been guided by evidence like the simulations we present above: we "analyze as we randomize" to avoid bias and increase statistical power, and we are careful in our choice of weighting approaches. Different designs will require different analytical decisions --- sometimes we may be willing to trade a small amount of bias for a guarantee that our estimates will be closer to the truth and more precise, on average (i.e. get lower mean-squared error, but more bias). Other studies will be so large or small that one or another strategy will become obvious. We use our Analysis Plans to explain our choices, and simulation studies like those shown here when we are uncertain about the applicability of statistical rules of thumb to any given design.

## Cluster-randomized trials {#clusterrandanalysis}

In a cluster randomized trial, treatment is assigned at the level of larger groups of units, called "clusters," instead of at the individual level.^[It's also possible to have trials where treatment is influenced *but not entirely determined* at the cluster level.] These studies tend to distinguish signal from noise less effectively than an experiment where we assign treatment directly to individuals --- i.e., they have less precision. The number of independent pieces of information available to learn about the treatment effect will generally be closer to the number of clusters (each of which tends to be assigned to treatment independently of each other) than the number of dependent observations within a cluster (See EGAP's [10 Things You Need to Know about Cluster Randomization](https://egap.org/resource/10-things-to-know-about-cluster-randomization/)).^[When treatment assignment is *influenced by clustering* but not completely determined at the cluster level, standard error adjustments for clustering will tend to be too conservative. See @abadie2023should for more discussion of this and some potential solutions.]

Since we analyze as we randomize, a cluster randomized experiment may require that we (1) weight cluster-level average treatment effects by cluster size if we are trying to estimate the average of the individual level causal effects [@middleton2015unbiased]. They also require that we (2) change how we calculate standard errors and $p$-values to account for the fact that uncertainty is generated at the level of the cluster and not at the level of the individual [@hansen_covariate_2008; @gerber_field_2012]. For example, imagine that we had 10 clusters (administrative offices, physicians groups, etc.) with half assigned to treatment and half assigned to control.

::: {.tab} 
<button class="tablinks" onclick="unrolltab(event, 'ch5R30')">R code</button>
<button class="tablinks" onclick="unrolltab(event, 'ch5Stata30')">Stata code</button>
<button class="tablinks" onclick="unrolltab(event, 'ch5Hide30')">Hide</button>
::: {#ch5R30 .tabcontent} 
<br />
```{r, highlight = F, eval = F}
## Create a copy of dat2 to use for our clustering examples
dat3 <- dat2
dat3$cluster <- dat3$b
dat3$clusterF <- factor(dat3$cluster)
ndat3 <- nrow(dat3)

## Randomly assign half of the clusters to treatment and half to control
set.seed(12345)
dat3$Zcluster <- cluster_ra(cluster=dat3$cluster)
with(dat3,table(Zcluster,cluster))
```
:::
::: {#ch5Stata30 .tabcontent} 
<br />
```{stata, highlight = F, eval = F}
** Modify dat2 to use for our clustering examples
import delimited using "blocksimdat2.csv", clear // dat2 in the R code
gen cluster = b

** Randomly assign half of the clusters to treatment and half to control
set seed 12345
cluster_ra zcluster, cluster_var(cluster)
table zcluster cluster
```
::: 
::: {#ch5Hide30 .tabcontent}
::: 
:::

```{r makeblocksintoclusters, echo=F, eval = T}
## Create a copy of dat2 to use for our clustering examples
dat3 <- dat2
dat3$cluster <- dat3$b
dat3$clusterF <- factor(dat3$cluster)
ndat3 <- nrow(dat3)

## Randomly assign half of the clusters to treatment and half to control
set.seed(12345)
dat3$Zcluster <- cluster_ra(cluster=dat3$cluster)
with(dat3,table(Zcluster,cluster))
```

```{r, echo = F, eval = F, include = F}
write.csv(dat3, "dat3.csv", row.names = F)
```

Although our example data has `r ndat3` observations, we do not have `r ndat3` pieces of independent information about the effect of the treatment because people were assigned in groups. Rather we have some amount of information in between `r ndat3` and the number of clusters (in this case, 
`r length(unique(dat3$cluster))`). We can see above that the number of
people within each cluster --- and notice that all of the people are coded as
either control or treatment because assignment is at the level of the cluster.

Before continuing, in R, let's set up some `DeclareDesign` elements for a simulation of clustered experimental designs. We'll do some comparable preparation in Stata.

::: {.tab} 
<button class="tablinks" onclick="unrolltab(event, 'ch5R31')">R code</button>
<button class="tablinks" onclick="unrolltab(event, 'ch5Stata31')">Stata code</button>
<button class="tablinks" onclick="unrolltab(event, 'ch5Hide31')">Hide</button>
::: {#ch5R31 .tabcontent} 
<br />
```{r, highlight = F, eval = F}
## Get weights for one estimation strategy used below
library(ICC)
iccres <- ICCest(x=clusterF,y=Y,data=dat3)
dat3$varweight <- 1/(iccres$vara + (iccres$varw/dat3$nb))

## Define various DeclareDesign elements
thepop3 <- declare_population(dat3)
po_functionCluster <- function(data){
	data$Y_Zcluster_0 <- data$y0
	data$Y_Zcluster_1 <- data$y1
	data
}
theysCluster  <- declare_potential_outcomes(handler = po_functionCluster)
theestimandCluster <- declare_inquiry(ATE = mean(Y_Zcluster_1 - Y_Zcluster_0))
theassignCluster <- declare_assignment(Zcluster=cluster_ra(clusters=cluster))
theobsidentCluster <- declare_reveal(Y, Zcluster)
thedesignCluster <- thepop3 + theysCluster + theestimandCluster + 
  theassignCluster + theobsidentCluster
datCluster <- draw_data(thedesignCluster)
```
:::
::: {#ch5Stata31 .tabcontent} 
<br />
```{stata, highlight = F, eval = F}
** Update the sampling program for a basic clustered design
capture program drop sample_from
program define sample_from, rclass
	
	* Load baseline data
	import delimited using "blocksimdat2.csv", clear // dat2 in the R code
	gen cluster = b
	
	* Re-assign (simulated) treatment in this draw of the data
	cluster_ra zcluster, cluster_var(cluster)
	
	* No additional treatment effects
	gen y_zcluster_0 = y0
	gen y_zcluster_1 = y1
	
	* Get true ATE in r()
	gen true_effect = y_zcluster_1 - y_zcluster_0
	sum true_effect, meanonly
	return scalar ATE = r(mean)
	
	* Revealed outcome
	gen ynew = (zcluster * y_zcluster_1) + ((1 - zcluster) * y_zcluster_0)
	
	** Get weights for one estimation strategy used below
	qui loneway ynew cluster
	gen varweight = 1 / ( r(sd_b)^2 + ( r(sd_w)^2 / nb ) ) 

end
```
::: 
::: {#ch5Hide31 .tabcontent}
::: 
:::

```{r setupclusterdesign, results="hide", echo = F, eval = T}
## Get weights for one estimation strategy used below
library(ICC)
iccres <- ICCest(x=clusterF,y=Y,data=dat3)
dat3$varweight <- 1/(iccres$vara + (iccres$varw/dat3$nb))

## Define various DeclareDesign elements
thepop3 <- declare_population(dat3)
po_functionCluster <- function(data){
	data$Y_Zcluster_0 <- data$y0
	data$Y_Zcluster_1 <- data$y1
	data
}
theysCluster  <- declare_potential_outcomes(handler = po_functionCluster)
theestimandCluster <- declare_inquiry(ATE = mean(Y_Zcluster_1 - Y_Zcluster_0))
theassignCluster <- declare_assignment(Zcluster=cluster_ra(clusters=cluster))
theobsidentCluster <- declare_reveal(Y, Zcluster)
thedesignCluster <- thepop3 + theysCluster + theestimandCluster + 
  theassignCluster + theobsidentCluster
datCluster <- draw_data(thedesignCluster)
```

```{r, echo = F, eval = F, include = F}
write.csv(datCluster, "datCluster.csv", row.names = F)
```

In everyday practice, with more than about 50 (equally-sized) clusters, we often produce estimates using more or less the same kinds estimators as those above, but changing our standard error calculations to instead rely on `CR2` cluster-robust standard error [@pustejovsky2019cr]. We show two approaches to such adjustment.

First, the design-based `difference_in_means()` function in R:^[See the `estimatr` [mathematical notes](https://declaredesign.org/r/estimatr/articles/mathematical-notes.html) for more detail on how this is implemented.]

```{r simplecr1}
## CR2 via difference in means
estAndSE4a <- difference_in_means(Y~Zcluster, data=datCluster, clusters=cluster)
estAndSE4a
```

Second, via linear regression with an appropriate error adjustment:

::: {.tab} 
<button class="tablinks" onclick="unrolltab(event, 'ch5R32')">R code</button>
<button class="tablinks" onclick="unrolltab(event, 'ch5Stata32')">Stata code</button>
<button class="tablinks" onclick="unrolltab(event, 'ch5Hide32')">Hide</button>
::: {#ch5R32 .tabcontent} 
<br />
```{r, highlight = F, eval = F}
## CR2 via linear regression
estAndSE4b <- lm_robust(Y~Zcluster, data=datCluster, clusters=cluster, se_type="CR2")
estAndSE4b
```
:::
::: {#ch5Stata32 .tabcontent} 
<br />
```{stata, highlight = F, eval = F}
** Get the treatment assignment used in the R code
import delimited using "datCluster.csv", clear

** Stata's default clustered errors
reg y zcluster, vce(cluster cluster)
local CR1 = _se[zcluster]

** CR2 errors via reg_sandwich
* ssc install reg_sandwich
reg_sandwich y zcluster, cluster(cluster)
local CR2 = _se[zcluster]

** Notice: Stata's default is CR1, not CR2
macro list _CR1 _CR2
assert `CR1' != `CR2'
```
::: 
::: {#ch5Hide32 .tabcontent}
::: 
:::

```{r simplecr2, eval = T, echo = F}
## CR2 via linear regression
estAndSE4b <- lm_robust(Y~Zcluster, data=datCluster, clusters=cluster, se_type="CR2")
estAndSE4b
```

### Bias when cluster size is correlated with potential outcomes

When clusters have unequal sizes, in addition to adjusting our standard error calculations, we might worry about bias as well [@middleton2015unbiased] (see also this [Declare Design blog post](https://declaredesign.org/blog/bias-cluster-randomized-trials.html)). Here, we demonstrate how bias could emerge in the analysis of a cluster randomized trial, as well as how to reduce that bias.

::: {.tab} 
<button class="tablinks" onclick="unrolltab(event, 'ch5R33')">R code</button>
<button class="tablinks" onclick="unrolltab(event, 'ch5Stata33')">Stata code</button>
<button class="tablinks" onclick="unrolltab(event, 'ch5Hide33')">Hide</button>
::: {#ch5R33 .tabcontent} 
<br />
```{r, highlight = F, eval = F}
## Define estimators that can be repeated in the simulation below

## No adjustment for clustering
estC0 <- declare_estimator(Y~Zcluster, inquiry=theestimand, .method=lm, label="C0: Ignores Clusters, IID SE")

## HC2 SEs
estC1 <- declare_estimator(Y~Zcluster, inquiry=theestimand, .method=lm_robust, label="C1: Ignores Clusters, HC22 SE")

## Clustered SEs (CR1; Stata default)
estC2 <- declare_estimator(Y~Zcluster, inquiry=theestimand, .method=lm_robust, clusters=cluster, se_type="stata", label="C2: OLS, CR1 SE (Stata)")

## CR2 SEs (difference in means)
estC3 <- declare_estimator(Y~Zcluster, inquiry=theestimand, .method=difference_in_means, clusters = cluster, label="C3: Diff Means, CR2 SE")

## CR2 SEs (linear regression)
estC4 <- declare_estimator(Y~Zcluster, inquiry=theestimand, .method=lm_robust, clusters = cluster, se_type="CR2", label="C4: OLS, CR2 SE")

## Horvitz-Thompson estimator with clustering
estC5 <- declare_estimator(Y~Zcluster, inquiry=theestimand, .method=horvitz_thompson, clusters=cluster, simple=FALSE, condition_prs=.5, label="C5: Horvitz-Thompson Cluster, Young SE")

## Linear regression, CR2 errors, weight by cluster size
estC6 <- declare_estimator(Y~Zcluster, inquiry=theestimand,
.method=lm_robust, weights = nb, clusters=cluster, se_type="CR2", label="C6: OLS with Cluster Size Weights, CR2 SE")

## Linear regression, CR2 errors, control for cluster size, variance weights
estC7 <- declare_estimator(Y~Zcluster+nb, inquiry=theestimand,
.method=lm_robust, weights=varweight, clusters=cluster, se_type="CR2", label="C7: OLS Clusters with Weights, CR2 RCSE")

## Linear regression, CR2 errors, control for cluster size
estC8 <- declare_estimator(Y~Zcluster+nb, inquiry=theestimand, .method=lm_robust, clusters=cluster, se_type="CR2",
label="C8: OLS Clusters with adj for cluster size, CR2 RCSE")

## Linear regression, CR2 errors, lin estimation for cluster size
estC9 <- declare_estimator(Y~Zcluster, inquiry=theestimand,
.method=lm_lin, covariates=~nb, clusters=cluster, se_type="CR2",
label="C9: OLS Clusters with adj for cluster size, CR2 RCSE")

## Add all these estimators to the design
thedesignClusterPlusEstimators <- thedesignCluster +
	estC0 + estC1 + estC2 + 
  estC3 + estC4 + estC5 + 
  estC6 + estC7 + estC8 + 
  estC9

## Simulate the performance of these estimators
sims <- 200
set.seed(12345)
thediagnosisCluster <- diagnose_design(thedesignClusterPlusEstimators, sims = sims, bootstrap_sims = 0)
```
:::
::: {#ch5Stata33 .tabcontent} 
<br />
```{stata, highlight = F, eval = F}
** Define estimators that can be repeated in the simulation below
capture program drop apply_estimators
program define apply_estimators, rclass

	syntax[ , datCluster]
	
	if "`datCluster'" == "" {
		** Call the data generation program defined above
		sample_from
		return scalar ATE = r(ATE)
	}
	
	else {
		** Get the treatment assignment used in the R code
		import delimited using "datCluster.csv", clear
		rename y ynew
	}
	
	** C0: Ignores Clusters, IID SE
	qui reg ynew zcluster
	return scalar estC0_est = _b[zcluster]
	return scalar estC0_se = _se[zcluster]
	return scalar estC0_p = r(table)["pvalue", "zcluster"]
	
	** C1: Ignores Clusters, HC2 SE
	qui reg ynew zcluster, vce(hc2)
	return scalar estC1_est = _b[zcluster]
	return scalar estC1_se = _se[zcluster]
	return scalar estC1_p = r(table)["pvalue", "zcluster"]
	
	** C2: OLS, CR1 SE (Stata)
	qui reg ynew zcluster, vce(cluster cluster)
	return scalar estC2_est = _b[zcluster]
	return scalar estC2_se = _se[zcluster]
	return scalar estC2_p = r(table)["pvalue", "zcluster"]

	** C3: OLS, CR2 SE
	qui reg_sandwich ynew zcluster, cluster(cluster)
	return scalar estC3_est = _b[zcluster]
	return scalar estC3_se = _se[zcluster]
	local p = (2 * ttail(e(dfs)[1,1], abs(_b[zcluster]/_se[zcluster])))
	return scalar estC3_p = `p'

	** C6: OLS with Cluster Size Weights, CR2 SE
	qui reg_sandwich ynew zcluster [pw = nb], cluster(cluster)
	return scalar estC6_est = _b[zcluster]
	return scalar estC6_se = _se[zcluster]
	local p = (2 * ttail(e(dfs)[1,1], abs(_b[zcluster]/_se[zcluster])))
	return scalar estC6_p = `p'
	
	** C7: OLS with Cluster Control and Var. Weights, CR2 SE
	qui reg_sandwich ynew zcluster nb [pw = varweight], cluster(cluster)
	return scalar estC7_est = _b[zcluster]
	return scalar estC7_se = _se[zcluster]
	local p = (2 * ttail(e(dfs)[1,1], abs(_b[zcluster]/_se[zcluster])))
	return scalar estC7_p = `p'
	
	** C8: OLS Clusters with adj for cluster size, CR2 RCSE
	qui reg_sandwich ynew zcluster nb, cluster(cluster)
	return scalar estC8_est = _b[zcluster]
	return scalar estC8_se = _se[zcluster]
	local p = (2 * ttail(e(dfs)[1,1], abs(_b[zcluster]/_se[zcluster])))
	return scalar estC8_p = `p'
	
	** C9: OLS Clusters with adj for cluster size, CR2 RCSE
	qui reg ynew zcluster nb
	gen sample = e(sample)
	qui sum nb if sample == 1
	gen mc_nb = nb - r(mean) if sample == 1
	gen interaction = zcluster * mc_nb
	qui reg_sandwich ynew zcluster mc_nb interaction, cluster(cluster)
	return scalar estC9_est = _b[zcluster]
	return scalar estC9_se = _se[zcluster]
	local p = (2 * ttail(e(dfs)[1,1], abs(_b[zcluster]/_se[zcluster])))
	return scalar estC9_p = `p'
	
end

** Simulate the performance of these estimators
apply_estimators
local rscalars : r(scalars)
local to_store ""
foreach item of local rscalars {
  local to_store "`to_store' `item' = r(`item')"
}
simulate ///
`to_store', ///
reps(200): ///
apply_estimators

** Create summary matrix
qui des, short
di (`r(k)' - 1)/3
matrix diagnosands = J((`r(k)' - 1)/3, 6, .)
matrix rownames diagnosands = "C0: IID SE" "C1: HC2 SE" "C2: CR1 SE" "C3: CR2 SE" "C6: Cluster Size Weights" "C7: Var Weights" "C8: Cluster size control" "C9: Lin cluster size adjust"
matrix colnames diagnosands = "Mean estimand" "Mean estimate" "Bias" "SD Estimate" "Power" "Coverage"

** Get variable name roots to loop through
local roots estC0 estC1 estC2 estC3 estC6 estC7 estC8 estC9
 
** Calculate quantities to include
local row = 0
foreach l of local roots {
	
	local ++row
	
	* Estimand
	qui sum ATE, meanonly
	matrix diagnosands[`row', 1] = `r(mean)'
	
	* Estimate
	qui sum `l'_est, meanonly
	matrix diagnosands[`row', 2] = `r(mean)'
	
	* Bias
	qui gen biascalc = `l'_est - ATE
	qui sum biascalc, meanonly
	matrix diagnosands[`row', 3] = r(mean)
	drop biascalc
	
	* SD estimate (based on population variance, no n-1 in the variance denom.)
	qui sum `l'_est
	qui gen sdcalc = (`l'_est - r(mean))^2
	qui sum sdcalc
	matrix diagnosands[`row', 4] = sqrt(r(sum)/r(N))
	drop sdcalc
	
	* Power
	gen rejectnull = `l'_p <= 0.05
	qui sum rejectnull, meanonly
	matrix diagnosands[`row', 5] = r(mean)
	drop rejectnull
	
	* Coverage (z-stat CIs to limit estimates stored above)
	qui gen conflow = `l'_est - (1.96 * `l'_se)
	qui gen confhigh = `l'_est + (1.96 * `l'_se)
	qui gen inrange = ATE <= confhigh & ATE >= conflow
	qui sum inrange
	matrix diagnosands[`row', 6] = r(mean)
	drop conf* inrange
	
}

* View the results
matrix list diagnosands
```
::: 
::: {#ch5Hide33 .tabcontent}
::: 
:::

```{r defineestimatorsCluster, results="hide", echo = F, eval = T}
## Define estimators that can be repeated in the simulation below

## No adjustment for clustering
estC0 <- declare_estimator(Y~Zcluster, inquiry=theestimand, .method=lm, label="C0: Ignores Clusters, IID SE")

## HC2 SEs
estC1 <- declare_estimator(Y~Zcluster, inquiry=theestimand, .method=lm_robust, label="C1: Ignores Clusters, HC22 SE")

## Clustered SEs (CR1; Stata default)
estC2 <- declare_estimator(Y~Zcluster, inquiry=theestimand, .method=lm_robust, clusters=cluster, se_type="stata", label="C2: OLS, CR1 SE (Stata)")

## CR2 SEs (difference in means)
estC3 <- declare_estimator(Y~Zcluster, inquiry=theestimand, .method=difference_in_means, clusters = cluster, label="C3: Diff Means, CR2 SE")

## CR2 SEs (linear regression)
estC4 <- declare_estimator(Y~Zcluster, inquiry=theestimand, .method=lm_robust, clusters = cluster, se_type="CR2", label="C4: OLS, CR2 SE")

## Horvitz-Thompson estimator with clustering
estC5 <- declare_estimator(Y~Zcluster, inquiry=theestimand, .method=horvitz_thompson, clusters=cluster, simple=FALSE, condition_prs=.5, label="C5: Horvitz-Thompson Cluster, Young SE")

## Linear regression, CR2 errors, weight by cluster size
estC6 <- declare_estimator(Y~Zcluster, inquiry=theestimand,
.method=lm_robust, weights = nb, clusters=cluster, se_type="CR2", label="C6: OLS with Cluster Size Weights, CR2 SE")

## Linear regression, CR2 errors, control for cluster size, variance weights
estC7 <- declare_estimator(Y~Zcluster+nb, inquiry=theestimand,
.method=lm_robust, weights=varweight, clusters=cluster, se_type="CR2", label="C7: OLS Clusters with Weights, CR2 RCSE")

## Linear regression, CR2 errors, control for cluster size
estC8 <- declare_estimator(Y~Zcluster+nb, inquiry=theestimand, .method=lm_robust, clusters=cluster, se_type="CR2",
label="C8: OLS Clusters with adj for cluster size, CR2 RCSE")

## Linear regression, CR2 errors, lin estimation for cluster size
estC9 <- declare_estimator(Y~Zcluster, inquiry=theestimand,
.method=lm_lin, covariates=~nb, clusters=cluster, se_type="CR2",
label="C9: OLS Clusters with adj for cluster size, CR2 RCSE")

## Add all these estimators to the design
thedesignClusterPlusEstimators <- thedesignCluster +
	estC0 + estC1 + estC2 + 
  estC3 + estC4 + estC5 + 
  estC6 + estC7 + estC8 + 
  estC9

## Simulate the performance of these estimators
sims <- 200
set.seed(12345)
thediagnosisCluster <- diagnose_design(thedesignClusterPlusEstimators, sims = sims, bootstrap_sims = 0)
```

```{r, echo = F, include = F, eval = F}
## Bill: code I'm leaving out for now

## Get a character vector of the estimators just created
## (using a regular expression)
theestimatorsC <- ls(patt="^estC[0-9]")

## Apply each of these estimators to the data
## (get(x) calls an object with this name from the environment;
## this function is then applied to the dataframe in question)
checkestC <- sapply(
  theestimatorsC,
  function(x){
    get(x)(as.data.frame(datCluster))[c("estimate","std.error")]
    }
  )

## Review the results
checkestC
```


The results of our simulation show how certain approaches can yield very biased estimates, while other approaches can reduce the bias. With variaion in cluster size, the C5 and C6 estimators have the lowest bias in this particular example (with very few clusters). We also see evidence of problems with some of these standard errors --- the actual standard errors (in "SD Estimate") should be much higher than those estimated by the approaches ignoring the clustered design (C0 and C1).

```{r diagclustres1, echo = F}
kable(reshape_diagnosis(thediagnosisCluster)[, c(3, 6:9, 11:12)])
```

```{r setupclustgraph1, echo = F, eval = F}
## commenting out for now
simdesigns <- get_simulations(thediagnosis)
## simdesigns <- simulate_design(thedesign,sims=sims)
simmeans <- simdesigns %>% group_by(estimator) %>% summarize(expest=mean(estimate))
```

<!-- The following plot shows many of the estimators produce results far from the -->
<!-- truth (shown by the vertical black bar), and also shows the diversity in -->
<!-- precision of the estimators. -->

```{r clusterresultsplot, eval = F, echo = F}
## commenting out for now
## Now compare to better behaved outcomes.
gDiagClust <- ggplot(data=simdesigns,aes(x=estimate,color=estimator)) +
	geom_density() +
	geom_vline(xintercept=trueATE1) +
	geom_point(data=simmeans,aes(x=expest,y=rep(0,10)),shape=17,size=6) +
	theme_bw() #+
	#theme(legend.position = c(.9,.8))
print(gDiagClust)
```

### Incorrect false positive rates from tests and confidence intervals

When we have few clusters, analytic standard errors could lead to incorrect false positive rates for our hypothesis tests or confidence intervals, even after we adjust our standard errors for clustering (e.g., using the default adjustment in Stata, CR1). The CR2 errors OES prefers tend to perform slightly better in settings with fewer clusters [@pustejovsky2019cr]. We demonstrate below using CR2 errors rather than CR1 can sometimes help ensure that the false positive rates of our hypothesis tests is controlled correctly.

To distinguish between (1) the problems of bias arising from unequal sized clusters and (2) problems of false positive rates or poor covereage arising from the presence of few clusters, we use a design with equal numbers of units per cluster:

```{r clusterdesign2, echo = F}
with(dat1,table(Zcluster,buildingID))
```

::: {.tab} 
<button class="tablinks" onclick="unrolltab(event, 'ch5R35')">R code</button>
<button class="tablinks" onclick="unrolltab(event, 'ch5Stata35')">Stata code</button>
<button class="tablinks" onclick="unrolltab(event, 'ch5Hide35')">Hide</button>
::: {#ch5R35 .tabcontent} 
<br />
```{r, highlight = F, eval = F}
## Break any relationship between treatment and outcomes by permuting
## or shuffling the treatment variable. This means that H0, the null,
## of no effects is true.
checkFP <- function(dat, setype="CR2"){
	dat$newZ <-  cluster_ra(cluster=dat$buildingID)
	newest <- lm_robust(Y~newZ, dat=dat, clusters=buildingID, se_type=setype)
	return(nullp = newest$p.value["newZ"])
}

## Construct a separate dataset for this example
smalldat <- dat1[, c("Y","buildingID")]

## Apply the function above 1000 times (CR2)
set.seed(123)
fpresCR2 <- replicate(1000, checkFP(dat=smalldat))

## Apply the function above 1000 times (CR1, Stata)
set.seed(123)
fpresStata <- replicate(1000, checkFP(dat=smalldat, setype="stata"))

## Summarize the results
fprateCR205 <- mean(fpresCR2 <= .05)
paste0("fprateCR205 = ", fprateCR205)
fprateStata05 <- mean(fpresStata <= .05)
paste0("fprateStata05 = ", fprateStata05)
```
:::
::: {#ch5Stata35 .tabcontent} 
<br />
```{stata, highlight = F, eval = F}
** Return to data from Chapter 4
import delimited using "dat1_with_designs.csv", clear

** Break any relationship between treatment and outcomes by permuting
** or shuffling the treatment variable. This means that H0, the null,
** of no effects is true.
capture program drop checkFP
program define checkFP, rclass

	** Specify if you want CR2, rather than CR1 (Stata)
	syntax[, se_CR2]
	capture drop newZ
	cluster_ra newZ, cluster_var(buildingid)
	if "`se_CR2'" == "" {
		reg_sandwich y newZ, cluster(buildingid)
		local p = (2 * ttail(e(dfs)[1,1], abs(_b[newZ]/_se[newZ])))
		return scalar nullp = `p'
	}
	else {
		reg y newZ, vce(cluster buildingid)
		local p = (2 * ttail(e(df_r), abs(_b[newZ]/_se[newZ])))
		return scalar nullp = `p'
	}

end

** Apply the function above 1000 times (CR2)
preserve
	set seed 123
	simulate ///
	nullp = r(nullp), ///
	reps(1000) nodots: ///
	checkFP, se_CR2
	
	qui gen reject = nullp <= 0.05
	qui sum reject
	di "fprateCR205 `r(mean)'"
restore

** Apply the function above 1000 times (Stata)
preserve
	set seed 123
	simulate ///
	nullp = r(nullp), ///
	reps(1000) nodots: ///
	checkFP
	
	qui gen reject = nullp <= 0.05
	qui sum reject
	di "fprateStata05 `r(mean)'"
restore
```
::: 
::: {#ch5Hide35 .tabcontent}
::: 
:::

```{r fprateCR2, cache=TRUE, resuls="hold", echo = F, eval = T, collapse=T}
## Break any relationship between treatment and outcomes by permuting
## or shuffling the treatment variable. This means that H0, the null,
## of no effects is true.
checkFP <- function(dat, setype="CR2"){
	dat$newZ <-  cluster_ra(cluster=dat$buildingID)
	newest <- lm_robust(Y~newZ, dat=dat, clusters=buildingID, se_type=setype)
	return(nullp = newest$p.value["newZ"])
}

## Construct a separate dataset for this example
smalldat <- dat1[, c("Y","buildingID")]

## Apply the function above 1000 times (CR2)
set.seed(123)
fpresCR2 <- replicate(1000, checkFP(dat=smalldat))

## Apply the function above 1000 times (CR1, Stata)
set.seed(123)
fpresStata <- replicate(1000, checkFP(dat=smalldat, setype="stata"))

## Summarize the results
fprateCR205 <- mean(fpresCR2 <= .05)
paste0("fprateCR205 = ", fprateCR205)
fprateStata05 <- mean(fpresStata <= .05)
paste0("fprateStata05 = ", fprateStata05)
```

In this case, with 10 equal sized clusters, a simple outcome, and equal numbers of clusters in treatment and control, we see that the CR2 standard error controls the false positive rate (*less than* 5% of the 1000 simulations testing a true null hypothesis of no effect return a $p$-value of less than .05) while the default "Stata" (CR1) standard error has a slightly too high false positive rate of `r fprateStata05` at the 5\% error level.

<!-- The following plot shows that, in this case the Stata standard error tends to -->
<!-- make slightly too many false positive errors (shown by open circles above the -->
<!-- 45 degree line) and the CR2 standard error tends control the error rate of the -->
<!-- test (with black dots below the 45 degree line). -->

```{r plotfpr, eval = F, echo = F}
## commenting out for now
plot(ecdf(fpresCR2),pch=19)
plot(ecdf(fpresStata),pch=21)
abline(0,1)
```

If a simulation like the one above shows false positive rate problems with the CR2 standard error as well, we may prefer to rely on permutation-based randomization inference. For more discussion of issues with standard error calculation in the presence of few clusters, and examples of some other possible approaches to testing with few clusters, see @esarey2019practical.

<!-- [TO DO: An example using permutation based inference, illustrating a better false positive rate]. -->

