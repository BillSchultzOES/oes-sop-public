```{r setupdat14, echo=FALSE}
## Read in the dat1 file created from 04-randomizeddesigns.Rmd
dat1 <- read.csv(here::here('dat1_with_designs.csv'))
dat1$rankY <- rank(dat1$Y)
ndat1 <- nrow(dat1)
```

<!-- Note: drafting this here, but will be inserting at the bottom of chapter 5 -->

# The problem of missing data

Researchers sometimes anticipate having access to data when designing a project that they ultimately cannot collect. This is called **missing data**, and it is a logistical reality of applied research, especially when working with datasets that were not initially developed for research purposes. When a variable is missing across an entire sample it has to be dropped from the planned analysis (assuming it can't be collected some other way). The best reponse is more difficult when values of a variable are missing for only some of the sample (e.g.: not everyone filling out a benefit application form self-identified their race, but some did). We focus on incomplete missingness, and how an analysis might account for it when further data collection isn't an option.

We often encounter a few kinds of (incomplete) missing data: (1) missing covariate values; and (2) missing outcome data (i.e., **attrition**). Either may arise for a variety of reasons, such as necessary privacy restrictions that weren't clear until after an Analysis Plan was already written. Attrition in particular may stem from things like losing track of the subjects in a study, or subjects choosing not to participate in outcome data collection. Either form of missing data can be completely random, conditionally random, confined to a subgroup, or other [@gerber_field_2012]. Missing covariate data and attrition sometimes require different responses from a researcher. We point this out below where relevant.

Keep in mind that we'll focus less on providing concrete recommendations in this section, and instead focus more on helping team members think through the best strategy for their specific evaluations. This can be very context dependent.

## Missingness mechanisms

Missingness is often categorized as follows [@rubin1976inference]:

- Missing Completely at Random (**MCAR**)

- Missing at Random (**MAR**)

- Missing not at Random (**MNAR**)

In the case of attrition, MCAR may also be referred to as MIPO, or Missing Independent of Potential Outcomes [@gerber_field_2012]. Similarly, MAR may be referred to as Missing Independent of Potential Outcomes Given X (MIPO|X).

When a variable is MCAR, whether a given observation is missing is effectively random: it does not depend on the values of other observed covariates or the variable with missigness itself (e.g.: lower values of the variable are not more likely to be missing). Missingness may not be *truly* random, but it is at least idiosyncratic for the purposes of the evaluation at hand. This is the least problematic form of missingness.

When a variable is MAR, missingness is now non-random and depends on the set of *observed* covariates. However, among observations with the same values of these covariates, data are again MCAR. Many common strategies for addressing missing data in applied social science research, like multiple imputation, are developed for cases where MAR holds (MCAR implies MAR).

Lastly, a variable may be MNAR, which means that missigness is non-random and remains so even after conditioning on all observed covariates. Put differently, missingness depends on values of the variable itself in ways that observed covariates cannot fully explain. This is the most problematic form of missing data. Methods of accounting for missing data during estimation (e.g.: multiple imputation) that assume MAR may perform poorly when MNAR holds instead. However, it is possible to perform sensitivity analysis or bounding exercises to think through how serious a threat MNAR is.

To understand the intuition behind this categorization, keep in mind that causal inference itself can be thought of as a missing data problem [@aronow2019foundations]: we're using observed potential outcome values to estimate the distribution of unobserved potential outcome values. This is relatively easy to defend when treatment is random, since this means that which potential outcomes we observe is also random.

Imagine that there is missingness in some variable needed for an analysis, but we know MCAR holds. In response, we simply drop all observations with missing values. The set of observed potential outcomes we have to draw from when estimating the "missing" potential outcomes has become smaller. This could decrease power, but it would not introduce bias, since we know there is no systematic pattern to the potential outcomes we have lost. In contrast, this logic *does not apply under MNAR*, since MNAR implies that observations with missing values are systematically different in ways that we cannot explain. Therefore, the potential outcomes observed among complete cases may not be informative distribution of missing potential outcomes across our entire sample.

## Choose a missing data method

Our randomized evaluations often adjust for prognostic covariates in order to improve precision. When certain values of some of these covariates are missing, the researcher needs to decide whether it is better to exclude those covariates from the analysis or to include them and apply some missing data technique. Meanwhile, when the problem is a missing outcome measure (attrition), excluding the variable is generally not an option, and choosing some missing data technique to apply is required.

We suggest OES team members proceed as follows:

<!-- Need to make a flowchart? -->

1. Consider whether any covariates with missingness are known to be strong predictors of the outcome, or whether we simply believe they might be. If it's the former, or if the problem is not missing covariates but attrition, proceed to Step 2. If it's the latter, consider first whether dropping the covariates from the analysis would be reasonable. Proceed to Step 2 if you still think it's important to include the covariates, perhaps to account for chance imbalances detected after random assignment, or because even small precision gains are essential to ensuring we have enough power.

2. Consider whether MCAR is appropriate (if not, proceed to step 3). For instance, missing values may be a result of logistical or data sharing issues that are unrelated to characteristics of the units we're studying or the goals of our evaluation. In this situation, the simplest method to consider is **listwise deletion** (**complete case analysis**). This means that you drop any observations with missing values on relevant variables. Most of the commonly used statistical packages in Political Science and Economics do this by default, *even if they do not tell you*. The obvious disadvantage is the loss of power when many observations are missing [@king2001analyzing]. Consider whether the loss of power from applying listwise deletion for your evaluation would be substantial (and in the case of missing covariates, how might it compare to dropping those covariates instead?). Proceed to Step 3 if you are interested in whether more sophisticated methods could yield more precise treatment effect estimates.
  
3. Consider whether MAR is appropriate (if not, proceed to step 4). This amounts to determining whether violations of MCAR can be entirely explained by other variables in our dataset (even if we are not planning on using these variables in treatment effect estimation). MAR may be more likely to hold as the available data for an evaluation grows, though a large dataset does not *necessarily* justify MAR. There are many methods out there for analyzing data under MAR (or MCAR). A few options that are common in the academic fields OES research overlaps with the most (see subsections below): multiple imputation (MI); inverse propensity score weighting (IPW); and the missingness indicator method (IM). Other methods may be acceptable as well. Sometimes, a review will quickly indicate that one method makes more sense for a given evaluation. When the right method isn't immediately clear, simulation may help researchers think through this decision. See our examples below.

4. If we expect MNAR to hold, we recommend providing results from several methods, and conducting some form of sensitivity analysis or bounding to explore how unexplained missingness might influence our findings (see below). MNAR is easiest to account for when it is anticipated during an evaluation's design stage. For instance, additional data collection might be planned for observations missing in the first round, or the study could plan to leverage some plausible instrument for missingness in a selection model.^[@behaghel2015please and @molina2017attrition discuss research exploring some of these options.]

## Common missing data methods in policy research

### Multiple imputation (MI)

**Quick summary**: Randomly generate $k$ datasets (5, 10, 100, etc.) in which each missing value is filled in using various imputation techniques. These techniques include simultaneously drawing missing values from a joint multivariate normal distribution [@honaker2011amelia] or fitting separate imputation models for each variable with missingness [@white2011multiple].^[The latter is often referred to as a "chained equations" or "fully conditional" method.] Then, summarize results across the $k$ datasets following *Rubin's rules* to produce a single point estimate and standard error estimate [@rubin1976inference; @royston2004multiple].

**High-level considerations**: MI is the most complex method we highlight, but also the one that likely reduces bias and variance in the greatest variety of situations under MAR.^[MI will not always be superior. See @little2022comparison, @carpenter2021missing, and @hughes2019accounting for discussion of some possible exceptions.] MI takes more work to program, and it will likely take longer for the analysis to finish running. Keep processing time concerns in mind if an evaluation already involves a relatively complicated model or a large dataset. MI has practical and efficiency advantages over other methods when there is missingness in either many covariates or in both outcomes and covariates.

**Implementation notes**: As long as the sample is large enough, drawing missing values for many variables simultaneously from a multivariate normal distribution (e.g.: the `Amelia` package in R or `mi impute mvn` in Stata) generally produces similar findings to methods that impute missing values for each variable separately (e.g.: the `mice` package in R or `mi impute chained` in Stata), even if the variable with missingness is not continuous or normally distributed [@lee2010multiple]. It will probably also run more quickly. But if you need variables with missingness to be imputed in a way that respects their distribution, use the separate-models approach ("chained equations").

The performance of MI relative to other methods, especially in terms of reducing variance, generally improves as more **auxiliary** variables are added to the imputation procedure---assuming the sample size is large enough [@hardt2012auxiliary]. These are covariates that may not be included in the treatment effect estimation step, but which help better predict missing values in the imputation step. Also, when imputing missing covariate values, be sure to include outcome measures and treatment indicators in the imputation model alongside any auxiliary variables (and always include treatment indicators when imputing missing outcomes). Additionally, any interaction terms used in the estimation model should be added to the dataset *as separate variables* and included in the imputation model as well. In other words, don't generate an interaction of imputed variables.

### Inverse propensity score weighting (IPW)

**Quick summary**: Predict the probability of missingness for each observation, and then perform an analysis among complete cases where the inverse of this probability is used as a weight (i.e., listwise deletion with weights).

### Indicator method (IM)

**Quick summary**: Generate a separate dummy variable for each variable with missingness (1 if an observation is missing for this variable, 0 otherwise). Fill in missing values with 0, or the variable's observed mean. Perform an analysis on the full (now modified) sample, including all of the dummies as controls.

### Reasons to return to listwise deletion

It's helpful to review some situations where listwise deletion (complete case analysis) might perform better than researchers would expect and is worth a second look. Note that these points are based on discussions that do not consider the missingness indicator method [@little2022comparison; @hughes2019accounting]. We also assume a researcher is estimating experimental treament effects using an OLS linear regression analysis.

First, when only the outcome has missingness and MAR holds, a regression of the outcome on treatment and covariates among complete cases will often be unbiased and no less efficient than methods like MI [@little2022comparison; @carpenter2021missing].^[In fact, MI could be less efficient in some cases. See @hughes2019accounting, Box 2. But this is less likely in the presence of prognostic auxiliary variables.] I.e., the weaker MAR assumption is often sufficient to justify listwise deletion if attrition can be completely explained by covariates. @little2022comparison note that IPW might perform better than in this case than complete case analysis if the estimation model is misspecified while the model predicting missingness is not (so consider IPW as a robustness check).

Second, complete case analysis can be unbiased under even under MNAR when missingness is in a covariate and *does not depend on the outcome* [@hughes2019accounting; @carpenter2021missing]. Other missing data methods that assume MAR, like MI, are sometimes biased in this setting.^[When MNAR holds, we know that values we observe may not be representative of values we don't observe. Imputing missing values based on that observed data will sometimes introduce biases that complete case analysis would not.] However, even when it is biased, MI will likely still be more efficient here---and possibly preferred overall on grounds of minimizing mean squared error. The efficiency advantage of MI grows as missingness becomes scattered across more covariates. 

Third, all methods we discuss are generally unbiased (though MI in particular will likely still be more efficient) when missingness is in an covariate and MAR holds conditionally on just other observed covariates. Here, the potential advantage of complete case analysis is just its simplicity, assuming the precision gains from MI aren't a priority (e.g., if only a small proportion of the sample is missing).

## Comparing methods through simulation

imp using amelia, great guide written by package authors [here](https://cran.r-project.org/web/packages/Amelia/vignettes/using-amelia.html)

conditions

- missingness only in covariate, MAR vs MNAR hold, never outcome

- missingness in covariates and outcome

```{r}
set.seed(20405)

## Base data: potential outcomes, treatment, and 8 observed covariates
base_dat <- dat1 %>% 
  select(y0, y1, Z, paste0("cov", c(1:6)))

# Want a larger dataset in one case; bootstrap
sel <- sample(1:nrow(base_dat), 100 * 10, replace = T)
base_dat2 <- base_dat[sel,]

## Dataset for condition 1: attrition under MAR
# (due only to a covariate included in the model)
dat1_missY_MAR <- base_dat
dat1_missY_MAR$prob_miss <- ifelse(
  base_dat$cov2 > quantile(base_dat$cov2, probs = 0.75), 0.5, 0.1
  )

## Dataset for condition 2: differential attrition under MAR
# (differential part added below)
dat1_missY_MAR2 <- dat1_missY_MAR

## Dataset for condition 3: attrition under MNAR
dat1_missY_MNAR <- base_dat
dat1_missY_MNAR$prob_miss <- ifelse(
  base_dat$y1 > quantile(base_dat$y1, probs = 0.75), 0.5, 0.1
  )

## Dataset for condition 4: differential attrition under MNAR
# (differential part added below)
dat1_missY_MNAR2 <- dat1_missY_MNAR

## Dataset for condition 5: missing covariates under MAR
# (missingness is not conditional on outcome)
dat1_missCov1_MAR <- base_dat
dat1_missCov1_MAR$sum <- dat1_missCov1_MAR$cov1
dat1_missCov1_MAR$prob_miss <- ifelse(
  dat1_missCov1_MAR$sum > quantile(dat1_missCov1_MAR$sum, probs = 0.75), 0.5, 0.1
  )
dat1_missCov1_MAR <- dat1_missCov1_MAR %>% select(-sum)

## Dataset for condition 6: missing covariates under MAR
# (missingness is conditional on outcome)
dat1_missCov1_MAR2 <- base_dat
dat1_missCov1_MAR2$sum <- dat1_missCov1_MAR2$y1
dat1_missCov1_MAR2$prob_miss <- ifelse(
  dat1_missCov1_MAR2$sum > quantile(dat1_missCov1_MAR2$sum, probs = 0.5), 0.3, 0.1
  )
dat1_missCov1_MAR2 <- dat1_missCov1_MAR2 %>% select(-sum)

## Dataset for condition 7: missing covariate under MNAR, n = 100
dat1_missCov1_MNAR <- base_dat
dat1_missCov1_MNAR$sum <- dat1_missCov1_MNAR$cov1 * dat1_missCov1_MNAR$cov2
dat1_missCov1_MNAR$prob_miss <- ifelse(
  dat1_missCov1_MNAR$sum > quantile(dat1_missCov1_MNAR$sum, probs = 0.75), 0.50, 0.1
  )
dat1_missCov1_MNAR <- dat1_missCov1_MNAR %>% select(-sum)
# dat1_missCov1_MNAR <- dat1_missCov1_MNAR %>% select(y0, y1, Z, cov1, prob_miss)

## Dataset for condition 8: missing covariate under MNAR, n = 1000
dat1_missCov1_MNAR2 <- base_dat2
dat1_missCov1_MNAR2$sum <- dat1_missCov1_MNAR2$cov1
dat1_missCov1_MNAR2$prob_miss <- ifelse(
  dat1_missCov1_MNAR2$sum > quantile(dat1_missCov1_MNAR2$sum, probs = 0.75), 0.5, 0.1
  )
dat1_missCov1_MNAR2 <- dat1_missCov1_MNAR2 %>% select(-sum)

## Estimation formulas used:
## Primary is regression of y on treatment, with 8 covariates (cov1-cov8).
## Revealed outcome and permuted treatment generated below.
est_model <- reformulate(
  termlabels = c("Znew", paste0("cov", seq(1:3))),
  response = "y"
  )

test <- reformulate(
  termlabels = c("Z", paste0("cov", seq(1:6))),
  response = "Y"
  )

# Used for condition 6
est_model2 <- reformulate(
  termlabels = c("Znew", "cov1"),
  response = "y"
  )

## True ATE for comparison / quantifying bias
trueATE <- mean(base_dat$y1 - base_dat$y0)
```

```{r}
## Function to fit the desired estimators
## for datasets with different missingness conditions.
## May need updating to apply in other settings.
fit_missing_models <- 
  
  ## Input the data (with a built in missingness condition)
  ## and the names of variables with missingness. One or several.
  function(
    data,
    miss_var = "y",
    differential = NULL,
    f = est_model,
    aux = F
    ) {
  
    ## Randomly permute Z; matches how the DeclareDesign
    ## simulations elsewhere introduce random noise
    ## (following a randomization-design-based approach to inference).
    data$Znew <- sample(data$Z)  
    
    ## Set revealed outcomes, given the permuted Z
    data$y <- data$y1 * data$Znew + (1 - data$Znew) * data$y0
  
    ## Set missingness pattern, determined when
    ## coding different condition datasets above.
    ## Loops through each miss_var specified during function call.
    
    # First: add differential attrition multiplier?
    if (!is.null(differential)) {
      data$prob_miss <- ifelse(
        data$Znew == 1,
        differential * data$prob_miss,
        data$prob_miss
        )
      }
    
    # Second: loop mentioned above
    data$sel_miss <- rbinom(nrow(data), 1, data$prob_miss)
    for (k in 1:length(miss_var)) {
      data[data$sel_miss == 1, miss_var[k]] <- NA
      }
  
    ## Drop variables we don't want to include
    ## in the imputation model during MI.
    data <- data %>% select(-sel_miss, -y0, -y1, -Z, -prob_miss)
    
    ## Drop auxiliary variables be default, option to keep
    if (aux == F) {
      data <- data %>% select(-paste0("cov", c(4:6)))
    }
    
    ## Method 1: Listwise Deletion (Complete Case Analysis)
    
    # Fit the model (HC2 errors). Notice: no warning about CCA.
    cca_mod <- lm_robust(f, data, se_type = "HC2")
    # (cca_mod$nobs) # Less than 100 observations used (full sample size)
    
    # Organize output to return from CCA
    cca_out <- c(cca_mod$coefficients["Znew"], cca_mod$std.error["Znew"])
    names(cca_out) <- c("est", "se")
    
    ## Method 2: Multiple Imputation
    
    # Using amelia for illustration. See also mice,
    # both have very useful guides you can find online.
    # Code is just hiding console output amelia produces.
    # This step generates 10 multiply imputed datasets.
    amelia_dat <- amelia(data, m = 10, p2s = 0)
    
    # Estimate the desired model for each imputed dataset.
    # amelia provides a way to code this much more simply
    # if OLS (IID) errors are reasonable. Forcing HC2
    # via lm_robust requires just a bit more work:
    amelia_sep_out <- lapply(
      
      # For each imputed dataset...
      amelia_dat$imputations,
      
      # ...apply the following function
      # (where .x is the imputed dataset)
      function(.x) {
        mod <- lm_robust(f, data = .x)
        out <- c(mod$coefficients["Znew"], mod$std.error["Znew"])
        names(out) <- c("coef", "se")
        return(out)
        }
      ) %>%
      
      # Stack output together as a df (tibble)
      purrr::reduce(bind_rows)
    
    # Aggregate results across imputed datasets using "Rubin's rules".
    # Again, amelia provides a simpler automated approach.
    # Doing it like this to force the use of output from lm_robust.
    amelia_mod <- mi.meld(
      as.matrix(amelia_sep_out$coef),
      as.matrix(amelia_sep_out$se),
      byrow = T
      )
    
    # Organize output to return from amelia
    amelia_out <- c(amelia_mod$q.mi, amelia_mod$se.mi)
    names(amelia_out) <- c("est", "se")
    
    ## Organize all results and calculate additional information
    
    # Stack output together as separate rows
    out <- rbind(
      cca_out,
      amelia_out
      ) %>% 
      as.data.frame()
    
    # Method categorical variable
    out$method <- c("cca", "mi")
    
    # Z score, p-value, CI bounds.
    # Assuming normal sampling distr. is reasonable (CLT).
    out$z <- abs(out$est/out$se)
    out$p <- 2 * pnorm(out$z, lower.tail=FALSE)
    out$ci.low <- out$est - (1.96 * out$se)
    out$ci.hi <- out$est + (1.96 * out$se)
    
    return(out)
  
  }
```

```{r}
## Apply the missingness method function 200 times.
diagnose <- 
  
  # Apply this function repeatedly 200 times
  replicate(
    n = 200,
    expr = fit_missing_models(dat1_missY_MAR, aux = T),
    simplify = F
    ) %>%
  
    # Stack the results together into a single df (tibble)
    purrr::reduce(bind_rows)

## Diagnose the performance of different methods
## under condition 1: attrition under MAR.
c1_res <- diagnose %>%
  
  ## Summarize results within each method.
  group_by(method) %>%
  
  ## Provide some of the same statistics as DeclareDesign.
  summarize(
    mean_estimate = mean(est),
    bias = mean(est - trueATE),
    sd_estimate = sd(est),
    rmse = sqrt(mean((est - trueATE) ^ 2)),
    power = mean(p <= 0.05),
    coverage = mean(trueATE <= ci.hi & trueATE >= ci.low)
    )

```

```{r}
## Apply the missingness method function 200 times.
diagnose <- 
  
  # Apply this function repeatedly 200 times
  replicate(
    n = 200,
    expr = fit_missing_models(dat1_missY_MAR2, differential = 1.5, aux = T),
    simplify = F
    ) %>%
  
    # Stack the results together into a single df (tibble)
    purrr::reduce(bind_rows)

## Diagnose the performance of different methods
## under condition 2: differential attrition under MAR.
c2_res <- diagnose %>%
  
  ## Summarize results within each method.
  group_by(method) %>%
  
  ## Provide some of the same statistics as DeclareDesign.
  summarize(
    mean_estimate = mean(est),
    bias = mean(est - trueATE),
    sd_estimate = sd(est),
    rmse = sqrt(mean((est - trueATE) ^ 2)),
    power = mean(p <= 0.05),
    coverage = mean(trueATE <= ci.hi & trueATE >= ci.low)
    )

```

```{r}
## Apply the missingness method function 200 times.
diagnose <- 
  
  # Apply this function repeatedly 200 times
  replicate(
    n = 200,
    expr = fit_missing_models(dat1_missY_MNAR, aux = T),
    simplify = F
    ) %>%
  
    # Stack the results together into a single df (tibble)
    purrr::reduce(bind_rows)

## Diagnose the performance of different methods
## under condition 3: attrition under MNAR.
c3_res <- diagnose %>%
  
  ## Summarize results within each method.
  group_by(method) %>%
  
  ## Provide some of the same statistics as DeclareDesign.
  summarize(
    mean_estimate = mean(est),
    bias = mean(est - trueATE),
    sd_estimate = sd(est),
    rmse = sqrt(mean((est - trueATE) ^ 2)),
    power = mean(p <= 0.05),
    coverage = mean(trueATE <= ci.hi & trueATE >= ci.low)
    )

```

```{r}
## Apply the missingness method function 200 times.
diagnose <- 
  
  # Apply this function repeatedly 200 times
  replicate(
    n = 200,
    expr = fit_missing_models(dat1_missY_MNAR2, differential = 1.5, aux = T),
    simplify = F
    ) %>%
  
    # Stack the results together into a single df (tibble)
    purrr::reduce(bind_rows)

## Diagnose the performance of different methods
## under condition 3: differential attrition under MNAR.
c4_res <- diagnose %>%
  
  ## Summarize results within each method.
  group_by(method) %>%
  
  ## Provide some of the same statistics as DeclareDesign.
  summarize(
    mean_estimate = mean(est),
    bias = mean(est - trueATE),
    sd_estimate = sd(est),
    rmse = sqrt(mean((est - trueATE) ^ 2)),
    power = mean(p <= 0.05),
    coverage = mean(trueATE <= ci.hi & trueATE >= ci.low)
    )

```

```{r}
## Apply the missingness method function 200 times.
diagnose <- 
  
  # Apply this function repeatedly 200 times
  replicate(
    n = 200,
    expr = fit_missing_models(dat1_missCov1_MAR, miss_var = c("cov1")),
    simplify = F
    ) %>%
  
    # Stack the results together into a single df (tibble)
    purrr::reduce(bind_rows)

## Diagnose the performance of different methods
## under condition 5: missing covariate under MAR
# (does not depend on Y)
c5_res <- diagnose %>%
  
  ## Summarize results within each method.
  group_by(method) %>%
  
  ## Provide some of the same statistics as DeclareDesign.
  summarize(
    mean_estimate = mean(est),
    bias = mean(est - trueATE),
    sd_estimate = sd(est),
    rmse = sqrt(mean((est - trueATE) ^ 2)),
    power = mean(p <= 0.05),
    coverage = mean(trueATE <= ci.hi & trueATE >= ci.low)
    )

```

```{r}
## Apply the missingness method function 200 times.
diagnose <- 
  
  # Apply this function repeatedly 200 times
  replicate(
    n = 200,
    expr = fit_missing_models(dat1_missCov1_MAR2, miss_var = c("cov1", "cov2")),
    simplify = F
    ) %>%
  
    # Stack the results together into a single df (tibble)
    purrr::reduce(bind_rows)

## Diagnose the performance of different methods
## under condition 6: missing covariate under MAR
# (missingness depends on covariates and Y)
c6_res <- diagnose %>%
  
  ## Summarize results within each method.
  group_by(method) %>%
  
  ## Provide some of the same statistics as DeclareDesign.
  summarize(
    mean_estimate = mean(est),
    bias = mean(est - trueATE),
    sd_estimate = sd(est),
    rmse = sqrt(mean((est - trueATE) ^ 2)),
    power = mean(p <= 0.05),
    coverage = mean(trueATE <= ci.hi & trueATE >= ci.low)
    )

```

```{r}
## Apply the missingness method function 200 times.
diagnose <- 
  
  # Apply this function repeatedly 200 times
  replicate(
    n = 200,
    expr = fit_missing_models(dat1_missCov1_MNAR, miss_var = c("cov1", "cov2"), aux = T),
    simplify = F
    ) %>%
  
    # Stack the results together into a single df (tibble)
    purrr::reduce(bind_rows)

## Diagnose the performance of different methods
## under condition 7: missing covariate under MNAR
c7_res <- diagnose %>%
  
  ## Summarize results within each method.
  group_by(method) %>%
  
  ## Provide some of the same statistics as DeclareDesign.
  summarize(
    mean_estimate = mean(est),
    bias = mean(est - trueATE),
    sd_estimate = sd(est),
    rmse = sqrt(mean((est - trueATE) ^ 2)),
    power = mean(p <= 0.05),
    coverage = mean(trueATE <= ci.hi & trueATE >= ci.low)
    )

```

## Missing Independent of Potential Outcomes (MIPO)

If we suspect that our data is missing independent of potential outcomes, this
type of attrition can be seen as random and should have no effect on outcomes.
Therefore, we can directly estimate the ATE in our experiment without concern
for bias.

## Missing Independent of Potential Outcomes Given X (MIPO|X)

If we suspect that our data is missing independent of potential outcomes given
X, this type of attrition can be seen as random conditional on X, a
pre-treatment covariate.  This conditionality suggests that within each subgroup
of covariate X, our missing data is random.  We can have an unbiased estimate by
taking the weighted average within each subgroup.

If there is missing data within one subgroup, we could use inverse probability
weighting to obtain the average effect, where we divide the outcome recorded for
each subject without missing data by the inverse of the ratio of subjects
treated without missing data in the subgroup to the total subjects treated in
the subgroup.  We can then subtract the results of the control from the treats
of the treated to obtain ATE.

## Bounds

If we are unsure about whether our missing data is random, we may place bounds
on the treatment effect by filling in the missing data with extremely high or
extremely low outcomes and estimating the ATE after filling in the missing data.
We determine a range of outcomes for all subjects.  We fill in all of the
missing data with the highest value in the range to estimate the upper bound
ATE.  We fill in all of the missing data with the lowest value in the range to
estimate the lower bound ATE.

We now have some information that the true ATE lies within the upper and lower
bounds.  However, the greater the rate of attrition, larger the difference
between the bounds and the less informative the bounds will be.

## Sensitivity Analyses

```{r}

miss_X <- miss_Y <- dat1

miss_X[]

```
