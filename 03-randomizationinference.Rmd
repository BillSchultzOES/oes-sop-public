# Design based inference

Most policy evaluations using administrative data or surveys report the results of their studies using **estimators** and **hypothesis tests**. We briefly define each for clarity.

* *Estimators*: Although we can never know the true causal effect of a new policy on our beneficiaries (see Chapter 1), we can choose a method that provides us a best guess. E.g.: "The average amount saved for retirement by people in the treatment group was \$1000 more than the average amount in the control group, so our estimate of the average treatment effect is \$1000."

* *Hypothesis tests*: We can also test the plausibility of a particular hunch or hypothesis. Commonly, these tests focus on plausibility of the null hypothesis of no effect. E.g.: "We can reject the null hypothesis of no effect at a $5\%$ significance level given an estimated p-value of $2\%$." Confidence intervals can be used to summarize hypothesis tests, so we think of them as tests rather than estimators.

When we are asked *why* we used some method for calculating an
average treatment effect, $p$-value, or confidence interval, we often say that our statistical analysis choices are based on the *design* of our
studies. When applied to randomized experiments, this principle can be written
simply as: "analyze as you randomize." We provide an example of this
principle in practice below, and the intuition behind it is to model unexplained variation in the data as noise introduced by random treatment assignment.

This idea, often known as randomization based or **design based inference**, was proposed by two of the founders of modern statistics. Jerzy Neyman's 1923 paper showed how to use randomization to learn about what we would currently call "average treatment effects" [@neyman_application_1923], and Ronald A.  Fisher's 1935 book showed how to use randomization to test hypotheses about what we would currently call "treatment effects" [@fisher_design_1935]. We favor design based justifications because we know exactly how a study was designed---after all, we and our agency collaborators chose the sample size, experimental arms, and outcome data.

Of course, there are other ways to justify statistical procedures, too. We don't exclude any reasonable approach in our work. It is standard for researchers in many applied fields to instead motivate their tests using the idea of sampling from a larger (possibly infinite) population. These statistical procedures often depend on theoretical models of an estimator's behavior as sample size increases indefinitely (i.e., asymptotic theory). OES projects frequently employ statistical procedures justified in this way as well. And note that some common procedures, like the calculation of HC2 standard errors discussed below, can be justified under either design based or sampling based statistical inference.

## An example using simulated data {#randinfex}

Imagine we have a simple randomized experiment where the relationship between
outcomes and treatment is shown in Figure \@ref(fig:boxplot1) (we illustrate the first 6 rows in the table below). Notice that, in this simulated experiment, the treatment changes the variability of the outcome in the treated group --- this is a common pattern when the control group is a status quo policy.

<!-- Adds copy code button -->
```{r klippy, echo=FALSE, include=TRUE}
klippy::klippy(all_precode = T, position = c("top", "right"))
```

<!-- Used (and iteratively updated) in the {oes_code_tab} snippets below. -->
<!-- set chapter number and reset count -->
```{r, include = F, echo = F}
# cnum is modified automatically to iteratively count chunks
# when using the oes_code_tab markdown snippet. each use of
# the snippet adds a value of 1.
ch <- 3
cnum <- 0
```

::: {.tab} 
<button class="tablinks" onclick="unrolltab(event, 'ch3R1')">R code</button>
<button class="tablinks" onclick="unrolltab(event, 'ch3Stata1')">Stata code</button>
<button class="tablinks" onclick="unrolltab(event, 'ch3Hide1')">Hide</button>
::: {#ch3R1 .tabcontent} 
<br />
```{r, eval = F, highlight = F}
## Read in data for the fake experiment.
dat1 <- read.csv("dat1.csv")

## Table of the first few observations.
knitr::kable(head(dat1[, c(1, 23:27)]))
```
:::
::: {#ch3Stata1 .tabcontent} 
<br />
```{stata, eval = F, highlight = F}
** Read in data for the fake experiment.
import delimited using "dat1.csv", clear
rename v1 x

** Table of the first few observations.
list x y0 y1 z y id in 1/6, sep(6)
```
::: 
::: {#ch3Hide1 .tabcontent}
::: 
:::

```{r, eval = T, echo = F}
## Read in data for the fake experiment.
dat1 <- read.csv("dat1.csv")

## Table of the first few observations.
knitr::kable(head(dat1[, c(1, 23:27)]))
```

::: {.tab} 
<button class="tablinks" onclick="unrolltab(event, 'ch3R2')">R code</button>
<button class="tablinks" onclick="unrolltab(event, 'ch3Stata2')">Stata code</button>
<button class="tablinks" onclick="unrolltab(event, 'ch3Hide2')">Hide</button>
::: {#ch3R2 .tabcontent} 
<br />
```{r, eval = F, highlight = F}
## y0 and y1 are the true underlying potential outcomes.
with(dat1, {boxplot(list(y0,y1), names=c("Control","Treatment"), ylab="Outcomes")
stripchart(list(y0,y1), add=TRUE, vertical=TRUE)
stripchart(list(mean(y0), mean(y1)), add=TRUE, vertical=TRUE, pch=19, cex=2)})
```
:::
::: {#ch3Stata2 .tabcontent} 
<br />
```{stata, eval = F, highlight = F}
** y0 and y1 are the true underlying potential outcomes.
label var y0 "Control"
label var y1 "Treatment"
* ssc install stripplot
stripplot y0 y1, box vertical iqr whiskers(recast(rcap)) ytitle("Outcomes") variablelabels
```
::: 
::: {#ch3Hide2 .tabcontent}
::: 
:::

```{r boxplot1, fig.cap="Simulated Experimental Outcomes", echo = F}
## y0 and y1 are the true underlying potential outcomes.
with(dat1, {boxplot(list(y0,y1), names=c("Control","Treatment"), ylab="Outcomes")
stripchart(list(y0,y1), add=TRUE, vertical=TRUE)
stripchart(list(mean(y0), mean(y1)), add=TRUE, vertical=TRUE, pch=19, cex=2)})
```

In this simulated data, we know the true average treatment effect (ATE) because we know both of the underlying true potential outcomes. The control potential outcome, $y_{i|Z_i=0}$, is written in the code as `y0`, meaning "the response person $i$ would provide if he/she were in the status quo or control group". The treatment potential outcome,  $y_{i|Z_i = 1}$, is written in the code as `y1`, meaing "the response person $i$ would provide if he/she were in the new policy or treatment group." We use $Z_i$ to refer to the experimental arm. In this case $Z_i=0$ for people in the status quo group and $Z_i=1$ for people in the new policy group. The **true average treatment effect** for each person is then the difference between that person's potential outcomes under each treatment condition.

::: {.tab} 
<button class="tablinks" onclick="unrolltab(event, 'ch3R3')">R code</button>
<button class="tablinks" onclick="unrolltab(event, 'ch3Stata3')">Stata code</button>
<button class="tablinks" onclick="unrolltab(event, 'ch3Hide3')">Hide</button>
::: {#ch3R3 .tabcontent} 
<br />
```{r, highlight = F, eval = F}
trueATE <- with(dat1, mean(y1) - mean(y0))
trueATE
```
:::
::: {#ch3Stata3 .tabcontent} 
<br />
```{stata, highlight = F, eval = F}
qui tabstat y0 y1, stat(mean) save
global trueATE = r(StatTotal)[1,2] - r(StatTotal)[1,1]
```
::: 
::: {#ch3Hide3 .tabcontent}
::: 
:::

```{r trueATE, eval = T, echo = F}
trueATE <- with(dat1, mean(y1) - mean(y0))
trueATE
```

At this point, we have one *realized* experiment (defined by randomly assigning half of the people to treatment and half to control). And we know that the observed difference of means of the outcome, $Y$, between treated and control groups is an unbiased estimator of the true ATE (by virtue of random assignment to treatment).^[See https://egap.org/resource/10-types-of-treatment-effect-you-should-know-about/ for a demonstration that the difference-in-means between the observed treatment and control groups is an unbiased estimator of the average treatment effect itself, and what it means to be unbiased.] We can calculate this in a few ways: we can just calculate the difference of means, or we can take advantage of the fact that an ordinary least squares linear regression produces the same estimate when the explanatory variable is a binary treatment indicator.

::: {.tab} 
<button class="tablinks" onclick="unrolltab(event, 'ch3R4')">R code</button>
<button class="tablinks" onclick="unrolltab(event, 'ch3Stata4')">Stata code</button>
<button class="tablinks" onclick="unrolltab(event, 'ch3Hide4')">Hide</button>
::: {#ch3R4 .tabcontent} 
<br />
```{r, highlight = F, eval = F}
estATE1 <- with(dat1, mean(Y[Z==1]) - mean(Y[Z==0]))
estATE2 <- lm(Y~Z, data=dat1)$coef[["Z"]]
c(estimatedATEv1=estATE1, estimatedATEv2=estATE2)
stopifnot(all.equal(estATE1, estATE2))
```
:::
::: {#ch3Stata4 .tabcontent} 
<br />
```{stata, highlight = F, eval = F}
qui ttest y, by(z)
global estATE1 = round(r(mu_2) - r(mu_1), 0.001)
qui reg y z
global estATE2 = round(r(table)[1,1], 0.001)
di "estimatedATEv1=$estATE1 estimatedATEv2=$estATE2"
assert $estATE1 == $estATE2
```
::: 
::: {#ch3Hide4 .tabcontent}
::: 
:::

```{r estATE, echo = F, eval = T}
## Y is the observed outcome, Z is the observed treatment.
estATE1 <- with(dat1, mean(Y[Z==1]) - mean(Y[Z==0]))
estATE2 <- lm(Y~Z, data=dat1)$coef[["Z"]]
c(estimatedATEv1=estATE1, estimatedATEv2=estATE2)
```

###  Randomization-based standard errors

How much would these estimates of the average treatment effect vary due to "random noise" if we repeated an experiment on the same group of people multiple times (randomly re-assigning treatment each time)? The **standard error** of an estimate of the average treatment effect is one answer to this question. As the expected variation due to random noise gets larger relative to the size of our treatment effect estimate, we should become increasingly cautious about the risk that our finding is actually an artifact of random noise.^[As discussed above, under a design based approach, random noise is assumed to be introduced by treatment assignment decisions. Under more common sampling-based inference procedures, it is instead assumed to be introduced by the process of sampling from a larger population.]

Below, we simulate a simple, individual-level experiment to help provide more intuition about what the standard error captures. We randomly re-assign treatment many times, save the resulting treatment effect estimates from each re-randomization, and calculate the standard deviation across them. This provides information about how far we should expect any one re-randomized treatment effect estimate to be from their mean.

::: {.tab} 
<button class="tablinks" onclick="unrolltab(event, 'ch3R5')">R code</button>
<button class="tablinks" onclick="unrolltab(event, 'ch3Stata5')">Stata code</button>
<button class="tablinks" onclick="unrolltab(event, 'ch3Hide5')">Hide</button>
::: {#ch3R5 .tabcontent} 
<br />
```{r, highlight = F, eval = F}
## A function to re-assign treatment and recalculate the difference of means.
## Treatment was assigned without blocking or other structure, so we
## just permute or shuffle the existing treatment assignment vector.
simEstAte <- function(Z,y1,y0){
	Znew <- sample(Z)
	Y <- Znew * y1 + (1-Znew) * y0
	estate <- mean(Y[Znew == 1]) - mean(Y[Znew == 0])
	return(estate)
}

## Set up and perform the simulation
sims <- 10000
set.seed(12345)
simpleResults <- with(dat1,replicate(sims,simEstAte(Z = Z,y1 = y1,y0 = y0)))
seEstATEsim <- sd(simpleResults)

## The standard error of this estimate of the ATE (via simulation)
seEstATEsim
```
:::
::: {#ch3Stata5 .tabcontent} 
<br />
```{stata, highlight = F, eval = F}
** A program to re-assign treatment and recalculate the difference of means.
** Treatment was assigned without blocking or other structure, so we
** just permute or shuffle the existing treatment assignment vector.
capture program drop simEstAte
program define simEstAte, rclass sortpreserve

	version 18.0
	syntax varlist(min=1 max=1), ///
	  control_outcome(varname) treat_outcome(varname)
	
	qui sum `varlist' // Get # treated units
	local numtreat = r(sum)
	
	tempvar rand // Randomly sort (temporary var)
	qui gen `rand' = runiform()
	sort `rand'
	
	tempvar Znew // New treatment
	qui gen `Znew' = 0
	qui replace `Znew' = 1 in 1/`numtreat'
	
	tempvar Ynew // New revealed outcome
	qui gen `Ynew' = (`Znew' * `treat_outcome') + ((1 - `Znew') * `control_outcome')
	
	qui ttest `Ynew', by(`Znew')
	return scalar estate = r(mu_2) - r(mu_1)
	
end

** Set up and perform the simulation
global sims 10000
set seed 12345
preserve
	qui simulate estate = r(estate), reps($sims): simEstAte z, control_outcome(y0) treat_outcome(y1)
	qui sum estate
	global seEstATEsim = r(sd)
restore

** The standard error of this estimate of the ATE (via simulation)
di "$seEstATEsim"
```
::: 
::: {#ch3Hide5 .tabcontent}
::: 
:::

```{r simsesetup, cache = T, echo = F, eval = T}
## A function to re-assign treatment and recalculate the difference of means.
## Treatment was assigned without blocking or other structure, so we
## just permute or shuffle the existing treatment assignment vector.
simEstAte <- function(Z,y1,y0){
	Znew <- sample(Z)
	Y <- Znew * y1 + (1-Znew) * y0
	estate <- mean(Y[Znew == 1]) - mean(Y[Znew == 0])
	return(estate)
}

## Set up and perform the simulation
sims <- 10000
set.seed(12345)
simpleResults <- with(dat1,replicate(sims,simEstAte(Z = Z,y1 = y1,y0 = y0)))
seEstATEsim <- sd(simpleResults)

## The standard error of this estimate of the ATE (via simulation)
seEstATEsim
```

While this is useful for illustration, we do not need to rely on simulation to develop a design-based standard error estimator. @gerber_field_2012 and @dunning_natural_2012 walk through the following expression for a "feasible" (see below) design based standard error of a simple average treatment effect estimator (e.g., a difference in means based on randomly assigned treatment). If we write $T$ as the set of all $m$ treated units and $C$ as the set of all $n-m$ non-treated units, we then have:

$$
\widehat{SE}(\tau) = \sqrt{\frac{s^2(Y_{i,i \in T})}{m} + \frac{s^2(Y_{i,i \in C})}{(n-m)}}
$$

where $s^2(x)$ is the sample variance such that $s^2(x) = (1/(n-1))\sum^n_{i = 1}(x_i-\bar{x})^2$.

Since this expression is something we can calculate with a real sample, it's called a **feasible standard error**. In contrast, we cannot calculate the *true* design based standard error with a real sample, since it depends on knowing the covariance between each unit's potential outcomes (which is unobservable in real data). However, we *can* calculate this for our example, since we generated the data ourselves. Though we don't write it out here, you can see an expression for the true design based standard error in the code below.

The feasible SE is designed to be *at least as large* as the true standard error on average. This means it is intentionally conservative. To illustrate these different calculation methods, we can compare the results of our simulation above to those of the feasible standard error expression. We can also compare both to the true standard error.

We already have the simulated SE from the code above. Next, let's calculate the true SE. 

```{r oldcode, echo = F, include = F}
# This was used previously to get the true SE, but may be wrong?
#varestATE <- ((N-nt)/(N-1)) * (vart/(N-nt)) + ((N-nc)/(N-1)) * (varc/nc) + (2/(N-1)) * covtc
```

::: {.tab} 
<button class="tablinks" onclick="unrolltab(event, 'ch3R6')">R code</button>
<button class="tablinks" onclick="unrolltab(event, 'ch3Stata6')">Stata code</button>
<button class="tablinks" onclick="unrolltab(event, 'ch3Hide6')">Hide</button>
::: {#ch3R6 .tabcontent} 
<br />
```{r, highlight = F, eval = F}
## True SE (Dunning Chap 6, Gerber and Green Chap 3, or Freedman, Pisani and Purves A-32).
## Requires knowing the true covariance between potential outcomes.
N <- nrow(dat1)
V <- var(cbind(dat1$y0,dat1$y1))
varc <- V[1,1]
vart <- V[2,2]
covtc <- V[1,2]
nt <- sum(dat1$Z)
nc <- N-nt

## Gerber and Green, p.57, equation (3.4)
varestATE <- (((varc * nt) / nc) + ((vart * nc) / nt) + (2 * covtc)) / (N - 1)
seEstATETrue <- sqrt(varestATE)
```
:::
::: {#ch3Stata6 .tabcontent} 
<br />
```{stata, highlight = F, eval = F}
** True SE (Dunning Chap 6, Gerber and Green Chap 3, or Freedman, Pisani and Purves A-32).
** Requires knowing the true covariance between potential outcomes.
qui count
local N = r(N)
qui cor y0 y1, cov
local varc = r(C)[1,1]
local vart = r(C)[2,2]
local covtc = r(C)[1,2]
qui sum z
local nt = r(sum)
local nc = `N' - `nt'

** Gerber and Green, p.57, equation (3.4)
local varestATE = (((`varc' * `nt') / `nc') + ((`vart' * `nc') / `nt') + (2 * `covtc')) / (`N' - 1)
global seEstATETrue = sqrt(`varestATE')
```
::: 
::: {#ch3Hide6 .tabcontent}
::: 
:::

```{r calctruese, echo = F, eval = T}
## True SE (Dunning Chap 6, Gerber and Green Chap 3, or Freedman, Pisani and Purves A-32).
## Requires knowing the true covariance between potential outcomes.
N <- nrow(dat1)
V <- var(cbind(dat1$y0,dat1$y1))
varc <- V[1,1]
vart <- V[2,2]
covtc <- V[1,2]
nt <- sum(dat1$Z)
nc <- N-nt

## Gerber and Green, p.57, equation (3.4)
varestATE <- (((varc * nt) / nc) + ((vart * nc) / nt) + (2 * covtc)) / (N - 1)
seEstATETrue <- sqrt(varestATE)
```

Then, let's calculate the feasible standard error.

::: {.tab} 
<button class="tablinks" onclick="unrolltab(event, 'ch3R7')">R code</button>
<button class="tablinks" onclick="unrolltab(event, 'ch3Stata7')">Stata code</button>
<button class="tablinks" onclick="unrolltab(event, 'ch3Hide7')">Hide</button>
::: {#ch3R7 .tabcontent} 
<br />
```{r, highlight = F, eval = F}
## Feasible SE
varYc <- with(dat1,var(Y[Z == 0]))
varYt <- with(dat1,var(Y[Z == 1]))
fvarestATE <- (N/(N-1)) * ( (varYt/nt) + (varYc/nc) )
estSEEstATE <- sqrt(fvarestATE)
```
:::
::: {#ch3Stata7 .tabcontent} 
<br />
```{stata, highlight = F, eval = F}
** Feasible SE
qui sum y if z == 0
local varYc = r(sd) * r(sd)
qui sum y if z == 1
local varYt = r(sd) * r(sd)
local fvarestATE = (`N'/(`N'-1)) * ( (`varYt'/`nt') + (`varYc'/`nc') )
global estSEEstATE = sqrt(`fvarestATE')
```
::: 
::: {#ch3Hide7 .tabcontent}
::: 
:::

```{r feasibleSE, echo = F, eval = T}
## Feasible SE
varYc <- with(dat1,var(Y[Z == 0]))
varYt <- with(dat1,var(Y[Z == 1]))
fvarestATE <- (N/(N-1)) * ( (varYt/nt) + (varYc/nc) )
estSEEstATE <- sqrt(fvarestATE)
```

Importantly, the feasible SE *is not* the standard error OLS regression provides by default. Among other things, default OLS SES are calculated under an *iid* errors assumption ("identically and independently distributed"). The feasible SE relaxes the "identically" part. Let's record the OLS SE for illustration.

::: {.tab} 
<button class="tablinks" onclick="unrolltab(event, 'ch3R8')">R code</button>
<button class="tablinks" onclick="unrolltab(event, 'ch3Stata8')">Stata code</button>
<button class="tablinks" onclick="unrolltab(event, 'ch3Hide8')">Hide</button>
::: {#ch3R8 .tabcontent} 
<br />
```{r, highlight = F, eval = F}
## OLS SE
lm1 <- lm(Y~Z, data=dat1)
iidSE <-  sqrt(diag(vcov(lm1)))[["Z"]]
```
:::
::: {#ch3Stata8 .tabcontent} 
<br />
```{stata, highlight = F, eval = F}
** OLS SE
qui reg y z
global iidSE = _se[z] // Or: sqrt(e(V)["z","z"])
```
::: 
::: {#ch3Hide8 .tabcontent}
::: 
:::

```{r olsSE, echo = F, eval = T}
## OLS SE
lm1 <- lm(Y~Z, data=dat1)
iidSE <-  sqrt(diag(vcov(lm1)))[["Z"]]
```

Finally, we'll calculate one more alternative called the *HC2 standard error*, which [@lin_agnostic_2013] shows to be a design based SE for treatment effects estimated via OLS regression, potentially including additional control variables (i.e., an alternative to the feasible SE for a simple difference in means above). Like the feasible SE above, HC2 errors should be conservative relative to the true SE. Note that under sampling based inference, the HC2 standard errors can instead be justified as a way of relaxing the assumption of "identically" distributed errors across. In other words, it is robust to a problem called "heteroscedasticity," or "heteroscedasticity-consistent" (hence the HC in its name).

::: {.tab} 
<button class="tablinks" onclick="unrolltab(event, 'ch3R9')">R code</button>
<button class="tablinks" onclick="unrolltab(event, 'ch3Stata9')">Stata code</button>
<button class="tablinks" onclick="unrolltab(event, 'ch3Hide9')">Hide</button>
::: {#ch3R9 .tabcontent} 
<br />
```{r, highlight = F, eval = F}
## HC2
HC2SE <- sqrt(diag(vcovHC(lm1, type = "HC2")))[["Z"]]
```
:::
::: {#ch3Stata9 .tabcontent} 
<br />
```{stata, highlight = F, eval = F}
** HC2
qui reg y z, vce(hc2)
global HC2SE = _se[z] // Or: sqrt(e(V)["z","z"])
```
::: 
::: {#ch3Hide9 .tabcontent}
::: 
:::

```{r NeymanSE, echo = F, eval = T}
## HC2
HC2SE <- sqrt(diag(vcovHC(lm1, type = "HC2")))[["Z"]]
```

All those SE estimates in hand, let's review differences between the true standard error, the feasible standard error, the HC2 (Neyman) standard error, the standard error arising from direct repetition of the experiment, and the OLS standard error.

The HC2 OLS SE is intended to be conservative relative to the true SE (at least as large or larger on average). This is the case in our example. HC2 is similar to the feasible SE outlined above, and both are larger than the true SE. We also illustrate the accuracy of our earlier SE simulation as a way of thinking more intuitively about what the standard error represents. Lastly, recall that our design involves different outcome variances for the treated group and the control group. We would therefore expect what we are calling the OLS IID SE to be at least somewhat inaccurate (different variances across treatment groups means that errors are not "identically" distributed). However, as you can see below, this inaccuracy is still sometimes negligible in practice.

::: {.tab} 
<button class="tablinks" onclick="unrolltab(event, 'ch3R10')">R code</button>
<button class="tablinks" onclick="unrolltab(event, 'ch3Stata10')">Stata code</button>
<button class="tablinks" onclick="unrolltab(event, 'ch3Hide10')">Hide</button>
::: {#ch3R10 .tabcontent} 
<br />
```{r, highlight = F, eval = F}
compareSEs <- c(simSE = seEstATEsim,
  feasibleSE = estSEEstATE,
  trueSE = seEstATETrue,
  olsIIDSE = iidSE,
  HC2SE = HC2SE)
sort(compareSEs)
```
:::
::: {#ch3Stata10 .tabcontent} 
<br />
```{stata, highlight = F, eval = F}
matrix compareSEs = J(1, 5, .)
matrix compareSEs[1, 1] = $seEstATEsim
matrix compareSEs[1, 2] = $estSEEstATE
matrix compareSEs[1, 3] = $seEstATETrue
matrix compareSEs[1, 4] = $iidSE
matrix compareSEs[1, 5] = $HC2SE
matrix colnames compareSEs = "simSE" "feasibleSE" "trueSE" "olsIIDSE" "HC2SE"
matrix list compareSEs
```
::: 
::: {#ch3Hide10 .tabcontent}
::: 
:::

```{r compareSEs2, echo = F, eval = T}
compareSEs <- c(simSE = seEstATEsim,
  feasibleSE = estSEEstATE,
  trueSE = seEstATETrue,
  olsIIDSE = iidSE,
  HC2SE = HC2SE)
sort(compareSEs)
```

To provide a more rigorous comparison of these SE estimation methods, the code chunk below defines a function to calculate an average treatment effect, the OLS iid SE, and the OLS HC2 SE. It then uses this function to calculate SEs for random permutations of treatment 10000 times. Averaging across the simulated estimates provides a better illustration of their relative performance. As expected, the OLS IID SE now *underestimates* the true SE, while the HC2 SE is conservative as intended. The risk of underestimating the SE is that it could lead us to be overconfident when interpreting statistical findings.

::: {.tab} 
<button class="tablinks" onclick="unrolltab(event, 'ch3R11')">R code</button>
<button class="tablinks" onclick="unrolltab(event, 'ch3Stata11')">Stata code</button>
<button class="tablinks" onclick="unrolltab(event, 'ch3Hide11')">Hide</button>
::: {#ch3R11 .tabcontent} 
<br />
```{r, highlight = F, eval = F}
## Define a function to calculate several SEs, given potential outcomes and treatment
sePerfFn <- function(Z,y1,y0){
	Znew <- sample(Z)
	Ynew <- Znew * y1 + (1-Znew) * y0
	lm1 <- lm(Ynew~Znew)
	iidSE <-  sqrt(diag(vcov(lm1)))[["Znew"]]
	NeymanSE <- sqrt(diag(vcovHC(lm1,type = "HC2")))[["Znew"]]
	return(c(estATE=coef(lm1)[["Znew"]],
		 estSEiid=iidSE,
		 estSENeyman=NeymanSE))
}

## Perform a simulation using this function
set.seed(12345)
sePerformance <- with(dat1, replicate(sims, sePerfFn(Z = Z, y1 = y1, y0 = y0)))
ExpectedSEs <- apply(sePerformance[c("estSEiid", "estSENeyman"),], 1, mean)
c(ExpectedSEs, trueSE=seEstATETrue, simSE=sd(sePerformance["estATE",]))
```
:::
::: {#ch3Stata11 .tabcontent} 
<br />
```{stata, highlight = F, eval = F}
** Define a function to calculate several SEs, given potential outcomes and treatment
capture program drop sePerfFn
program define sePerfFn, rclass sortpreserve

	version 18.0
	syntax varlist(min=1 max=1), control_outcome(varname) treat_outcome(varname)
	
	qui sum `varlist' // As in the program above
	local numtreat = r(sum)
	tempvar rand
	qui gen `rand' = runiform()
	sort `rand'
	tempvar Znew
	qui gen `Znew' = 0
	qui replace `Znew' = 1 in 1/`numtreat'
	tempvar Ynew
	qui gen `Ynew' = (`Znew' * `treat_outcome') + ((1 - `Znew') * `control_outcome')
	
	qui reg `Ynew' `Znew' // Regression now instead of ttest
	local iidSE = _se[`Znew']
	qui reg `Ynew' `Znew', vce(hc2)
	local NeymanSE = _se[`Znew']
	
	return scalar iidSE = `iidSE' // Prepare output
	return scalar NeymanSE = `NeymanSE'
	return scalar estATE = _b[`Znew']

end

** Perform a simulation using this function
set seed 12345

preserve
	qui simulate ///
	iidSE = r(iidSE) NeymanSE = r(NeymanSE) estATE = r(estATE), ///
	reps($sims): ///
	sePerfFn z, control_outcome(y0) treat_outcome(y1)
	qui sum estATE
	global simSE = r(sd)
	qui sum iidSE
	global estSEiid = r(mean)
	qui sum NeymanSE
	global estSENeyman = r(mean)
restore

di "Expected IID SE: $estSEiid"
di "Expected Neyman SE: $estSENeyman"
di "SIM SE: $simSE"
di "True SE: $seEstATETrue"
```
::: 
::: {#ch3Hide11 .tabcontent}
::: 
:::

```{r defsefn, cache = T, echo = F, eval = T}
## Define a function to calculate several SEs, given potential outcomes and treatment
sePerfFn <- function(Z,y1,y0){
	Znew <- sample(Z)
	Ynew <- Znew * y1 + (1-Znew) * y0
	lm1 <- lm(Ynew~Znew)
	iidSE <-  sqrt(diag(vcov(lm1)))[["Znew"]]
	NeymanSE <- sqrt(diag(vcovHC(lm1,type = "HC2")))[["Znew"]]
	return(c(estATE=coef(lm1)[["Znew"]],
		 estSEiid=iidSE,
		 estSENeyman=NeymanSE))
}

## Perform a simulation using this function
set.seed(12345)
sePerformance <- with(dat1, replicate(sims, sePerfFn(Z = Z, y1 = y1, y0 = y0)))
ExpectedSEs <- apply(sePerformance[c("estSEiid", "estSENeyman"),], 1, mean)
c(ExpectedSEs, trueSE=seEstATETrue, simSE=sd(sePerformance["estATE",]))
```

###  Randomization-based confidence intervals

When we have a two arm trial (and a relatively large sample size), we can estimate the ATE, calculate design based standard errors, and then use them to create large-sample justified confidence intervals through either of the following approaches:^[Though applications of design based inference often rely on hypothetical permutations of treatment rather than asymptotic (i.e., large-sample) justifications, asymptotic approximations can be used as a basis for design based statistical procedures as well.]

::: {.tab} 
<button class="tablinks" onclick="unrolltab(event, 'ch3R12')">R code</button>
<button class="tablinks" onclick="unrolltab(event, 'ch3Stata12')">Stata code</button>
<button class="tablinks" onclick="unrolltab(event, 'ch3Hide12')">Hide</button>
::: {#ch3R12 .tabcontent} 
<br />
```{r, highlight = F, eval = F}
## The difference_in_means function comes from the estimatr package.
# (design based feasible errors by default)
estAndSE1 <- difference_in_means(Y ~ Z, data = dat1)

## Note that coeftest and coefci come from the lmtest package
# (narrower intervals due to a different d.o.f.)
est2 <- lm(Y ~ Z, data = dat1)
estAndSE2 <- coeftest(est2, vcov.=vcovHC(est2, type = "HC2"))
estAndCI2 <- coefci(est2, vcov.=vcovHC(est2, type = "HC2"), parm = "Z")

## Organize output
out <- rbind( unlist(estAndSE1[c(1,2,6:8)]),  c(estAndSE2[2,-3], estAndCI2) )
out <- apply(out, 2, round, 3)
colnames(out) <- c("Est", "SE", "pvalue", "CI lower", "CI upper")
row.names(out) <- c("Approach 1 (diff. means)", "Approach 2 (OLS)")
out
```
:::
::: {#ch3Stata12 .tabcontent} 
<br />
```{stata, highlight = F, eval = F}
** Organize output
matrix compareCIs = J(2, 5, .)
matrix rownames compareCIs = "Approach 1 (diff. means)" "Approach 2 (OLS)"
matrix colnames compareCIs = "Est" "SE" "pvalue" "CI lower" "CI upper"

** A difference in means test assuming unequal variances
** (equivalent to the design-based estimator, as discussed above)
ttest y, by(z) unequal
local diffmeans = r(mu_2) - r(mu_1)
matrix compareCIs[1,1] = round(`diffmeans', 0.001)
matrix compareCIs[1,2] = round(r(se), 0.001)
matrix compareCIs[1,3] = round(r(p), 0.001)
matrix compareCIs[1,4] = round(`diffmeans' - (invttail(r(df_t), 0.025) * r(se)), 0.001)
matrix compareCIs[1,5] = round(`diffmeans' + (invttail(r(df_t), 0.025) * r(se)), 0.001)

** A regression-based approach (narrower intervals due to a different d.o.f)
reg y z, vce(hc2) // See: "matrix list r(table)"
matrix compareCIs[2,1] = round(r(table)[1, 1], 0.001)
matrix compareCIs[2,2] = round(r(table)[2, 1], 0.001)
matrix compareCIs[2,3] = round(r(table)[4, 1], 0.001)
matrix compareCIs[2,4] = round(r(table)[5, 1], 0.001)
matrix compareCIs[2,5] = round(r(table)[6, 1], 0.001)

matrix list compareCIs
```
::: 
::: {#ch3Hide12 .tabcontent}
::: 
:::

```{r estAndSEs, eval = T, echo = F}
## The difference_in_means function comes from the estimatr package.
estAndSE1 <- difference_in_means(Y ~ Z, data = dat1)

## Note that coeftest and coefci come from the lmtest package
# (narrower intervals due to a different d.o.f.)
est2 <- lm(Y ~ Z, data = dat1)
estAndSE2 <- coeftest(est2, vcov.=vcovHC(est2, type = "HC2"))
estAndCI2 <- coefci(est2, vcov.=vcovHC(est2, type = "HC2"), parm = "Z")

## Organize output
out <- rbind( unlist(estAndSE1[c(1,2,6:8)]),  c(estAndSE2[2,-3], estAndCI2) )
out <- apply(out, 2, round, 3)
colnames(out) <- c("Est", "SE", "pvalue", "CI lower", "CI upper")
row.names(out) <- c("Approach 1 (diff. means)", "Approach 2 (OLS)")
out
```

We can also compute confidence intervals using randomization inference through an entirely simulation-based procedure: test inversion. It proceeds as follows for a simple two-arm trial:

- Choose a "grid" of values to consider, $G$, generally symmetric around 0 (e.g., [-0.5, 0.5]).

- For each $g \in G$, construct a new outcome measure: $y_{i}^{g} = y_{i} - g \times treat_{i}$, and estimate a treatment effect, $\tau_{g}$. Then, calculate a p-value for $\tau_{g}$ using randomization inference:

  - We talk more about calculating p-values using treatment permutation in chapters 4 and 5.
  
  - Briefly: you randomly permute (i.e., re-randomize) treatment, yielding $treat_{i}^{r}$, estimate the treatment effect of interest, $\tau_{r}$, and repeat that process many times. At the end, you calculate the proportion of times the absolute value of $\tau_{r}$ was greater than or equal to the absolute value of $\tau_{g}$: $\frac{1}{n} \sum \mathcal{1}(|\tau_{r}| \geq |\tau_{g}|)$. This is your p-value for a given $g$.
  
- Compare the p-values of each $g \in G$. The highest and lowest values of $g$ that yield p-values greater than (e.g.) 0.05 are our simulated 95% confidence interval.

::: {.tab} 
<button class="tablinks" onclick="unrolltab(event, 'ch3R13')">R code</button>
<button class="tablinks" onclick="unrolltab(event, 'ch3Stata13')">Stata code</button>
<button class="tablinks" onclick="unrolltab(event, 'ch3Hide13')">Hide</button>
::: {#ch3R13 .tabcontent} 
<br />
```{r, highlight = F, eval = F}
## Define the grid to search through
grid <- seq(-10, 10, 0.05)

## Loop through values in this grid
res <- matrix(NA, length(grid), 2)
i <- 0
for (g in grid) {
  
  ## Update loop index
  i <- i + 1
  
  ## Create outcome for this g
  gdat <- dat1
  gdat$yg <- gdat$Y - g*gdat$Z
  
  ## "Real" treatment effect for this g
  mod <- lm(yg ~ Z, data = gdat)
  
  ## p-value through randomization inference
  ridraws <- lapply(
    1:500,
    function(.x) {
      gdat$Zri <- sample(gdat$Z, length(gdat$Z), replace = F)
      lm(yg ~ Zri, data = gdat)$coefficients[2]
      }
    )
  ridraws <- do.call(c, ridraws)
  res[i,1] <- g
  res[i,2] <- mean(abs(ridraws) >= abs(mod$coefficients[2]))
  
}

## Compute CI from results
res <- res[ res[,2] > 0.05, ]
c( min(res[,1]), max(res[,1]) )
```
:::
::: {#ch3Stata13 .tabcontent} 
<br />
```{stata, highlight = F, eval = F}
** Define the grid to search through
numlist "-10(0.05)10" 
local grid `r(numlist)'
macro list _grid

** Simple RI program to use here
capture program drop ri_p
program define ri_p, rclass
	capture drop Zri
	qui complete_ra Zri, m(25)
	qui reg yg Zri
	return scalar taur = _b[Zri]
end

** Loop through values in this grid
tempfile realdat
save `realdat', replace
local i = 0
foreach g of local grid {
	
	local ++i
	
	** Create outcome for this g
	use `realdat', clear
	qui gen yg = y - `g' * z
	
	** Real regression for this g
	qui reg yg z
	local taug = _b[z]
	
	** RI inference for this g
	qui simulate taur = r(taur), ///
	reps(500) : ///
	ri_p
	replace taur = abs(taur) >= abs(`taug')
	collapse (mean) taur
	qui gen g = `g'
	
	** Save results
	if `i' == 1 {
		qui tempfile running
		qui save `running'
	}
	else {
		append using `running'
		qui save `running', replace
	}

	** Return to original data
	use `realdat', clear
	
}

** Load the results
use `running', clear

** Compute the CI
keep if taur > 0.05
qui sum g
global lower = r(min)
global upper = r(max)
di "$lower, $upper"
```
::: 
::: {#ch3Hide13 .tabcontent}
::: 
:::

```{r, eval = T, echo = F, cache = T}
## Define the grid to search through
grid <- seq(-10, 10, 0.05)

## Loop through values in this grid
res <- matrix(NA, length(grid), 2)
i <- 0
for (g in grid) {
  
  ## Update loop index
  i <- i + 1
  
  ## Create outcome for this g
  gdat <- dat1
  gdat$yg <- gdat$Y - g*gdat$Z
  
  ## "Real" treatment effect for this g
  mod <- lm(yg ~ Z, data = gdat)
  
  ## p-value through randomization inference
  ridraws <- lapply(
    1:500,
    function(.x) {
      gdat$Zri <- sample(gdat$Z, length(gdat$Z), replace = F)
      lm(yg ~ Zri, data = gdat)$coefficients[2]
      }
    )
  ridraws <- do.call(c, ridraws)
  res[i,1] <- g
  res[i,2] <- mean(abs(ridraws) >= abs(mod$coefficients[2]))
  
}

## Compute CI from results
res <- res[ res[,2] > 0.05, ]
c( min(res[,1]), max(res[,1]) )
```

## Summary: What does a design based approach mean for policy evaluation?

Let's review some important terms. Hypothesis tests produce $p$-values telling us how much information we have against a null hypothesis. Estimators produce guesses about the size of some causal effect like the average treatment effect (i.e., "estimates"). Standard errors summarize how our estimates might vary from experiment to experiment by random chance, though we can only observe one experiment in practice. Confidence intervals tell us which ranges of null hypotheses are more versus less consistent with our data.

In the frequentist approach to probability (the only approach we consider in this SOP), the properties of both $p$-values and standard errors arise from some process of repetition. Statistics textbooks often encourage us to imagine that this process of repetition involves repeated sampling from a larger (potentially infinite) population. But most OES work involves a pool of people who do not represent a well-defined population, nor do we tend to have a strong probability model of how these people and not others entered our sample. Instead, we have a known process of random assignment to an experimental intervention within a fixed sample. This often makes a randomization based approach to inference natural for our work, and helps our work be easiest to explain and interepret for our policy partners.
