

# Basics of Experimental Design and Analysis

Here, we briefly define and describe some of the characteristics we consider when choosing statistical procedures. This will guide our decision making in the rest of this guide. Briefly, we want to create research designs that (1) provide a reasonably precise estimate of a policy intervention's impacts. We also want to use statistical tests that will (2) rarely mislead us--- i.e., will rarely give a false positive result--- and we want to estimate policy impacts (3) without bias (i.e., without systematically over-estimating or under-estimating). These characteristics depend on both the design of the study and the choices we make when computing results. We discuss these issues in more depth in other chapters that follow.

## (1) Statistical power: Designing Studies that effectively distinguish signal from noise

The research designs we use at OES aim to enhance our ability to
distinguish signal from random noise: estimates from studies with very few observations are subject to more random noise and therefore cannot tell us much about the treatment effect. In contrast, studies with very many observations are subject to less noise and provide a lot of information about the treatment effect. A study which effectively distinguishes signal from noise has excellent "statistical power," while a study which cannot do this has low statistical power. The Evidence in Governance and Politics (EGAP) Methods Guide [10 Things You Need to Know about Statistical Power](https://egap.org/resource/10-things-to-know-about-hypothesis-testing/) explains more about what statistical power is and how to assess it.

Before we field a research design, we assess its statistical power. If we
anticipate that the intervention will only make a small change in peoples'
behavior, then we will need a relatively large number of people in the study to ensure we can detect relatively small changes: too few people would result in a report saying something like, "The new policy might have improved the lives of participants, but we can't argue strongly that this is so because our margin of error is too wide."

## (2) Error Rates of Tests

A good statistical test rarely rejects a true hypothesis and often rejects
false hypotheses. The EGAP Methods Guide [10 Things to Know about Hypothesis
Testing](https://egap.org/resource/10-things-to-know-about-hypothesis-testing/) describes more about the basics of hypothesis tests. It also explains how one might know that a given $p$-value arises from a test with good properties in a given research design. Our team tries to choose testing procedures that are not likely to mislead analysts, both when we make our analysis plans and as we complete our analyses and re-analyses.

## (3) Bias in Estimators

While (1) above focuses on the problem of random noise, we also need to think about whether an estimator produces results that are *consistently* too high or too low ("biased"). A good estimator is not systematically different from the truth, either in general or in a particular application. The difference-in-means between a randomly assigned treatment and control group is a well known unbiased estimator of the sample average treatment effect, so this is often a primary quantity of interest that our team reports. In practice, we usually estimate this using linear regression. Even when dealing with binary outcomes, we still tend to prefer linear regression over, e.g., logistic (Logit) regression. This is due to concerns about the additional assumptions Logit regression might require [@gomila2021logistic; @freedman2008randomization] along with linear regression's ease of interpretation and well-established properties as an average treatment effect estimator [@angrist2009mostly; @aronow2019foundations].
