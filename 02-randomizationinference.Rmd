# Design-Based Principles of Statistical Inference

Most policy evaluations using administrative data or surveys report the results of their studies using estimators and tests. Although we can never know the true causal effect of a new policy on our beneficiaries, we can provide a best guess ("The average amount saved for retirement by people in the treatment group was $1000 more than the average amount in the control group: our estimate of the average treatment effect is $1000") and we can provide a test evaluating the plausibility of a hunch or hypothesis, such as the common null hypothesis of *no effect* ("We can reject the null hypothesis at the 5% significance level with $p=.02$"). Confidence intervals summarize hypothesis tests, so we think of them as tests rather than estimators.

Now, when we are asked *why* we used some method for calculating an
average treatment effect or a $p$-value or a confidence interval, our team has
tended to say that our statistical analyses depend on the **design** of our
studies. When applied to randomized experiments, this principle can be written
simply as: **analyze as you randomize.** We provide an example of this
principle in practice [below](#randinfex). This idea, often known as
"randomization based" or "design based" inference, was proposed by two of the
founders of modern statistics. Jerzy Neyman's 1923 paper showed how to use
randomization to learn about what we would currently call "average treatment
effects" [@neyman_application_1923], and Ronald A.  Fisher's 1935 book showed
how to use randomization to test hypotheses about what we would currently call
"treatment effects" [@fisher_design_1935].

We use a design based approach because we often know how a study was designed
---after all, we and our agency collaborators tend to be the ones deciding on
the sample size, the experimental arms, and the outcome data to be extracted
from administrative databases. There are other ways to justify statistical
procedures, and we do not exclude any reasonable approach in our work--- such
as approaches based on theoretical probability models, or asymptotic theory. However, building on what we know we did in a given study has served us well so far, and it thus forms the basis of our decisions in general.

## An example using simulated data {#randinfex}

Imagine we have a simple randomized experiment where the relationship between
outcomes and treatment is shown in Figure \@ref(fig:boxplot1) (we illustrate the first 6 rows in the table below). Notice that, in this simulated experiment, the treatment changes the variability in the outcome in the treated group--- this is a common pattern when the control group is status quo.

```{r makedat1}
## Read in data for the fake experiment.
dat1 <- read.csv("dat1.csv")

## Table of the first few observations.
knitr::kable(head(dat1[, c(1, 23:27)]))
```

```{r boxplot1, fig.cap="Simulated Experimental Outcomes"}
## y0 and y1 are the true underlying potential outcomes.
with(dat1,{boxplot(list(y0,y1),names=c("Control","Treatment"),ylab="Outcomes")
stripchart(list(y0,y1),add=TRUE,vertical=TRUE)
stripchart(list(mean(y0),mean(y1)),add=TRUE,vertical=TRUE,pch=19,cex=2)})
```

In this simulated data, we know the true average treatment effect (ATE) because we know both of the underlying true potential outcomes. The control potential outcome, $y_{i|Z_i=0}$, is written in the code as `y0`, meaning "the response person $i$ would provide if he/she were in the status quo or control group". The treatment potential outcome,  $y_{i|Z_i = 1}$, is written in the code as `y1`, meaing "the response person $i$ would provide if he/she were in the new policy or treatment group." We use $Z_i$ to refer to the experimental arm. In this case $Z_i=0$ for people in the status quo group and $Z_i=1$ for people in the new policy group. (You can click to SHOW the code.)

```{r trueATE}
trueATE <- with(dat1, mean(y1) - mean(y0))
trueATE
```

Now, we have one *realized* experiment (defined by randomly assigning half of the people to treatment and half to control). We know that the observed difference of means of the outcome, $Y$, between treated and control groups is an unbiased estimator of the true ATE (by virtue of random assignment to treatment). We can calculate this in a few ways: we can just calculate the difference of means, *or* we can take advantage of the fact that an ordinary least squares linear regression produces the same estimate when we have a binary treatment on the right hand side.

```{r estATE, echo=TRUE}
## Y is the observed outcome, Z is the observed treatment.
estATE1 <- with(dat1, mean(Y[Z==1]) - mean(Y[Z==0]))
estATE2 <- lm(Y~Z,data=dat1)$coef[["Z"]]
c(estimatedATEv1=estATE1,estimatedATEv2=estATE2)
stopifnot(all.equal(estATE1,estATE2))
```

This design-based perspective leads us to think differently about how to calculate standard errors (and thus $p$-values and confidence intervals), relative to a more common perspective based on asymptotic theory, or assumptions about an "infinite population."

###  How do we calculate randomization-based standard errors?

How would an estimate of the average treatment effect vary if we repeated an
experiment on the same group of people multiple times (randomly re-assigning treatment)? The **standard error** of an estimate of the average treatment effect is one answer to this question. Below, we simulate a simple, individual-level experiment to develop intuition about what the standard error is.^[See https://egap.org/resource/10-types-of-treatment-effect-you-should-know-about/ for a demonstration that the difference of means in the observed treatment and control groups is an unbiased estimator of the average treatment effect itself, and what it means to be unbiased.]

```{r simsesetup}
## A function to re-assign treatment and recalculate the difference of means.
## Treatment was assigned without blocking or other structure, so we
## just permute or shuffle the existing treatment assignment vector.
simEstAte <- function(Z,y1,y0){
	Znew <- sample(Z)
	Y <- Znew * y1 + (1-Znew) * y0
	estate <- mean(Y[Znew == 1]) - mean(Y[Znew == 0])
	return(estate)
}
```

```{r simse, cache=TRUE}
sims <- 10000
set.seed(12345)
simpleResults <- with(dat1,replicate(sims,simEstAte(Z = Z,y1 = y1,y0 = y0)))
seEstATEsim <- sd(simpleResults)

## The standard error of this estimate of the ATE (via simulation)
seEstATEsim
```

Although this preceding standard error is intuitive (the standard deviation of the distribution arising from repeating the experiment), more statistics-savvy readers will recognize closed-form expressions for the standard error like the following.^[See @gerber_field_2012 and
@dunning_natural_2012 for easy-to-read explanations and derivations of this
design-based expression for the standard error of the simple estimator of the average treatment effect.] If we write $T$ as the set of all $m$ treated units and $C$ as the set of all $n-m$ non-treated units, we then have:

$$
\widehat{Var}(\hat{T}) = \frac{s^2(Y_{i,i \in T})}{m} + \frac{s^2(Y_{i,i \in C})}{(n-m)}
$$

where $s^2(x)$ is the sample variance such that $s^2(x) = (1/(n-1))\sum^n_{i = 1}(x_i-\bar{x})^2$.

Let's compare the results of that simulation to the standard error this expression produces, along with the *true* standard error. We can calculate the true standard error of the estimated ATE in this example because we know the actual covariance between potential outcomes (normally we cannot observe this). You can think of the expression above as a *feasible* standard error, a modified version that is estimable in real datasets and designed to be *at least as large* as the true standard error on average (i.e., conservative). Although we don't formally write it out here, you can see a method of estimating the true standard error in our code.

Among other things, this exercise helps illustrate that the "standard deviation of the estimated ATE after repeating the experiment" is the same as expressions like the one above that textbooks are more likely to teach. We'll calculate the true SE next.

```{r oldcode, echo = F, include = F}
# This was used previously to get the true SE, but may be wrong?
#varestATE <- ((N-nt)/(N-1)) * (vart/(N-nt)) + ((N-nc)/(N-1)) * (varc/nc) + (2/(N-1)) * covtc
```

```{r calctruese}
## True SE (Dunning Chap 6, Gerber and Green Chap 3, and Freedman, Pisani and Purves A-32).
## Requires knowing the true covariance between the potential outcomes.
N <- nrow(dat1)
V <- var(cbind(dat1$y0,dat1$y1))
varc <- V[1,1]
vart <- V[2,2]
covtc <- V[1,2]
nt <- sum(dat1$Z)
nc <- N-nt

## Gerber and Green, p.57, equation (3.4).
varestATE <- (((varc * nt) / nc) + ((vart * nc) / nt) + (2 * covtc)) / (N - 1)
seEstATETrue <- sqrt(varestATE)
```

Now, we'll calculate the feasible standard error represented in the expression above.

```{r feasibleSE}
## Feasible SE
varYc <- with(dat1,var(Y[Z == 0]))
varYt <- with(dat1,var(Y[Z == 1]))
fvarestATE <- (N/(N-1)) * ( (varYt/nt) + (varYc/nc) )
estSEEstATE <- sqrt(fvarestATE)
```

Note that this feasible SE *is not* the standard error OLS provides by default, if you use a standard OLS regression to calculate a difference-in-means. We'll calculate the OLS SE as well, and include it in our comparison for illustration.

```{r olsSE}
## OLS SE
lm1 <- lm(Y~Z,data=dat1)
iidSE <-  sqrt(diag(vcov(lm1)))[["Z"]]
```

Finally, we'll calculate the HC2 standard error, which [@lin_agnostic_2013] shows to be the randomization-justified SE for OLS. From our design-based perspective, we prefer this method of estimating the standard error. Like the feasible SE above, it should be conservative relative to the true SE. 

```{r NeymanSE}
## Neyman SE (HC2).
## Worth noting that if we had covariates in the model we would want this one
## (which is identical to the previous one without covariates).
NeymanSE <- sqrt(diag(vcovHC(lm1,type = "HC2")))[["Z"]]
```

Now, we'll review differences between the true standard error, the feasible standard error, the HC2 SE (which is the same as the feasible standard error), the standard error arising from direct repetition of the experiment, and the OLS standard error.

What we call Neyman SE here (the HC2 OLS SE) is supposed to be conservative relative to the true SE (at least as large or larger on average). Below, we show this to be the case. The Neyman SE is, by design, similar to the feasible SE, and both are larger than the true SE. We also show the accuracy of our SE simulation procedure above as a way of thinking intuitively about what the standard error represents (though such a simulation is generally not possible with real data). Finally, recall that our design involves different outcome variances between the treated group and the control group. We would therefore expect what we are calling the "OLD iid" SE to be biased on average (though not necessarily guaranteed to be overly conservative or liberal in all cases). Here, it underestimates the truth only slightly, but in other circumstances it may perform worse.

```{r compareSEs2}
compareSEs <- c(simSE = seEstATEsim,
  feasibleSE = estSEEstATE,
  trueSE = seEstATETrue,
  olsIIDSE = iidSE,
  NeymanDesignSE = NeymanSE)
sort(compareSEs)
```

The first code chunk below defines a function to calculate an average treatment effect, OLS iid SE, and OLS HC2 (Neyman) SE, given a vector of treatment assignments, a vector of treatment potential outcomes, and a vector of control potential outcomes. The second code chunk below uses this function to calculate several of those SEs for 10000 simulated datasets, averaging across them to provide a better illustration of their relative performance.

```{r defsefn}
sePerfFn <- function(Z,y1,y0){
	Znew <- sample(Z)
	Ynew <- Znew * y1 + (1-Znew) * y0
	lm1 <- lm(Ynew~Znew)
	iidSE <-  sqrt(diag(vcov(lm1)))[["Znew"]]
	NeymanSE <- sqrt(diag(vcovHC(lm1,type = "HC2")))[["Znew"]]
	return(c(estATE=coef(lm1)[["Znew"]],
		 estSEiid=iidSE,
		 estSENeyman=NeymanSE))
}
```

```{r seperformance, cache=TRUE}
set.seed(12345)
sePerformance <- with(dat1,replicate(sims,sePerfFn(Z = Z,y1 = y1,y0 = y0)))
ExpectedSEs <- apply(sePerformance[c("estSEiid","estSENeyman"),],1,mean)
c(ExpectedSEs,trueSE=seEstATETrue,simSE=sd(sePerformance["estATE",]))
```

###  How do we calculate randomization-based confidence intervals?

When we have a two arm trial (and a relatively large sample size), we can estimate the ATE, calculate design-based standard errors, and then use them to create large-sample justified confidence intervals through either of the following approaches:

```{r estAndSEs}
## The difference_in_means function comes from the estimatr package.
estAndSE1 <- difference_in_means(Y ~ Z, data = dat1)
## coeftest and coefci come from the lmtest package.
est2 <- lm(Y ~ Z, data = dat1)
estAndSE2 <- coeftest(est2, vcov.=vcovHC(est2, type = "HC2"))
estAndCI2<- coefci(est2, vcov.=vcovHC(est2, type = "HC2"), parm = "Z")

estAndSE1
estAndSE2["Z", , drop=FALSE]
estAndCI2
```

## Summary: What does a design based approach mean for policy evaluation?

Let's review some important terms. Hypothesis tests produce $p$-values telling us how much information we have against a null hypothesis. Estimators produce estimates --- guesses about some causal effect like the average treatment effect. Standard errors summarize how our estimates might vary from experiment to experiment by random chance. Confidence intervals tell us which ranges of null hypotheses are more versus less consistent with our data.

Recall that $p$-values refer to probability distributions of test statistics under a null hypothesis, while standard errors refer to probability distributions of estimators across repeated experiments. In the frequentist approach to probability, both of these probability distributions arise from some process of repetition. Statistics textbooks often encourage us to imagine that this process of repetition involves repeated sampling from a population, or even a hypothetical (infinite) super-population. But most OES work involves a pool of people who do not represent a well-defined population, nor do we tend to have a strong probability model of how these people entered our sample. Instead, we have a known process of random assignment to an experimental intervention. This makes a randomization-based inference approach natural for our work, and helps our work be easiest to explain and interepret for our policy partners.
