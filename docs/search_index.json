[
["index.html", "OES Standard Operating Procedures for The Design and Statistical Analysis of Experiments Overview Purposes of this document Nature and limitations of this document Structure Help us improve our work! Technical details", " OES Standard Operating Procedures for The Design and Statistical Analysis of Experiments Jake Bowers, Ryan T. Moore, Miles Williams, and Bill Schultz October 01, 2024 Overview This document explains how our team, the Office of Evaluation Sciences in the General Services Administration (the OES), tends to do statistical analysis. It also explains why we do what we do.1 The research integrity processes OES follows are already documented on our Evaluation Resources Web Page. For example, on that page we provide templates for our research design and analysis pre-registration process. Here, we instead get into the nitty gritty of our statistical work. Purposes of this document First, this document educates new team members about the decisions past team members have made regarding research design and analysis. It also serves as a place to record decisions for our own future selves, and helps us harness the power that arises from our disciplinary diversity. That is, current and past team members have made decisions about how to approach statistical analyses that may differ from those that are common in any given academic discipline. This document, thus, helps explain why we have landed on those decisions (for now), and also illustrates how to implement them. Second, this document records decisions that we have made in the absence of pre-analysis plans, or in the context of circumstances unforeseen by our pre-analysis planning. Projects will sometimes encounter good reasons to make different decisions than those we describe here. But the SOP represents our methodological thinking in &#x201C;all else equal&#x201D; situations. Third, on a related note, this document should help us write better analysis plans and speed our practice of re-analysis. (Our team insists on a blind re-analysis of every study as a quality control for our results before they are reported to our agency partners.) Fourth, and finally, this document should help other teams working to learn about the causal impacts of policy interventions. We hope this contributes to the US government&#x2019;s own work pursuing evidence-based public policy. But we also hope that it helps teams doing work similar to our own in other settings. Nature and limitations of this document We (mostly) focus on randomized field experiments. This document focuses on design and analysis of randomized field experiments. Although we include some discussion about non-randomized studies, often known as observational studies, until now, our team has focused primarily on randomized field experiments. We plan to include more discussion of observational studies as we pursue more in the future. We (mostly) present examples using R We use the R statistical analysis language in this document because it is (a) one of the two industry standards in the field of data science (along with Python), (b) free, open source, and multiplatform, and (c) the locus of development for many of the latest new statistical techniques for social and behavioral scientists. Of course, members of our team also use other software like Stata, SAS, SPSS, and Python. To help improve accessibility to Stata users in particular, almost all of the R code in this SOP is accompanied by code showing how the same task, or something similar, could be accomplished in Stata. But reported results and figures are generated based only on the R code. Structure This page provides a basic introduction to what the OES SOP hopes to accomplish. The rest of the document can be thought of as consisting of two parts: For policymakers and agency partners - Chapters 1-3 provide a higher level overview of how statistical tests can inform policy learning, our priorities when designing tests, and how we prefer to justify the tests we use. This provides more context for how OES make analysis decisions and what we aim to learn from impact evaluations. Chapter 3 goes into more technical detail than Chapters 1 and 2, overlapping with the next part of this document. For OES team members - Chapters 3 and onward are intended to serve as a reference for design and analysis decisions that need to be made at stages of our project process. Chapter 3 reviews a randomization based framework for statistical decision-making that motivates many of the recommendations in later chapters. Chapter 4 is for teams making decisions about how to randomly assignment treatment or ensure that randomization occurred as planned. Chapter 5 is for teams making decisions about how to analyze their data. Finally, Chapter 6 is for teams interested in performing ex ante power simulations before making design choices. Help us improve our work! Since we hope to improve our analytic workflow with every project, this document should be seen as provisional &#x2014; as a record and a guide for our continuous learning and improvement. We invite comments in the form of submissions to our Google Form (only for OES team members), or as Issues or pull requests on Github. Technical details This book was written in bookdown. The complete source is available from GitHub. This version of the book was built with R version 4.2.1 (2022-06-23 ucrt) and the following packages. package version source blockTools 0.6.4 CRAN (R 4.2.3) bookdown 0.7 CRAN (R 4.2.1) coin 1.4-2 CRAN (R 4.2.1) DeclareDesign 1.0.0 CRAN (R 4.2.1) devtools 2.4.4 CRAN (R 4.2.1) estimatr 1.0.0 CRAN (R 4.2.1) fabricatr 1.0.0 CRAN (R 4.2.1) foreach 1.5.2 CRAN (R 4.2.1) future 1.28.0 CRAN (R 4.2.1) future.apply 1.9.1 CRAN (R 4.2.1) here 1.0.1 CRAN (R 4.2.2) ICC 2.4.0 CRAN (R 4.2.0) kableExtra 1.3.4 CRAN (R 4.2.1) katex 1.4.1 https://ropensci.r-universe.dev (R 4.2.3) klippy 0.0.0.9500 Github (rlesur/klippy@378c247fbbc76ec662f6c1ed1103121b87091be4) knitr 1.33 CRAN (R 4.2.1) lmtest 0.9-40 CRAN (R 4.2.1) multcomp 1.4-20 CRAN (R 4.2.1) nbpMatching 1.5.1 CRAN (R 4.2.3) quickblock 0.2.0 CRAN (R 4.2.1) randomizr 0.22.0 CRAN (R 4.2.1) remotes 2.4.2 CRAN (R 4.2.1) ri2 0.4.0 CRAN (R 4.2.2) sandwich 3.1-0 RSPM (R 4.2.0) tidyverse 1.3.2 CRAN (R 4.2.2) V8 4.3.3 CRAN (R 4.2.3) withr 2.5.0 CRAN (R 4.2.1) "],
["using-tests-to-inform-policy.html", "Chapter 1 Using tests to inform policy", " Chapter 1 Using tests to inform policy Most of this SOP dives into our statistical decision making. It assumes that the reader has heard of hypothesis tests and statistical estimators. Here, we first explain in broad terms how tests and estimators are useful for helping the US federal government improve public policy. &#x201C;Evidence-based public policy&#x201D; can refer to both &#x201C;evidence-as-insight&#x201D; (the use of previous scientific literature as input to the design of new policies) and &#x201C;evidence-as-evaluation&#x201D; (the careful design of studies to learn about how and whether a new policy worked). For more on this distinction, see Bowers and Testa (2019). Our team aims to help government agencies both design new policies and learn about how well those new ideas work. The OES SOP focuses on the learning part of our work. How would we know whether and how a new policy worked? In an ideal and unrealistic case, we would know that a new policy improved the life of a single person, John, if we could compare his decisions both under the new policy and under the status quo at the same moment in time (i.e., if we could observe two different realities at once). If we saw that John&#x2019;s decisions were better under the new policy than under the status quo, we would say that the new policy caused John to make better decisions. Since no one can observe John in both situations &#x2014; say, making health decisions with and without a new procedure for visiting the doctor &#x2014; researchers try to find other people who represent how John would have acted without being exposed to the new policy. Holland (1986) calls this the &#x201C;fundamental problem of causal inference&#x201D; and explains more formally when we might believe that other people are a good approximations for how John would have acted without the new policy. For example, if access to the new policy is randomized, it is easier to argue that that the two groups (access vs no access) are good &#x201C;counterfactuals&#x201D; for each other. Our team tends to think about the causal effects of a policy in counterfactual terms. This in mind, we often use randomized experiments to create groups of people who represent behavior under both the new policy and the status quo. In medical experiments, these two groups tend to be called the &#x201C;treatment group&#x201D; and the &#x201C;control group.&#x201D; Social scientists often use that same language even if we are not really providing a new treatment, but are, instead, offering a new communication or decision-making structure. If we pilot a new policy by offering it to people chosen at random, we can use what we see from one group to learn about what would likely have happened if the control group had received the new policy instead, or if the treatment group had not received it. In any given sample, we won&#x2019;t know exactly how the treatment group would have behaved if they had been assigned to the control group instead. For example, imagine we pulled 500 out of 1000 names from a hat and assigned those 500 people to receive treatment. Any &#x201C;treated&#x201D; sample we observe is just one of many possible sets of 500 people we could have drawn. If we were to do the experiment again, and pulled a different 500 names at random, this second experiment will also have a randomly selected treatment group, but the second 500 people will be at least a little different from the first 500 people. It is important to ask questions about how this influences our findings (&#x201C;statistical uncertainty&#x201D;). How much could our result differ just due to pulling a different 500 people from the hat rather than due to a real treatment effect? Our team draws on statistical theory to estimate the causal effect of a new policy and its associated statistical uncertainty. We also use statistical theory to answer questions like &#x201C;Is this effect meaningfully distinguishable from zero?&#x201D; or &#x201C;How many people do we need to observe in order to distinguish a positive effect from a zero effect?&#x201D; The rest of this document presents decisions we have made about the particulars of estimators and tests, as well as other tricky decisions that we have had to confront&#x2014;like how we can learn from pre-intervention data to design experiments that provide more precise results. For more on the basics of how statistics helps us answer questions about causal effects, we recommend chapters 1&#x2013;3 of Gerber and Green (2012) (which focuses on randomized experiments) and the first part of Rosenbaum (2017) (which focuses on both experiments and research designs without randomization). Another good treatment comes from the opening chapters of Angrist and Pischke (2009). References "],
["key-design-criteria.html", "Chapter 2 Key design criteria 2.1 High statistical power 2.2 Controlled error rates 2.3 Unbiased estimators", " Chapter 2 Key design criteria Here, we briefly define and describe some of the things we prioritize when choosing statistical procedures or designing evaluations. Briefly, we want to observe a reasonably precise estimate of a policy intervention&#x2019;s impacts (i.e., a small margin of error). We also want to use statistical tests that will rarely mislead us&#x2014;i.e., will rarely give a false positive result&#x2014;and we want to estimate policy impacts without bias (i.e., without systematically over-estimating or under-estimating). These properties depend on both the initial design of the study and the choices we make when calculating results. 2.1 High statistical power The research designs we use at OES aim to enhance our ability to distinguish signal from random noise: estimates from studies with very few observations are subject to more random noise and therefore cannot tell us much about the treatment effect. In contrast, studies with very many observations are subject to less noise and provide a lot of information about the treatment effect. A study which effectively distinguishes signal from noise has excellent statistical power, while a study which cannot do this has low statistical power. The Evidence in Governance and Politics (EGAP) Methods Guide 10 Things You Need to Know about Statistical Power explains more about what statistical power is and how to assess it. Before we field a research design, we assess its statistical power. If we anticipate that the intervention will only make a small change in peoples&#x2019; behavior, then we will need a relatively large number of people in the study: too few people would result in a report saying something like, &#x201C;The new policy might have improved the lives of participants, but we can&#x2019;t argue strongly that this is so because our margin of error is too wide.&#x201D; 2.2 Controlled error rates A good statistical test rarely rejects a true hypothesis and often rejects false hypotheses. For us, the relevant hypothesis for error rate control is the null hypothesis of no treatment effect. It is a norm in many applied fields to design tests so that, over the long run (i.e., across many studies), the rate of rejecting a true null hypothesis will be no more than 5%, while the rate of rejecting a false null hypothesis will be at least 80%.2 The EGAP Methods Guide 10 Things to Know about Hypothesis Testing describes more about the basics of hypothesis tests. Our team tries to choose testing procedures that are not likely to mislead analysts (i.e., adequately control both error rates), both when we make our analysis plans and as we complete our analyses and re-analyses. 2.3 Unbiased estimators While the other criteria focus on the problem of random noise, we also need to think about whether an estimator produces results that are consistently wrong in a particular direction. A good estimation procedure (i.e., a method of estimating a treatment effect) should not systematically over-estimate or under-estimate the truth&#x2014;i.e., it should be unbiased. The difference-in-means between a randomly assigned treatment and control group is well known to be an unbiased estimator of the sample average treatment effect, so this is often a primary quantity of interest that our team reports. In practice, we usually estimate this using a statistical procedure called linear regression. Even when dealing with binary outcomes, we still tend to prefer linear regression over other statistical models designed for binary outcomes such as logistic (Logit) regression. This is partly due to concerns about the additional assumptions Logit regression might require when interpreting results (Gomila 2021; Freedman 2008b), and to linear regression&#x2019;s ease of interpretation and well-established properties as an average treatment effect estimator (Angrist and Pischke 2009; Aronow and Miller 2019). That said, we do consider alternative models or estimators in situations where we think they will significantly out perform linear regression. References "],
["design-based-inference.html", "Chapter 3 Design based inference 3.1 An example using simulated data 3.2 Summary: What does a design based approach mean for policy evaluation?", " Chapter 3 Design based inference Most policy evaluations using administrative data or surveys report the results of their studies using estimators and hypothesis tests. We briefly define each for clarity. Estimators: Although we can never know the true causal effect of a new policy on our beneficiaries (see Chapter 1), we can choose a method that provides us a best guess. E.g.: &#x201C;The average amount saved for retirement by people in the treatment group was $1000 more than the average amount in the control group, so our estimate of the average treatment effect is $1000.&#x201D; Hypothesis tests: We can also test the plausibility of a particular hunch or hypothesis. Commonly, these tests focus on plausibility of the null hypothesis of no effect. E.g.: &#x201C;We can reject the null hypothesis of no effect at a 5%5\\%5% significance level given an estimated p-value of 2%2\\%2%.&#x201D; Confidence intervals can be used to summarize hypothesis tests, so we think of them as tests rather than estimators. When we are asked why we used some method for calculating an average treatment effect, ppp-value, or confidence interval, we often say that our statistical analysis choices are based on the design of our studies. When applied to randomized experiments, this principle can be written simply as: &#x201C;analyze as you randomize.&#x201D; We provide an example of this principle in practice below, and the intuition behind it is to model unexplained variation in the data as noise introduced by random treatment assignment. This idea, often known as randomization based or design based inference, was proposed by two of the founders of modern statistics. Jerzy Neyman&#x2019;s 1923 paper showed how to use randomization to learn about what we would currently call &#x201C;average treatment effects&#x201D; (Neyman 1923), and Ronald A. Fisher&#x2019;s 1935 book showed how to use randomization to test hypotheses about what we would currently call &#x201C;treatment effects&#x201D; (Fisher 1935). We favor design based justifications because we know exactly how a study was designed&#x2014;after all, we and our agency collaborators chose the sample size, experimental arms, and outcome data. Of course, there are other ways to justify statistical procedures, too. We don&#x2019;t exclude any reasonable approach in our work. It is standard for researchers in many applied fields to instead motivate their tests using the idea of sampling from a larger (possibly infinite) population. This often depends on theoretical models of an estimator&#x2019;s behavior as sample size increases indefinitely (&#x201C;asymptotic&#x201D; theory). OES projects frequently employ sampling-justified procedures as well. Moreover, some common procedures like the calculation of HC2 standard errors (discussed below) can be justified under either design based or sampling based statistical inference. 3.1 An example using simulated data Imagine we have a simple randomized experiment where the relationship between outcomes and treatment is shown in Figure 3.1 (we illustrate the first 6 rows in the table below). Notice that, in this simulated experiment, the treatment changes the variability of the outcome in the treated group &#x2014; this is a common pattern when the control group is a status quo policy. R code Stata code Hide ## Read in data for the fake experiment. dat1 &lt;- read.csv(&quot;dat1.csv&quot;) ## Table of the first few observations. knitr::kable(head(dat1[, c(1, 23:27)])) ** Read in data for the fake experiment. import delimited using &quot;dat1.csv&quot;, clear rename v1 x ** Table of the first few observations. list x y0 y1 z y id in 1/6, sep(6) X y0 y1 Z Y id 1 0.000 6.123 0 0.000 1 2 0.000 2.741 0 0.000 2 3 0.000 7.215 0 0.000 3 4 2.056 6.708 0 2.056 4 5 0.000 5.654 1 5.654 5 6 10.409 15.561 0 10.409 6 R code Stata code Hide ## y0 and y1 are the true underlying potential outcomes. with(dat1, {boxplot(list(y0,y1), names=c(&quot;Control&quot;,&quot;Treatment&quot;), ylab=&quot;Outcomes&quot;) stripchart(list(y0,y1), add=TRUE, vertical=TRUE) stripchart(list(mean(y0), mean(y1)), add=TRUE, vertical=TRUE, pch=19, cex=2)}) ** y0 and y1 are the true underlying potential outcomes. label var y0 &quot;Control&quot; label var y1 &quot;Treatment&quot; * ssc install stripplot stripplot y0 y1, box vertical iqr whiskers(recast(rcap)) ytitle(&quot;Outcomes&quot;) variablelabels Figure 3.1: Simulated Experimental Outcomes In this simulated data, we know the true average treatment effect (ATE) because we know both of the underlying true potential outcomes. The control potential outcome, yi&#x2223;Zi=0y_{i|Z_i=0}yi&#x2223;Zi&#x200B;=0&#x200B;, is written in the code as y0, meaning &#x201C;the response person iii would provide if he/she were in the status quo or control group&#x201D;. The treatment potential outcome, yi&#x2223;Zi=1y_{i|Z_i = 1}yi&#x2223;Zi&#x200B;=1&#x200B;, is written in the code as y1, meaing &#x201C;the response person iii would provide if he/she were in the new policy or treatment group.&#x201D; We use ZiZ_iZi&#x200B; to refer to the experimental arm. In this case Zi=0Z_i=0Zi&#x200B;=0 for people in the status quo group and Zi=1Z_i=1Zi&#x200B;=1 for people in the new policy group. The true average treatment effect for each person is then the difference between that person&#x2019;s potential outcomes under each treatment condition. R code Stata code Hide trueATE &lt;- with(dat1, mean(y1) - mean(y0)) trueATE qui tabstat y0 y1, stat(mean) save global trueATE = r(StatTotal)[1,2] - r(StatTotal)[1,1] [1] 5.453 At this point, we have one realized experiment (defined by randomly assigning half of the people to treatment and half to control). And we know that the observed difference of means of the outcome, YYY, between treated and control groups is an unbiased estimator of the true ATE (by virtue of random assignment to treatment).3 We can calculate this in a few ways: we can just calculate the difference of means, or we can take advantage of the fact that an ordinary least squares linear regression produces the same estimate when the explanatory variable is a binary treatment indicator. R code Stata code Hide estATE1 &lt;- with(dat1, mean(Y[Z==1]) - mean(Y[Z==0])) estATE2 &lt;- lm(Y~Z, data=dat1)$coef[[&quot;Z&quot;]] c(estimatedATEv1=estATE1, estimatedATEv2=estATE2) stopifnot(all.equal(estATE1, estATE2)) qui ttest y, by(z) global estATE1 = round(r(mu_2) - r(mu_1), 0.001) qui reg y z global estATE2 = round(r(table)[1,1], 0.001) di &quot;estimatedATEv1=$estATE1 estimatedATEv2=$estATE2&quot; assert $estATE1 == $estATE2 estimatedATEv1 estimatedATEv2 4.637 4.637 3.1.1 Randomization-based standard errors How much would these estimates of the average treatment effect vary due to &#x201C;random noise&#x201D; if we repeated an experiment on the same group of people multiple times (randomly re-assigning treatment each time)? The standard error of an estimate of the average treatment effect is one answer to this question. As the expected variation due to random noise gets larger relative to the size of our treatment effect estimate, we should become increasingly cautious about the risk that our finding is actually an artifact of random noise.4 Below, we simulate a simple, individual-level experiment to help provide more intuition about what the standard error captures. We randomly re-assign treatment many times, save the resulting treatment effect estimates from each re-randomization, and calculate the standard deviation across them. This provides information about how far we should expect any one re-randomized treatment effect estimate to be from their mean. R code Stata code Hide ## A function to re-assign treatment and recalculate the difference of means. ## Treatment was assigned without blocking or other structure, so we ## just permute or shuffle the existing treatment assignment vector. simEstAte &lt;- function(Z,y1,y0){ Znew &lt;- sample(Z) Y &lt;- Znew * y1 + (1-Znew) * y0 estate &lt;- mean(Y[Znew == 1]) - mean(Y[Znew == 0]) return(estate) } ## Set up and perform the simulation sims &lt;- 10000 set.seed(12345) simpleResults &lt;- with(dat1,replicate(sims,simEstAte(Z = Z,y1 = y1,y0 = y0))) seEstATEsim &lt;- sd(simpleResults) ## The standard error of this estimate of the ATE (via simulation) seEstATEsim ** A program to re-assign treatment and recalculate the difference of means. ** Treatment was assigned without blocking or other structure, so we ** just permute or shuffle the existing treatment assignment vector. capture program drop simEstAte program define simEstAte, rclass sortpreserve version 18.0 syntax varlist(min=1 max=1), /// control_outcome(varname) treat_outcome(varname) qui sum `varlist&apos; // Get # treated units local numtreat = r(sum) tempvar rand // Randomly sort (temporary var) qui gen `rand&apos; = runiform() sort `rand&apos; tempvar Znew // New treatment qui gen `Znew&apos; = 0 qui replace `Znew&apos; = 1 in 1/`numtreat&apos; tempvar Ynew // New revealed outcome qui gen `Ynew&apos; = (`Znew&apos; * `treat_outcome&apos;) + ((1 - `Znew&apos;) * `control_outcome&apos;) qui ttest `Ynew&apos;, by(`Znew&apos;) return scalar estate = r(mu_2) - r(mu_1) end ** Set up and perform the simulation global sims 10000 set seed 12345 preserve qui simulate estate = r(estate), reps($sims): simEstAte z, control_outcome(y0) treat_outcome(y1) qui sum estate global seEstATEsim = r(sd) restore ** The standard error of this estimate of the ATE (via simulation) di &quot;$seEstATEsim&quot; [1] 0.9256 While this is useful for illustration, we do not need to rely on simulation to develop a design-based standard error estimator. Gerber and Green (2012) and Dunning (2012) walk through the following expression for a &#x201C;feasible&#x201D; (see below) design based standard error of a simple average treatment effect estimator (e.g., a difference in means based on randomly assigned treatment). If we write TTT as the set of all mmm treated units and CCC as the set of all n&#x2212;mn-mn&#x2212;m non-treated units, we then have: SE^(&#x3C4;)=s2(Yi,i&#x2208;T)m+s2(Yi,i&#x2208;C)(n&#x2212;m)\\widehat{SE}(\\tau) = \\sqrt{\\frac{s^2(Y_{i,i \\in T})}{m} + \\frac{s^2(Y_{i,i \\in C})}{(n-m)}}SE(&#x3C4;)=ms2(Yi,i&#x2208;T&#x200B;)&#x200B;+(n&#x2212;m)s2(Yi,i&#x2208;C&#x200B;)&#x200B;&#x200B; where s2(x)s^2(x)s2(x) is the sample variance such that s2(x)=(1/(n&#x2212;1))&#x2211;i=1n(xi&#x2212;x&#x2C9;)2s^2(x) = (1/(n-1))\\sum^n_{i = 1}(x_i-\\bar{x})^2s2(x)=(1/(n&#x2212;1))&#x2211;i=1n&#x200B;(xi&#x200B;&#x2212;x&#x2C9;)2. Since this expression is something we can calculate with a real sample, it&#x2019;s called a feasible standard error. In contrast, we cannot calculate the true design based standard error with a real sample, since it depends on knowing the covariance between each unit&#x2019;s potential outcomes (which is unobservable in real data). However, we can calculate this for our example, since we generated the data ourselves. Though we don&#x2019;t write it out here, you can see an expression for the true design based standard error in the code below. The feasible SE is designed to be at least as large as the true standard error on average. This means it is intentionally conservative. To illustrate these different calculation methods, we can compare the results of our simulation above to those of the feasible standard error expression. We can also compare both to the true standard error. We already have the simulated SE from the code above. Next, let&#x2019;s calculate the true SE. R code Stata code Hide ## True SE (Dunning Chap 6, Gerber and Green Chap 3, or Freedman, Pisani and Purves A-32). ## Requires knowing the true covariance between potential outcomes. N &lt;- nrow(dat1) V &lt;- var(cbind(dat1$y0,dat1$y1)) varc &lt;- V[1,1] vart &lt;- V[2,2] covtc &lt;- V[1,2] nt &lt;- sum(dat1$Z) nc &lt;- N-nt ## Gerber and Green, p.57, equation (3.4) varestATE &lt;- (((varc * nt) / nc) + ((vart * nc) / nt) + (2 * covtc)) / (N - 1) seEstATETrue &lt;- sqrt(varestATE) ** True SE (Dunning Chap 6, Gerber and Green Chap 3, or Freedman, Pisani and Purves A-32). ** Requires knowing the true covariance between potential outcomes. qui count local N = r(N) qui cor y0 y1, cov local varc = r(C)[1,1] local vart = r(C)[2,2] local covtc = r(C)[1,2] qui sum z local nt = r(sum) local nc = `N&apos; - `nt&apos; ** Gerber and Green, p.57, equation (3.4) local varestATE = (((`varc&apos; * `nt&apos;) / `nc&apos;) + ((`vart&apos; * `nc&apos;) / `nt&apos;) + (2 * `covtc&apos;)) / (`N&apos; - 1) global seEstATETrue = sqrt(`varestATE&apos;) Then, let&#x2019;s calculate the feasible standard error. R code Stata code Hide ## Feasible SE varYc &lt;- with(dat1,var(Y[Z == 0])) varYt &lt;- with(dat1,var(Y[Z == 1])) fvarestATE &lt;- (N/(N-1)) * ( (varYt/nt) + (varYc/nc) ) estSEEstATE &lt;- sqrt(fvarestATE) ** Feasible SE qui sum y if z == 0 local varYc = r(sd) * r(sd) qui sum y if z == 1 local varYt = r(sd) * r(sd) local fvarestATE = (`N&apos;/(`N&apos;-1)) * ( (`varYt&apos;/`nt&apos;) + (`varYc&apos;/`nc&apos;) ) global estSEEstATE = sqrt(`fvarestATE&apos;) Importantly, the feasible SE is not the standard error OLS regression provides by default. Among other things, default OLS SES are calculated under an iid errors assumption (&#x201C;identically and independently distributed&#x201D;). The feasible SE relaxes the &#x201C;identically&#x201D; part. Let&#x2019;s record the OLS SE for illustration. R code Stata code Hide ## OLS SE lm1 &lt;- lm(Y~Z, data=dat1) iidSE &lt;- sqrt(diag(vcov(lm1)))[[&quot;Z&quot;]] ** OLS SE qui reg y z global iidSE = _se[z] // Or: sqrt(e(V)[&quot;z&quot;,&quot;z&quot;]) Finally, we&#x2019;ll calculate one more alternative called the HC2 standard error, which (Lin 2013) shows to be a design based SE for treatment effects estimated via OLS regression, potentially including additional control variables (i.e., an alternative to the feasible SE for a simple difference in means above). Like the feasible SE above, HC2 errors should be conservative relative to the true SE. Note that under sampling based inference, the HC2 standard errors can instead be justified as a way of relaxing the assumption of &#x201C;identically&#x201D; distributed errors across. In other words, it is robust to a problem called &#x201C;heteroscedasticity,&#x201D; or &#x201C;heteroscedasticity-consistent&#x201D; (hence the HC in its name). R code Stata code Hide ## HC2 HC2SE &lt;- sqrt(diag(vcovHC(lm1, type = &quot;HC2&quot;)))[[&quot;Z&quot;]] ** HC2 qui reg y z, vce(hc2) global HC2SE = _se[z] // Or: sqrt(e(V)[&quot;z&quot;,&quot;z&quot;]) All those SE estimates in hand, let&#x2019;s review differences between the true standard error, the feasible standard error, the HC2 (Neyman) standard error, the standard error arising from direct repetition of the experiment, and the OLS standard error. The HC2 OLS SE is intended to be conservative relative to the true SE (at least as large or larger on average). This is the case in our example. HC2 is similar to the feasible SE outlined above, and both are larger than the true SE. We also illustrate the accuracy of our earlier SE simulation as a way of thinking more intuitively about what the standard error represents. Lastly, recall that our design involves different outcome variances for the treated group and the control group. We would therefore expect what we are calling the OLS IID SE to be at least somewhat inaccurate (different variances across treatment groups means that errors are not &#x201C;identically&#x201D; distributed). However, as you can see below, this inaccuracy is still sometimes negligible in practice. R code Stata code Hide compareSEs &lt;- c(simSE = seEstATEsim, feasibleSE = estSEEstATE, trueSE = seEstATETrue, olsIIDSE = iidSE, HC2SE = HC2SE) sort(compareSEs) matrix compareSEs = J(1, 5, .) matrix compareSEs[1, 1] = $seEstATEsim matrix compareSEs[1, 2] = $estSEEstATE matrix compareSEs[1, 3] = $seEstATETrue matrix compareSEs[1, 4] = $iidSE matrix compareSEs[1, 5] = $HC2SE matrix colnames compareSEs = &quot;simSE&quot; &quot;feasibleSE&quot; &quot;trueSE&quot; &quot;olsIIDSE&quot; &quot;HC2SE&quot; matrix list compareSEs olsIIDSE trueSE simSE HC2SE feasibleSE 0.8930 0.9189 0.9256 1.0387 1.0439 To provide a more rigorous comparison of these SE estimation methods, the code chunk below defines a function to calculate an average treatment effect, the OLS iid SE, and the OLS HC2 SE. It then uses this function to calculate SEs for random permutations of treatment 10000 times. Averaging across the simulated estimates provides a better illustration of their relative performance. As expected, the OLS IID SE now underestimates the true SE, while the HC2 SE is conservative as intended. The risk of underestimating the SE is that it could lead us to be overconfident when interpreting statistical findings. R code Stata code Hide ## Define a function to calculate several SEs, given potential outcomes and treatment sePerfFn &lt;- function(Z,y1,y0){ Znew &lt;- sample(Z) Ynew &lt;- Znew * y1 + (1-Znew) * y0 lm1 &lt;- lm(Ynew~Znew) iidSE &lt;- sqrt(diag(vcov(lm1)))[[&quot;Znew&quot;]] NeymanSE &lt;- sqrt(diag(vcovHC(lm1,type = &quot;HC2&quot;)))[[&quot;Znew&quot;]] return(c(estATE=coef(lm1)[[&quot;Znew&quot;]], estSEiid=iidSE, estSENeyman=NeymanSE)) } ## Perform a simulation using this function set.seed(12345) sePerformance &lt;- with(dat1, replicate(sims, sePerfFn(Z = Z, y1 = y1, y0 = y0))) ExpectedSEs &lt;- apply(sePerformance[c(&quot;estSEiid&quot;, &quot;estSENeyman&quot;),], 1, mean) c(ExpectedSEs, trueSE=seEstATETrue, simSE=sd(sePerformance[&quot;estATE&quot;,])) ** Define a function to calculate several SEs, given potential outcomes and treatment capture program drop sePerfFn program define sePerfFn, rclass sortpreserve version 18.0 syntax varlist(min=1 max=1), control_outcome(varname) treat_outcome(varname) qui sum `varlist&apos; // As in the program above local numtreat = r(sum) tempvar rand qui gen `rand&apos; = runiform() sort `rand&apos; tempvar Znew qui gen `Znew&apos; = 0 qui replace `Znew&apos; = 1 in 1/`numtreat&apos; tempvar Ynew qui gen `Ynew&apos; = (`Znew&apos; * `treat_outcome&apos;) + ((1 - `Znew&apos;) * `control_outcome&apos;) qui reg `Ynew&apos; `Znew&apos; // Regression now instead of ttest local iidSE = _se[`Znew&apos;] qui reg `Ynew&apos; `Znew&apos;, vce(hc2) local NeymanSE = _se[`Znew&apos;] return scalar iidSE = `iidSE&apos; // Prepare output return scalar NeymanSE = `NeymanSE&apos; return scalar estATE = _b[`Znew&apos;] end ** Perform a simulation using this function set seed 12345 preserve qui simulate /// iidSE = r(iidSE) NeymanSE = r(NeymanSE) estATE = r(estATE), /// reps($sims): /// sePerfFn z, control_outcome(y0) treat_outcome(y1) qui sum estATE global simSE = r(sd) qui sum iidSE global estSEiid = r(mean) qui sum NeymanSE global estSENeyman = r(mean) restore di &quot;Expected IID SE: $estSEiid&quot; di &quot;Expected Neyman SE: $estSENeyman&quot; di &quot;SIM SE: $simSE&quot; di &quot;True SE: $seEstATETrue&quot; estSEiid estSENeyman trueSE simSE 0.8511 0.9574 0.9189 0.9256 3.1.2 Randomization-based confidence intervals When we have a two arm trial (and a relatively large sample size), we can estimate the ATE, calculate design based standard errors, and then use them to create large-sample justified confidence intervals through either of the following approaches:5 R code Stata code Hide ## The difference_in_means function comes from the estimatr package. # (design based feasible errors by default) estAndSE1 &lt;- difference_in_means(Y ~ Z, data = dat1) ## Note that coeftest and coefci come from the lmtest package # (narrower intervals due to a different d.o.f.) est2 &lt;- lm(Y ~ Z, data = dat1) estAndSE2 &lt;- coeftest(est2, vcov.=vcovHC(est2, type = &quot;HC2&quot;)) estAndCI2 &lt;- coefci(est2, vcov.=vcovHC(est2, type = &quot;HC2&quot;), parm = &quot;Z&quot;) ## Organize output out &lt;- rbind( unlist(estAndSE1[c(1,2,6:8)]), c(estAndSE2[2,-3], estAndCI2) ) out &lt;- apply(out, 2, round, 3) colnames(out) &lt;- c(&quot;Est&quot;, &quot;SE&quot;, &quot;pvalue&quot;, &quot;CI lower&quot;, &quot;CI upper&quot;) row.names(out) &lt;- c(&quot;Approach 1 (diff. means)&quot;, &quot;Approach 2 (OLS)&quot;) out ** Organize output matrix compareCIs = J(2, 5, .) matrix rownames compareCIs = &quot;Approach 1 (diff. means)&quot; &quot;Approach 2 (OLS)&quot; matrix colnames compareCIs = &quot;Est&quot; &quot;SE&quot; &quot;pvalue&quot; &quot;CI lower&quot; &quot;CI upper&quot; ** A difference in means test assuming unequal variances ** (equivalent to the design-based estimator, as discussed above) ttest y, by(z) unequal local diffmeans = r(mu_2) - r(mu_1) matrix compareCIs[1,1] = round(`diffmeans&apos;, 0.001) matrix compareCIs[1,2] = round(r(se), 0.001) matrix compareCIs[1,3] = round(r(p), 0.001) matrix compareCIs[1,4] = round(`diffmeans&apos; - (invttail(r(df_t), 0.025) * r(se)), 0.001) matrix compareCIs[1,5] = round(`diffmeans&apos; + (invttail(r(df_t), 0.025) * r(se)), 0.001) ** A regression-based approach (narrower intervals due to a different d.o.f) reg y z, vce(hc2) // See: &quot;matrix list r(table)&quot; matrix compareCIs[2,1] = round(r(table)[1, 1], 0.001) matrix compareCIs[2,2] = round(r(table)[2, 1], 0.001) matrix compareCIs[2,3] = round(r(table)[4, 1], 0.001) matrix compareCIs[2,4] = round(r(table)[5, 1], 0.001) matrix compareCIs[2,5] = round(r(table)[6, 1], 0.001) matrix list compareCIs Est SE pvalue CI lower CI upper Approach 1 (diff. means) 4.637 1.039 0 2.524 6.750 Approach 2 (OLS) 4.637 1.039 0 2.576 6.699 We can also compute confidence intervals using randomization inference through an entirely simulation-based procedure: test inversion. It proceeds as follows for a simple two-arm trial: Choose a &#x201C;grid&#x201D; of values to consider, GGG, generally symmetric around 0 (e.g., [-0.5, 0.5]). For each g&#x2208;Gg \\in Gg&#x2208;G, construct a new outcome measure: yig=yi&#x2212;g&#xD7;treatiy_{i}^{g} = y_{i} - g \\times treat_{i}yig&#x200B;=yi&#x200B;&#x2212;g&#xD7;treati&#x200B;, and estimate a treatment effect, &#x3C4;g\\tau_{g}&#x3C4;g&#x200B;. Then, calculate a p-value for &#x3C4;g\\tau_{g}&#x3C4;g&#x200B; using randomization inference: We talk more about calculating p-values using treatment permutation in chapters 4 and 5. Briefly: you randomly permute (i.e., re-randomize) treatment, yielding treatirtreat_{i}^{r}treatir&#x200B;, estimate the treatment effect of interest, &#x3C4;r\\tau_{r}&#x3C4;r&#x200B;, and repeat that process many times. At the end, you calculate the proportion of times the absolute value of &#x3C4;r\\tau_{r}&#x3C4;r&#x200B; was greater than or equal to the absolute value of &#x3C4;g\\tau_{g}&#x3C4;g&#x200B;: 1n&#x2211;1(&#x2223;&#x3C4;r&#x2223;&#x2265;&#x2223;&#x3C4;g&#x2223;)\\frac{1}{n} \\sum \\mathcal{1}(|\\tau_{r}| \\geq |\\tau_{g}|)n1&#x200B;&#x2211;1(&#x2223;&#x3C4;r&#x200B;&#x2223;&#x2265;&#x2223;&#x3C4;g&#x200B;&#x2223;). This is your p-value for a given ggg. Compare the p-values of each g&#x2208;Gg \\in Gg&#x2208;G. The highest and lowest values of ggg that yield p-values greater than (e.g.) 0.05 are our simulated 95% confidence interval. R code Stata code Hide ## Define the grid to search through grid &lt;- seq(-10, 10, 0.05) ## Loop through values in this grid res &lt;- matrix(NA, length(grid), 2) i &lt;- 0 for (g in grid) { ## Update loop index i &lt;- i + 1 ## Create outcome for this g gdat &lt;- dat1 gdat$yg &lt;- gdat$Y - g*gdat$Z ## &quot;Real&quot; treatment effect for this g mod &lt;- lm(yg ~ Z, data = gdat) ## p-value through randomization inference ridraws &lt;- lapply( 1:500, function(.x) { gdat$Zri &lt;- sample(gdat$Z, length(gdat$Z), replace = F) lm(yg ~ Zri, data = gdat)$coefficients[2] } ) ridraws &lt;- do.call(c, ridraws) res[i,1] &lt;- g res[i,2] &lt;- mean(abs(ridraws) &gt;= abs(mod$coefficients[2])) } ## Compute CI from results res &lt;- res[ res[,2] &gt; 0.05, ] c( min(res[,1]), max(res[,1]) ) ** Define the grid to search through numlist &quot;-10(0.05)10&quot; local grid `r(numlist)&apos; macro list _grid ** Simple RI program to use here capture program drop ri_p program define ri_p, rclass capture drop Zri qui complete_ra Zri, m(25) qui reg yg Zri return scalar taur = _b[Zri] end ** Loop through values in this grid tempfile realdat save `realdat&apos;, replace local i = 0 foreach g of local grid { local ++i ** Create outcome for this g use `realdat&apos;, clear qui gen yg = y - `g&apos; * z ** Real regression for this g qui reg yg z local taug = _b[z] ** RI inference for this g qui simulate taur = r(taur), /// reps(500) : /// ri_p replace taur = abs(taur) &gt;= abs(`taug&apos;) collapse (mean) taur qui gen g = `g&apos; ** Save results if `i&apos; == 1 { qui tempfile running qui save `running&apos; } else { append using `running&apos; qui save `running&apos;, replace } ** Return to original data use `realdat&apos;, clear } ** Load the results use `running&apos;, clear ** Compute the CI keep if taur &gt; 0.05 qui sum g global lower = r(min) global upper = r(max) di &quot;$lower, $upper&quot; [1] 2.85 6.35 3.2 Summary: What does a design based approach mean for policy evaluation? Let&#x2019;s review some important terms. Hypothesis tests produce ppp-values telling us how much information we have against a null hypothesis. Estimators produce guesses about the size of some causal effect like the average treatment effect (i.e., &#x201C;estimates&#x201D;). Standard errors summarize how our estimates might vary from experiment to experiment by random chance, though we can only observe one experiment in practice. Confidence intervals tell us which ranges of null hypotheses are more versus less consistent with our data. In the frequentist approach to probability (the only approach we consider in this SOP), the properties of both ppp-values and standard errors arise from some process of repetition. Statistics textbooks often encourage us to imagine that this process of repetition involves repeated sampling from a larger (potentially infinite) population. But most OES work involves a pool of people who do not represent a well-defined population, nor do we tend to have a strong probability model of how these people and not others entered our sample. Instead, we have a known process of random assignment to an experimental intervention within a fixed sample. This often makes a randomization based approach to inference natural for our work, and helps our work be easiest to explain and interepret for our policy partners. References "],
["randomization-choices.html", "Chapter 4 Randomization choices 4.1 Coin flipping vs urn-drawing randomization 4.2 Randomization into 2 or more groups 4.3 Factorial designs 4.4 Block random assignment 4.5 Cluster random assignment 4.6 Other randomized designs 4.7 As-if random assignment 4.8 Assessing randomization (balance testing)", " Chapter 4 Randomization choices After working together with our agency partners to translate insights from the social and behavioral sciences into potential policy recommendations, we assess those new ideas by observing differences or changes in real world outcomes (usually measured using existing administrative data).6 In most cases, we design a randomized control trial (an RCT) to ensure that the differences or changes we observe are driven by the policy intervention itself. Here, we show examples of the different methods we consider for randomly assigning units to treatment. These form the core of the different types of RCTs that we use to build evidence about the effectiveness of the new policies. 4.1 Coin flipping vs urn-drawing randomization Many discussions of RCTs begin by talking about the intervention being assigned to units (people, schools, offices, districts) &#x201C;by the flip of a coin,&#x201D; or simple random assignment. Each unit&#x2019;s assignment to treatment occurs separately, and there is no ex ante guarantee as to exactly what the final number of treated or control units will be. We don&#x2019;t always use this method in practice, even though it is a useful way to introduce the idea that RCTs guarantee fair access to a new policy. The following code contrasts coin-flipping style random assignment with drawing-from-an-urn style, or complete random assignment (where a fixed number of units are randomly chosen for treatment). Coin-flipping based experiments are still valid and tell us about the underlying counterfactuals, but they can have less statistical power, so we try to avoid them where possible. Notice that the simple random assignment implemented in the code below results in more observations in the treatment group (group T) than in the control group (group C). Complete random assignment will always assign 5 units to the treatment, 5 to the control. R code Stata code Hide ## Start with a small experiment with only 10 units n &lt;- 10 ## Set a random seed for replicability set.seed(12345) ## Function to add more intuitive labels labelTC &lt;- function(assignments) {ifelse(assignments == 1, &quot;T&quot;, &quot;C&quot;)} ## Simulation using functions from the randomizr package trt_coinflip &lt;- labelTC(simple_ra(n)) trt_urn &lt;- labelTC(complete_ra(n)) ## Coin flipping does not guarantee half and half treated and control. ## Drawing from an urn, guarantees half treated and control. table(trt_coinflip) table(trt_urn) ## Alternative approach using base R # set.seed(12345) # trt_coinflip &lt;- labelTC(rbinom(10, size = 1, prob = .5)) # trt_urn &lt;- labelTC(sample(rep(c(1, 0), n / 2))) # table(trt_coinflip) # table(trt_urn) ** Start with a small experiment with only 10 units clear global n = 10 set obs $n ** Set a random seed for replicability set seed 12345 ** Simulation using functions from the randomizr package * ssc install randomizr simple_ra trt_coinflip * Or, e.g.: gen trt_coinflip = rbinomial(1, 0.5) complete_ra trt_urn /* * Or, e.g.: local num_treat = $n/2 gen rand = runiform() sort rand gen trt_urn = 1 in 1/`num_treat&apos; replace trt_urn = 0 if missing(trt_urn) drop rand */ ** Add more informative labels label define tc 0 &quot;C&quot; 1 &quot;T&quot; label values trt_coinflip tc label values trt_urn tc ** Coin flipping does not guarantee half and half treated and control. ** Drawing from an urn, guarantees half treated and control. table trt_coinflip table trt_urn trt_coinflip C T 3 7 trt_urn C T 5 5 4.2 Randomization into 2 or more groups We often use the randomizr R package (Coppock 2022a) for simple designs rather than the base R sample function because randomizr does some quality control checks. Notice that we implement a check on our code below with the stopifnot command: the code will stop and issue a warning if we didn&#x2019;t actually assign 1/4 of the observations to the treatment condition. Here, we assign the units first to 2 arms with equal probability (Z2armEqual). Then, to show how the code works, we assign them to 2 arms where one arm has only a 14\\frac{1}{4}41&#x200B; probability of receiving treatment (e.g., imagine a design with an expensive intervention). Last, we assign them based on a design with 4 different arms, each with equal probability (e.g., one control group and three different treatments under consideration). We often use ZZZ to refer to the variable recording our intervention arms. R code Stata code Hide N &lt;- nrow(dat1) set.seed(12345) ## Two equal arms dat1$Z2armEqual &lt;- labelTC(complete_ra(N)) ## Two unequal arms: .25 chance of treatment (.75 chance of control0 dat1$Z2armUnequalA &lt;- labelTC(complete_ra(N,prob=.25)) stopifnot(sum(dat1$Z2armUnequalA==&quot;T&quot;)==N/4) dat1$Z2armUnequalB &lt;- labelTC(complete_ra(N,m=N/4)) ## Four equal arms dat1$Z4arms &lt;- complete_ra(N, m_each=rep(N/4,4)) table(Z2armEqual=dat1$Z2armEqual) table(Z2armUnequalA=dat1$Z2armUnequalA) table(Z2armUnequalB=dat1$Z2armUnequalB) table(Z4arms=dat1$Z4arms) ** Return to data for the fake experiment. import delimited using &quot;dat1.csv&quot;, clear qui count global N = r(N) set seed 12345 ** Two equal arms complete_ra z2armEqual label define tc 0 &quot;C&quot; 1 &quot;T&quot; label values z2armEqual tc ** Two unequal arms: .25 chance of treatment (.75 chance of control) complete_ra z2armUnequalA, prob(0.25) label values z2armUnequalA tc qui sum z2armUnequalA global expected = $N/4 assert r(sum) == $expected complete_ra z2armUnequalB, m($expected) label values z2armUnequalB tc ** Four equal arms local count_list : di _dup(4) &quot;$expected &quot; // List of sample sizes for each group macro list _count_list complete_ra z4arms, m_each(`count_list&apos;) table z2armEqual table z2armUnequalA table z2armUnequalB table z4arms Z2armEqual C T 50 50 Z2armUnequalA C T 75 25 Z2armUnequalB C T 75 25 Z4arms T1 T2 T3 T4 25 25 25 25 4.3 Factorial designs It&#x2019;s possible to test the effects of more than one intervention while losing less statistical power by randomly assigning multiple treatments independently of each other. The simplest design that we use for this purpose is the 2&#xD7;22 \\times 22&#xD7;2 factorial design. For example, in the next table we see that we have assigned 50 observations to each arm of two separate interventions. Since the randomization of treatment1 is independent of treatment2, we can assess the effects of each treatment separately and pay less of a power penalty (unless one of the treatments dramatically increases the variance of the outcome compared to a hypothetical experiment with only one treatment assigned). R code Stata code Hide ## Two equal arms, adding a second cross treatment dat1$Z2armEqual2 &lt;- labelTC(complete_ra(N)) table(treatment1=dat1$Z2armEqual,treatment2=dat1$Z2armEqual2) ** Two equal arms, adding a second cross treatment complete_ra z2armEqual2 label var z2armEqual2 &quot;Treatment 2&quot; label var z2armEqual &quot;Treatment 1&quot; label val z2armEqual2 tc table z2armEqual z2armEqual2 treatment2 treatment1 C T C 23 27 T 27 23 Although factorial designs allow us to test more than one intervention at the same time, they may not provide the same degree of power when testing hypotheses about the interaction between the two treatments. If we want to learn about how two different interventions work together, then the sample size requirements will be much larger than if we were satisfied with learning about each treatment separately.7 Importantly, recent work highlights some important concerns regarding (1) the consequences of omitting interaction terms when estimating separate effects of each treatment and (2) the interpretation of factorial treatment effects (Muralidharan, Romero, and W&#xFC;thrich 2023). First, on (1), even if the interaction between treatments is not of academic or policy relevance, including it in the estimation model may be important for making correct inferences. Specifically, if the true interaction effect is not zero, excluding it from the model could increase the risk of Type I errors (i.e., false positives). Meanwhile, on (2), consider a two-arm factorial design with two Treatments, A and B, with 25% of the sample is in each treatment condition. An estimated effect of Treatment A from a model without an interaction should be interpreted as a weighted average of the effects of A across two subsamples: those receiving B (50%), and those not receiving B (50%). This weighted average treatment effect may or may not provide useful information about the likely effects of Treatment A if it is scaled up later.8 For instance, there may be a substantial interaction with treatment B, which is rarely administered in reality and which will not be scaled up alongside A. The subgroup effect of A among &#x201C;not B&#x201D; is then more policy relevant, but the subgroup effect of A among &#x201C;receiving B&#x201D; pulls the overall estimated average effect of A away from it. To deal with those issues, the OES Methods Team recommends estimating treatment effects in factorial experiments using a model that includes an interaction, at least as a robustness check. 4.4 Block random assignment 4.4.1 The benefits of blocking Statistical power depends not only on the size of the experiment and the strength of the treatment effect, but also on the amount of &#x201C;noise&#x201D; (non-treatment-related variability) in the outcome measure. Block-randomized designs can help reduce this noise while simultaneously minimizing estimation error&#x2014;the amount that our particular experiment&#x2019;s estimate differs from the truth. In a block-randomized, or stratified, design, we randomly assign units to the policy intervention within pre-specified groups.9 Suppose we are evaluating whether dedicated navigators can increase the percentage of students living in public housing who complete federal financial aid applications (FAFSA). Our experiment will send navigators to two of four eligible buildings, two of which are large and two of which are small. In a real study we can never know the outcome in all buildings both with and without navigators (the &#x201C;fundamental problem of causal inference&#x201D; from the last chapter). But if we could, we might have the data below: Building Size % FAFSA (No Navigator) % FAFSA (With Navigator) 1 Large 20 60 2 Large 30 70 3 Small 20 30 4 Small 30 40 Mean 25 50 The true average treatment effect for this sample is the average under treatment (i.e., the average treated potential outcome) minus the average under control (i.e., the average control potential outcome): ATE=50&#x2212;25=25\\text{ATE} = 50-25=25ATE=50&#x2212;25=25 percent more applications per building when a navigator is deployed. In a real study, we might randomly allocate two buildings to treatment and two buildings to control. If complete random assignment led to us treating the first two buildings, then we might observe: Building Size Treated % FAFSA (No Navigator) % FAFSA (With Navigator) 1 Large 1 60 2 Large 1 70 3 Small 0 20 4 Small 0 30 Mean 25 65 This yields a treatment effect estimate of 65&#x2212;25=4065-25 = 4065&#x2212;25=40 percent more applications due to the presence of a navigator. This is larger than the true value of 252525. Or, if random assignment led to the other two buildings being treated instead, we might then observe: Building Size Treated % FAFSA (No Navigator) % FAFSA (With Navigator) 1 Large 0 20 2 Large 0 30 3 Small 1 30 4 Small 1 40 Mean 25 35 This, in contrast, yields an estimated treatment effect of 35&#x2212;25=1035-25 = 1035&#x2212;25=10 percentage point more applications due to the navigators &#x2013; now smaller than the true value of 252525. All of the possible (equiprobable) assignments with two treated and two control units, along with their estimated treatment effects, are listed in the table below: Assignments Estimated Effect TTCC 40 CTCT 35 TCCT 25 CTTC 25 TCTC 15 CCTT 10 These possible treatment effect estimates have a mean equal to the true value of 25, illustrating the difference in means is an unbiased estimator. However, some of these estimates are far from the truth, and they have a lot of variability. To design an experiment that better estimates the true value, and does so with more statistical power (less variability), we can randomly assign units within blocks. In general, units should be sorted into different blocks based on their similarity across one or more characteristics that we expect to be correlated with our outcome. Here, blocking implies restricting the possible random assignments to those that have one large and one small building in each treatment group: Assignments Estimated Effect CTCT 35 TCCT 25 CTTC 25 TCTC 15 With this blocked design restricting the random assignments that are possible, we now get an estimate that is no more than 10 percentage points from the truth. Further, our estimates will have less variability (an SD of 8.16 rather than 11.4). This improves the statistical power of our design. For a more realistic example, suppose we are designing an experiment where the sample includes patients from two different hospitals. We might randomly assign patients to treatment and control within each hospital. For instance, we might assign half of the patients in hospital &#x201C;A&#x201D; to treatment and half to control, then do the same in hospital &#x201C;B.&#x201D;10 Below, we have 50 units in hospital &#x201C;A&#x201D; and 50 in hospital &#x201C;B&#x201D;: R code Stata code Hide dat1$blockID &lt;- gl(n = 2, k = N/2, labels = c(&quot;Block A&quot;, &quot;Block B&quot;)) with(dat1,table(blockID=dat1$blockID)) local Anum = $N/2 gen blockID = . tempvar rand gen `rand&apos; = runiform() sort `rand&apos; replace blockID = 1 in 1/`Anum&apos; replace blockID = 2 if missing(blockID) label define blocklab 1 &quot;Block A&quot; 2 &quot;Block B&quot; label values blockID blocklab table blockID blockID Block A Block B 50 50 We assign half of the units in each hospital to each treatment condition: R code Stata code Hide dat1$Z2armBlocked &lt;- labelTC(block_ra(blocks=dat1$blockID)) with(dat1, table(blockID, Z2armBlocked)) block_ra z2armBlocked, block_var(blockID) replace label val z2armBlocked tc table blockID z2armBlocked capture drop __00* // Clean up block_var temporary var Z2armBlocked blockID C T Block A 25 25 Block B 25 25 If, say, there were fewer people eligible for treatment in hospital &#x201C;A&#x201D; &#x2014; or perhaps the intervention was more expensive in that block &#x2014; we might choose different treatment probabilities for each block. The code below assigns half of the hospital &#x201C;A&#x201D; patients to treatment, but only a quarter of those from hospital &#x201C;B&#x201D;. Again, we also check that this code worked. This approach is an informal version of one of the best practices for writing code in general, called &#x201C;unit testing.&#x201D; See the EGAP Guide to Workflow for more examples. R code Stata code Hide ## Blocked assignment, unequal probability dat1$Z2armBlockedUneqProb &lt;- labelTC(block_ra(blocks=dat1$blockID, block_prob=c(.5,.25))) with(dat1, table(blockID, Z2armBlockedUneqProb)) ## Unit testing NumTreatedB &lt;- sum(dat1$Z2armBlockedUneqProb==&quot;T&quot; &amp; dat1$blockID==&quot;Block B&quot;) ExpectedNumTreatedB &lt;- sum(dat1$blockID==&quot;Block B&quot;)/4 stopifnot(NumTreatedB==ceiling(ExpectedNumTreatedB) | NumTreatedB==floor(ExpectedNumTreatedB)) ** Blocked assignment, unequal probability block_ra z2armBlockedUneqProb, block_var(blockID) block_prob(0.5 0.25) replace label val z2armBlockedUneqProb tc table blockID z2armBlockedUneqProb capture drop __00* // Clean up block_var temporary var ** Unit Testing qui count if z2armBlockedUneqProb == 1 &amp; blockID == 2 global NumTreatedB = `r(N)&apos; qui count if blockID == 2 global ExpectedNumTreatedB = `r(N)&apos;/4 assert ($NumTreatedB == ceil($ExpectedNumTreatedB)) | ($NumTreatedB == floor($ExpectedNumTreatedB)) Z2armBlockedUneqProb blockID C T Block A 25 25 Block B 38 12 Our team tries to implement block-randomized assignment whenever possible in order to increase the statistical power of our experiments. We also often find it useful in cases where different administrative units are implementing the treatment, or when we expect different groups of people to have different reactions to the treatment. 4.4.2 Using a few covariates to create blocks If we have background information on a few covariates, we can create blocks by hand through a process like the one demonstrated here: R code Stata code Hide ## For example, make three groups from the cov2 variable dat1$cov2cat &lt;- with(dat1, cut(cov2, breaks=3)) table(dat1$cov2cat, exclude=c()) with(dat1, tapply(cov2, cov2cat, summary)) ## And we can make blocks that are the same on two covariates dat1$cov1bin &lt;- as.numeric(dat1$cov1&gt;median(dat1$cov1)) # Binarize cov1 dat1$blockV2 &lt;- droplevels(with(dat1, interaction(cov1bin, cov2cat))) table(dat1$blockV2, exclude=c()) ## And then assign within these blocks set.seed(12345) dat1$ZblockV2 &lt;- labelTC(block_ra(blocks = dat1$blockV2)) with(dat1, table(blockV2, ZblockV2, exclude=c())) ** For example, make three groups from the cov2 variable egen cov2cat = cut(cov2), group(3) label table cov2cat bysort cov2cat : sum cov2 // Divides into intervals differently from R ** And we can make blocks that are the same on two covariates qui sum cov1, d gen cov1bin = cond(cov1 &gt; r(p50), 1, 0) // Similar to ifelse() in R decode cov2cat, generate(string_cov2cat) gen blockV2 = string(cov1bin) + &quot; &quot; + string_cov2cat table blockV2 ** And then assign within these blocks set seed 12345 block_ra zblockV2, block_var(blockV2) label val zblockV2 tc table blockV2 zblockV2 capture drop __00* // Clean up (-7.32,-2.6] (-2.6,2.1] (2.1,6.82] 11 68 21 $`(-7.32,-2.6]` Min. 1st Qu. Median Mean 3rd Qu. Max. -7.30 -5.03 -3.66 -4.14 -3.01 -2.77 $`(-2.6,2.1]` Min. 1st Qu. Median Mean 3rd Qu. Max. -2.3916 -0.7630 -0.0194 -0.0218 0.7592 2.0864 $`(2.1,6.82]` Min. 1st Qu. Median Mean 3rd Qu. Max. 2.13 2.46 2.95 3.24 3.68 6.80 0.(-7.32,-2.6] 1.(-7.32,-2.6] 0.(-2.6,2.1] 1.(-2.6,2.1] 0.(2.1,6.82] 1.(2.1,6.82] 7 4 38 30 5 16 ZblockV2 blockV2 C T 0.(-7.32,-2.6] 4 3 1.(-7.32,-2.6] 2 2 0.(-2.6,2.1] 19 19 1.(-2.6,2.1] 15 15 0.(2.1,6.82] 2 3 1.(2.1,6.82] 8 8 4.4.3 Blocking using many covariates If instead we have many background variables, we can increase precision by thinking about blocking as a problem of &#x201C;matching,&#x201D; or creating sets of units which are as similar as possible across the entire set of covariates (Moore 2012; Moore and Schnakenberg 2016). Here we show two approaches using R. Creating pairs: ## Using the blockTools package mvblocks &lt;- block(dat1, id.vars=&quot;id&quot;, block.vars=c(&quot;cov1&quot;,&quot;cov2&quot;), algorithm=&quot;optimal&quot;) dat1$blocksV3 &lt;- createBlockIDs(mvblocks, data=dat1, id.var = &quot;id&quot;) dat1$ZblockV3 &lt;- labelTC(block_ra(blocks = dat1$blocksV3)) ## Just show the first ten pairs with(dat1,table(blocksV3,ZblockV3,exclude=c()))[1:10,] ZblockV3 blocksV3 C T 1 1 1 2 1 1 3 1 1 4 1 1 5 1 1 6 1 1 7 1 1 8 1 1 9 1 1 10 1 1 Creating larger blocks: ## Using the quickblock package distmat &lt;- distances(dat1, dist_variables = c(&quot;cov1bin&quot;, &quot;cov2&quot;), id_variable = &quot;id&quot;, normalize=&quot;mahalanobiz&quot;) distmat[1:5,1:5] 1 2 3 4 5 1 0.0000 1.0697 0.3453 0.14026 0.14827 2 1.0697 0.0000 1.4150 0.92947 0.92146 3 0.3453 1.4150 0.0000 0.48555 0.49356 4 0.1403 0.9295 0.4856 0.00000 0.00801 5 0.1483 0.9215 0.4936 0.00801 0.00000 quantile(as.vector(distmat), seq(0,1,.1)) 0% 10% 20% 30% 40% 50% 60% 70% 80% 90% 100% -3.13279 -0.60902 -0.22924 -0.02745 0.09799 0.28989 0.68065 1.23306 1.76838 2.00726 2.91687 ## The caliper argument helps prevent ill-matched points mvbigblock &lt;- quickblock(distmat, size_constraint = 6L, caliper = 2.5) ## Look for missing points table(mvbigblock,exclude=c()) # One point dropped due to caliper mvbigblock 0 1 2 3 4 5 6 7 8 9 10 11 12 13 &lt;NA&gt; 7 6 6 6 6 6 7 7 8 8 9 7 6 10 1 dat1$blocksV4 &lt;- mvbigblock dat1$notblocked &lt;- is.na(dat1$blocksV4) dat1$ZblockV4[dat1$notblocked==F] &lt;- labelTC(block_ra(blocks = dat1$blocksV4)) with(dat1, table(blocksV4, ZblockV4, exclude=c()))[1:10,] ZblockV4 blocksV4 C T &lt;NA&gt; 0 4 3 0 1 3 3 0 2 3 3 0 3 3 3 0 4 3 3 0 5 3 3 0 6 4 3 0 7 4 3 0 8 4 4 0 9 4 4 0 It&#x2019;s worth pausing to examine the differences within blocks. We&#x2019;ll focus on the proportion of people in category &#x201C;1&#x201D; on the binary covariate (notice that the blocks are homogeneous on this covariate), as well as the difference between the largest and smallest value of the continuous covariate. This table also illustrates that, due to our use of a caliper when calling quickblock above, one observation was not included in treatment assignment. blockingDescEval &lt;- dat1 %&gt;% group_by(blocksV4) %&gt;% summarize( cov2diff = max(abs(cov2)) - min(abs(cov2)), cov1 = mean(cov1bin), count_in_block = n() ) blockingDescEval # A tibble: 15 &#xD7; 4 blocksV4 cov2diff cov1 count_in_block &lt;qb_blckn&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; 1 0 4.54 0 7 2 1 0.417 0 6 3 2 3.13 1 6 4 3 0.251 0 6 5 4 1.16 1 6 6 5 0.986 0 6 7 6 0.537 1 7 8 7 3.63 0 7 9 8 0.980 0 8 10 9 0.966 1 8 11 10 0.691 1 9 12 11 0.714 1 7 13 12 0.834 1 6 14 13 0.671 0 10 15 NA 0 1 1 4.4.4 Disadvantages Block-randomized assignment and analysis can help reduce both estimation error and non-treatment-related variability in our data. It may also be useful for ensuring equal distribution of treatment arms within less common subgroups in our sample, which can be especially important for preserving our statistical power when estimating heterogenous treatment effects. However, there are some practical disadvantages to consider. Block-randomized assignment can make treatment administration more complicated, both in terms of implementation by our partners and determining how we should incorporate blocking into our estimation strategy. Especially when a project needs to be rolled out on a short timeline, including a more complex blocking scheme might not be realistic. It may often be reasonable to conclude that the benefits of blocking are not worth the extra effort it entails. Adjusting for prognostic (i.e., correlated with YYY) covariates during the analysis stage may provide sufficient improvements in precision, especially in cases where we expect a design to already be reasonably powered under simple or complete random assignment or where we do not expect subgroup analyses (particularly for rare subgroups) to be a key component of an evaluation. But note that in smaller samples without block-randomized assignment, there might be a relatively greater risk of increasing variance by, e.g., adjusting for non-prognostic covariates (Miratrix, Sekhon, and Yu 2013). 4.5 Cluster random assignment We often implement a new policy intervention at the level of some group of people &#x2014; like a doctor&#x2019;s practice, or a building, or some other administrative unit. Even though we have 100 units in our example data, imagine now that they are grouped into 10 buildings, and the policy intervention is at the building level. Below, we assign 50 of those units to treatment and 50 to control. Everyone in each building has the same treatment assignment. R code Stata code Hide ## Make an indicator for cluster membership ndat1 &lt;- nrow(dat1) dat1$buildingID &lt;- rep(1:(ndat1/10), length=ndat1) set.seed(12345) dat1$Zcluster &lt;- labelTC(cluster_ra(cluster=dat1$buildingID)) with(dat1, table(Zcluster, buildingID)) ** Make an indicator for cluster membership qui count global ndat1 = r(N) egen buildingID = seq(), from(1) to(10) block(1) set seed 12345 cluster_ra zcluster, cluster_var(buildingID) table zcluster buildingID buildingID Zcluster 1 2 3 4 5 6 7 8 9 10 C 10 0 10 10 0 0 0 0 10 10 T 0 10 0 0 10 10 10 10 0 0 Cluster randomized designs raise new questions about estimation, testing, and statistical power. We describe our approaches to estimation and power analysis of cluster randomized designs in the chapter on analysis decisions. 4.6 Other randomized designs In the past, our team has also employed stepped-wedge style designs, saturation designs aimed at discovering whether the effects of the experimental intervention are communicated across people (via some spillover or network mechanism), and designs where we try to isolate certain experimental units (like buildings) from each other so that we can focus our learning about the effects of the intervention in isolation (rather than the effects when people can communicate with each other about the intervention). There are a variety of more specialized randomization designs that may be appropriate for particular projects, and the options discussed above should not be treated as exhaustive. We may expand on some of these other randomization options here in the future. 4.7 As-if random assignment In some circumstances, we might judge that randomly assigning a treatment of interest would be logistically infeasible. There are a variety of methods we have applied in such cases in the past to ensure assignment to treatment is at least idiosyncratic or arbitrary. What is key here is not really that assignment is random, per se. Instead, assignment must be conditionally independent of a unit&#x2019;s potential outcomes (Holland 1986). Random assignment is simply the best way of guaranteeing this. But sometimes, there are available methods of assigning treatment non-randomly that we decide are likely to satisfy this condition. Of course, relying on any as-if random assignment procedure makes it especially important to review evidence of appropriate treatment administration afterwards (see the next section). We list a few examples of as-if random assignment procedures below, all of which link to Analysis Plans pre-registered on the OES website. But note that an as-if random procedure that satisfies conditional independence in one study will not necessarily satisfy it in another. This needs to be determined on a case-by-case basis. When possible, it may be ideal to layer two arbitrary assignment procedures on top of each other rather than rely on the plausibility of only one. Grouping people into partitions based on the last two digits of their SSN and then rotating each partition&#x2019;s treatment assignment monthly Assigning program applicants to different conditions based on the last digits of their submission time and submission day (are both even, both odd, or is it mixed?) Assigning callers to different conditions based on the last four digits of their phone number 4.8 Assessing randomization (balance testing) If we have covariates, we can evaluate the implementation of random assignment by exploring covariate differences across treatment arms.11 The goal is to ensure that our data seem consistent with our intended randomization strategy (Imai, King, and Stuart 2008). Since we want to draw conclusions about the realized sample and not some broader population, it&#x2019;s important to think carefully about what we hope to learn from statistical significance tests. This often leads us to prefer omnibus tests of joint balance relying on randomization inference (see chapter 3), even if the evaluation itself will not rely on randomization inference. We lay out our reasoning in the sections below. 4.8.1 Separate tests for each covariate To see some issues significance testing raises when evaluating balance, consider the common practice of performing separate difference-in-means tests for each covariate. This is reasonable in some cases, but it could also raise &#x201C;multiple testing&#x201D; concerns (see Chapter 5). Briefly, it would be easy to discover one or a few covariates with noticeable mean differences due to random chance rather than real implementation problems. That is, in a well-operating experiment, we would expect some baseline imbalances for individual covariates&#x2014;roughly 5 in 100. The relationship between sample size and statistical significance is also important to remember. If a sample is too small, large imbalances (indicating real problems with treatment administration) may still be statistically insignificant. In other words, the balance test is too underpowered to be informative. And on the other hand, when working with large samples, negligible mean differences may still be significant, leading to spurious conclusions of imbalance. (This issue is relevant, to some degree, for any randomization assessment relying on significance testing, not just separate difference-in-means tests.) When comparing individual covariates across treatment arms is necessary, one strategy for dealing with the large-sample issue is to rely on equivalence tests, such as the &#x201C;Two One-Sided Test&#x201D; (TOST) procedure (Hartman and Hidalgo 2018; Rainey 2014).12 Another option is to evaluate statistics that are less sensitive to sample size like standardized mean differences (e.g., Cohen&#x2019;s ddd) and variance ratios. Here, it may be useful for transparency to pre-register what would represent evidence of meaningful treatment administration problems.13 A common heuristic, borrowing from the matching literature, is that values of Cohen&#x2019;s ddd greater than around 0.2 or 0.25, or variance ratios greater than 2 and less than 0.5, are more likely to represent a meaningful imbalance (Stuart 2010). But these are not hard and fast rules. There are fewer options for dealing with the small-sample issue above, though procedures employing randomization inference may perform relatively better in small samples (Reichardt and Gollob 1999; Hansen and Bowers 2008). But if the sample is too small to perform a well-powered balance test, it may also be too under-powered for treatment effect estimation. 4.8.2 Omnibus tests Instead of making separate comparisons for each covariate, we can instead judge whether a sample appears sufficiently incompatible with the joint (or &#x201C;omnibus&#x201D;) null hypothesis of no average covariate differences between treatment arms. Above all, this helps sidestep the multiple testing concerns noted above. This kind of test is often implemented in practice by OLS regressing a treatment indicator on a set of covariates and then calculating a p-value based on the model&#x2019;s F-statistic. Failing to find a significant difference in an omnibus F-test does not &#x201C;prove&#x201D; that there are no imbalances. But finding a statistically significant difference tells you that a closer look at covariate balance is needed. An omnibus F-statistic can also be calculated using a Wald test. The idea is to compare an unrestricted model regressing treatment on the set of covariates with a restricted model including only an intercept. The Wald test then provides evidence on the null hypothesis that the unrestricted model does not fit the data appreciably better. This method is useful when estimating balance while adjusting for block fixed effects. Here, both models should include block fixed effects, and the restricted model should simply drop the covariates (but is no longer be intercept-only). The Wald test more easily accommodates modifications like alternate standard errors (e.g.: HC2) than similar alternatives like a likelihood ratio (LR) test. Omnibus balance tests are often conducted in a typical sampling-based inference framework, but design-based inference is likely more appropriate for balance-testing. Failing to reject the null hypothesis in a randomization-based setting implies that observed imbalances are not sufficiently distinguishable from the distribution of imbalance estimates we&#x2019;d expect in a well-run experiment. When evaluating a randomized intervention, this is exactly what we hope to learn from balance checks! Moreover, omnibus tests based on randomly permuting treatment may control the Type-I error rate (limiting spurious conclusions that a sample is imbalanced) in small-to-moderate samples better (Kerwin, Rostom, and Sterck 2024; Hansen and Bowers 2008). See Hansen and Bowers (2008) for more discussion of this issue and an introduction to an alternative omnibus balance statistic (d2)(d^{2})(d2). 4.8.3 Summary We typically prefer to perform some form of omnibus balance test (ideally using randomization inference), and to write out a brief plan for exploring individual imbalances further if this test rejects the null hypothesis. Time-permitting, more extensive balance checks are always valuable. It can be important for balance testing procedures to account for any clustering or blocking employed when assigning treatment (Hansen and Bowers 2008). For blocking, this implies calculating balance within blocks (and then potentially aggregating across blocks to yield a single overall estimate). For clustering, this may imply exploring whether a comparison of cluster-level aggregates (rather than a comparison of individual units) provides evidence of treatment administration problems. Or it might imply just accounting for within-cluster independence when performing significance tests. All of those modifications are straightforward when estimating an omnibus balance statistic using a regression model. But they can also be incorporated into covariate-by-covariate statistics. For instance, in a blocked design, it may be reasonable to calculate separate standardized mean differences within each block (dividing within-block differences by the overall sample-level standard deviation) and then take a weighted average across blocks (where block-level estimates are weighted by their share of the overall sample size). 4.8.4 Coded examples We provide R and Stata examples of several procedures discussed above. In all cases, we evaluate balance for two covariates&#x2014;cov1 and cov2&#x2014;under one of the randomized designs discussed earlier in this chapter: complete randomization (Z2armEqual), clustered randomization (Zcluster), or blocked randomization (ZblockV2). First, we illustrate methods for calculating standardized mean differences (SMDs; mean difference divided by pooled SD) and variance ratios (VRs) for each covariate under different randomization designs. Although there are canned packages in R (e.g., cobalt) and Stata (e.g., tebalance) that can perform some of these calculations, we show how to do them manually. R code Stata code Hide ## List covariates to evaluate covlist &lt;- c(&quot;cov1&quot;, &quot;cov2&quot;) ## Define convenience function to perform calculations. ## May be applied to the whole dataset, cluster-level aggregates, ## or individual blocks. Assumes only 2 arms. See use below. bstats_2arm &lt;- function( d = dat1, # data to calculate balance within n = NULL, # total sample size; only use when d is a subset (blocking) tr = &quot;Z2armEqual&quot;, # treatment var name y = &quot;cov2&quot;, # evaluate balance for what covariate? psd = sd(dat1$cov2) # what sd to use in SMDs? ) { # Check whether a total sample size was specified. # Default is to impute this from d (assume d is not a subset). if (is.null(n)) { n &lt;- nrow(d) } # Get levels of treatment var, always decreasing order lvl &lt;- unique(d[,tr]) lvl &lt;- sort(lvl, decreasing = T) # Difference in means dm &lt;- mean(d[d[,tr]==lvl[1],y], na.rm = T) - mean(d[d[,tr]==lvl[2],y], na.rm = T) # Standardize using provided SD smd &lt;- dm/psd # Variance ratio vr &lt;- var(d[d[,tr]==lvl[1],y], na.rm = T) / var(d[d[,tr]==lvl[2],y], na.rm = T) # Proportion of total sample in subset # (1 if n was not set) prop &lt;- nrow(d)/n # Prepare output out &lt;- list( diff_means = dm, smd = smd, vr = vr, prop = prop ) return(out) } ## Iterate through this covariate list, building a table btab &lt;- data.frame(var = covlist) for (cov in covlist) { # approach 1: overall mean differences and SMDs sel &lt;- btab$var == cov pooled &lt;- sd(dat1[,cov]) overall &lt;- bstats_2arm(y = cov, psd = pooled) btab$diff[sel] &lt;- overall$diff_means btab$smd[sel] &lt;- overall$smd btab$vratio[sel] &lt;- overall$vr # approach 2: cluster-level mean differences and SMD. # approach 1 might be applied for clustered designs as well. by_groups &lt;- list(dat1$buildingID, dat1$Zcluster) clust &lt;- aggregate(dat1[,cov], by = by_groups, mean) clust &lt;- bstats_2arm(d = clust, tr = &quot;Group.2&quot;, y = &quot;x&quot;, psd = pooled) btab$cluster_diff[sel] &lt;- clust$diff_means btab$cluster_smd[sel] &lt;- clust$smd btab$cluster_vratio[sel] &lt;- clust$vr # approach 3: weighted avg of in-block mean differences and SMDs. blocked &lt;- by( data = dat1, INDICES = dat1$blockV2, FUN = function(x) { bstats_2arm(x, nrow(dat1), &quot;ZblockV2&quot;, cov, pooled) }, simplify = F ) smd_block &lt;- sapply(blocked, function(x) x$smd) dm_block &lt;- sapply(blocked, function(x) x$diff_means) vr_block &lt;- sapply(blocked, function(x) x$vr) prop_block &lt;- sapply(blocked, function(x) x$prop) btab$block_diff[sel] &lt;- weighted.mean(dm_block,prop_block) btab$block_smd[sel] &lt;- weighted.mean(smd_block,prop_block) btab$block_vratio[sel] &lt;- weighted.mean(vr_block,prop_block) } ## View output btab[, c(1, 3:4, 6:7, 9:10)] ** List covariates to evaluate global covlist cov1 cov2 ** Define convenience program to perform calculations. ** May be applied to the whole dataset, cluster-level aggregates, ** or individual blocks. Assumes only 2 arms. See use below. capture program drop bstats_2arm program define bstats_2arm, rclass sortpreserve byable(onecall) * varname = evaluate balance for what covariate? * tr = treatment var name * n = total sample size; only use when applied to a subset (Blocks) syntax varname [if] [in], tr(varname) /// [ n(string) psd(string) ttestopt(string) ] * Get list of by vars, passed from &quot;by varlist:&quot; or &quot;bysort varlist:&quot; local by &quot;`_byvars&apos;&quot; * Identify the correct sample to use (if/in) marksample touse * Further by var setup. * Confirm something was passed. capture confirm variable `by&apos; * If not, treat all obs as in same group. if _rc != 0 { tempvar group qui gen `group&apos; = 1 if `touse&apos; } * If something was, set this up as a grouping var. else { tempvar group qui egen `group&apos; = group(`by&apos;) if `touse&apos; } * In either case, get the levels as a macro. qui levelsof `group&apos; if `touse&apos;, local(by_levels) local len : word count `by_levels&apos; * Make rownames, used below local rownames foreach l of local by_levels { local rownames `rownames&apos; &quot;`l&apos;&quot; } * Get n as sample size in memory, if not specified if &quot;`n&apos;&quot;==&quot;&quot; { qui count if `touse&apos; local n = r(N) } * Get psd as pooled sd of varname, if not specified. * Though varname indicated above, still mapped to `varlist&apos;. if &quot;`psd&apos;&quot;==&quot;&quot; { qui sum `varlist&apos; if `touse&apos;, d local psd = r(sd) } * Loop through those levels, calculating * desired stats and saving in a matrix. matrix stats = J(`len&apos;, 5, .) matrix rownames stats = `rownames&apos; matrix colnames stats = &quot;dm&quot; &quot;smd&quot; &quot;vr&quot; &quot;prop&quot; &quot;group_n&quot; local i = 0 foreach l of local by_levels { * Update index local ++i * Difference in means (treatment level 2 is greater value) qui ttest `varlist&apos; if `touse&apos; &amp; `group&apos; == `l&apos;, by(`tr&apos;) `ttestopt&apos; local dm = r(mu_2) - r(mu_1) matrix stats[`i&apos;, 1] = `dm&apos; * Standardize using provided SD local smd = `dm&apos;/`psd&apos; matrix stats[`i&apos;, 2] = `smd&apos; * Variance ratio qui ttest `varlist&apos; if `touse&apos; &amp; `group&apos; == `l&apos;, by(`tr&apos;) `ttestopt&apos; local vr = (r(sd_2)^2)/(r(sd_1)^2) matrix stats[`i&apos;, 3] = `vr&apos; * Proportion of total sample in subset * (1 if n was not set) qui count if `touse&apos; &amp; `group&apos; == `l&apos; local prop = r(N)/`n&apos; local group_n = r(N) matrix stats[`i&apos;, 4] = `prop&apos; matrix stats[`i&apos;, 5] = `group_n&apos; } * Prepare output return matrix stats = stats end ** Iterate through this covariate list, building a table local clen : word count $covlist matrix btab = J(`clen&apos;, 6, .) local rownames foreach g of global covlist { local rownames `rownames&apos; &quot;`g&apos;&quot; } matrix rownames btab = `rownames&apos; matrix colnames btab = &quot;smd&quot; &quot;vratio&quot; &quot;cluster_smd&quot; &quot;cluster_vratio&quot; &quot;block_smd&quot; &quot;block_vratio&quot; local k = 0 foreach var of varlist $covlist { * approach 1: overall mean differences and SMDs local ++k qui sum `var&apos;, d local pooled = r(sd) bstats_2arm `var&apos;, tr(z2armequal) psd(`pooled&apos;) matrix btab[`k&apos;, 1] = r(stats)[1,&quot;smd&quot;] matrix btab[`k&apos;, 2] = r(stats)[1,&quot;vr&quot;] * approach 2: cluster-level mean differences and SMD. * approach 1 might be applied for clustered designs as well. preserve qui collapse (mean) cov1 cov2 (first) zcluster, by(buildingid) bstats_2arm `var&apos;, tr(zcluster) psd(`pooled&apos;) ttestopt(&quot;reverse&quot;) restore matrix btab[`k&apos;, 3] = r(stats)[1,&quot;smd&quot;] matrix btab[`k&apos;, 4] = r(stats)[1,&quot;vr&quot;] * approach 3: weighted avg of in-block mean differences and SMDs. preserve bysort blockv2: bstats_2arm `var&apos;, tr(zblockv2) psd(`pooled&apos;) n(100) qui clear qui svmat r(stats), names(col) qui sum smd [iw=prop] local blocked_smd = r(mean) qui sum vr [iw=prop] local blocked_vr = r(mean) restore matrix btab[`k&apos;, 5] = `blocked_smd&apos; matrix btab[`k&apos;, 6] = `blocked_vr&apos; } ** View output matrix list btab var smd vratio cluster_smd cluster_vratio block_smd block_vratio 1 cov1 0.085873 0.6287 0.12226 0.6578 -0.08384 1.521 2 cov2 0.007977 0.7846 0.02615 2.8861 0.13802 2.026 Applying common heuristics that SMDs should be less than about 0.25 while VRs should be between about 0.5 and 2 (Stuart 2010), the R output for the completely randomized design doesn&#x2019;t show sufficient evidence of imbalance, though the variance ratios come close (and may therefore be worth a closer look). Next, in the clustered design, for illustration, we assess balance by comparing cluster-level aggregates (whether this is the right approach in a real study with clustered assignment depends on context). Here, the variance ratios are more obviously concerning. This makes sense given that treatment was assigned at the level of only 10 clusters. Lastly, for the blocked design (where balance is assessed within each block, and then this is aggregated across blocks), recall that blocks were assigned based on first dividing cov2 into three groups and then determining whether cov1 was above it&#x2019;s median. Both groupings are relatively coarse, leading blocked randomization to also perform more poorly in terms of variance ratios. Again, it depends whether it is sufficient to evaluate balance in a blocked/clustered design as if complete random assignment were used, or whether it is important to do more to adapt the balance test to the randomization design. But evaluating overall balance in these settings can yield misleading conclusions (Hansen and Bowers 2008). Next, we illustrate methods for performing an omnibus Wald test under each randomization design, with examples of inference based on either standard asymptotic approximations or based on permutations of treatment assignment (i.e., randomization inference). Again, while there are canned packages to perform tasks like simulating the randomization distribution (see Chapter 5), we perform this manually for illustration. R code Stata code Hide ## Prepare data for this exercise dat1_ &lt;- dat1 dat1_$Z2armEqual &lt;- ifelse(dat1_$Z2armEqual == &quot;T&quot;, 1, 0) dat1_$Zcluster &lt;- ifelse(dat1_$Zcluster == &quot;T&quot;, 1, 0) dat1_$ZblockV2 &lt;- ifelse(dat1$ZblockV2 == &quot;T&quot;, 1, 0) dat1_$blockV2 &lt;- as.factor(dat1_$blockV2) ## Complete RA omnibus balance check bmod0 &lt;- lm(Z2armEqual ~ cov1 + cov2, data=dat1_) bmod1 &lt;- lm(Z2armEqual ~ 1, data=dat1_) wald &lt;- waldtest(bmod0, bmod1, vcov = vcovHC(bmod0, type = &quot;HC2&quot;)) ## Repeat using randomization inference ri_dist &lt;- sapply( 1:1000, function(.x) { dat1_$Z2armEqual_ri &lt;- complete_ra(nrow(dat1_)) unrestr &lt;- lm(Z2armEqual_ri ~ cov1 + cov2, data=dat1_) restr &lt;- lm(Z2armEqual_ri ~ 1, data=dat1_) waldtest(unrestr, restr, vcov = vcovHC(unrestr, type = &quot;HC2&quot;))$F[2] } ) wald$ri_p &lt;- c(NA, mean(abs(wald$F[2]) &lt;= abs(ri_dist))) ## Cluster RA omnibus balance check (error adjust only) bmod0b &lt;- lm(Zcluster ~ cov1 + cov2, data=dat1_) bmod1b &lt;- lm(Zcluster ~ 1, data=dat1_) waldb &lt;- waldtest(bmod0b, bmod1b, vcov = vcovCL(bmod0b, cluster = dat1_$buildingID, type = &quot;HC2&quot;)) ## Repeat using randomization inference ri_dist &lt;- sapply( 1:1000, function(.x) { dat1_$Zcluster_ri &lt;- cluster_ra(cluster=dat1_$buildingID) unrestr &lt;- lm(Zcluster_ri ~ cov1 + cov2, data=dat1_) restr &lt;- lm(Zcluster_ri ~ 1, data=dat1_) waldtest(unrestr, restr, vcov = vcovCL(unrestr, cluster = dat1_$buildingID, type = &quot;HC2&quot;))$F[2] } ) waldb$ri_p &lt;- c(NA, mean(abs(waldb$F[2]) &lt;= abs(ri_dist))) ## Cluster RA balance check: Wald test by_groups &lt;- list(dat1_$buildingID, dat1_$Zcluster) clust &lt;- aggregate(dat1_[,covlist], by = by_groups, mean) bmod2 &lt;- lm(Group.2 ~ cov1 + cov2, data=clust) bmod3 &lt;- lm(Group.2 ~ 1, data=clust) wald_cl &lt;- waldtest(bmod2, bmod3, vcov = vcovHC(bmod2, type = &quot;HC2&quot;)) ## Repeat using randomization inference ri_dist &lt;- sapply( 1:1000, function(.x) { dat1_$Zcluster_ri &lt;- cluster_ra(cluster=dat1_$buildingID) by_groups &lt;- list(dat1_$buildingID, dat1_$Zcluster_ri) clust_ri &lt;- aggregate(dat1_[,covlist], by = by_groups, mean) unrestr &lt;- lm(Group.2 ~ cov1 + cov2, data=clust_ri) restr &lt;- lm(Group.2 ~ 1, data=clust_ri) waldtest(unrestr, restr, vcov = vcovHC(unrestr, type = &quot;HC2&quot;))$F[2] } ) wald_cl$ri_p &lt;- c(NA, mean(abs(wald_cl$F[2]) &lt;= abs(ri_dist))) ## Blocked RA omnibus balance check bmod4 &lt;- lm(ZblockV2 ~ cov1 + cov2 + blockV2, data=dat1_) bmod5 &lt;- lm(ZblockV2 ~ blockV2, data=dat1_) wald_bl &lt;- waldtest(bmod4, bmod5, vcov = vcovHC(bmod4, type = &quot;HC2&quot;)) ## Repeat using randomization inference ri_dist &lt;- sapply( 1:1000, function(.x) { dat1_$ZblockV2_ri &lt;- block_ra(blocks = dat1$blockV2) unrestr &lt;- lm(ZblockV2_ri ~ cov1 + cov2 + blockV2, data=dat1_) restr &lt;- lm(ZblockV2_ri ~ blockV2, data=dat1_) waldtest(unrestr, restr, vcov = vcovHC(unrestr, type = &quot;HC2&quot;))$F[2] } ) wald_bl$ri_p &lt;- c(NA, mean(abs(wald_bl$F[2]) &lt;= abs(ri_dist))) ## Organize omnibus &lt;- data.frame( design = c(&quot;complete RA&quot;, &quot;clustered RA (SE only)&quot;, &quot;clustered RA&quot;, &quot;blocked RA&quot;), wald_p = c(wald$`Pr(&gt;F)`[2], waldb$`Pr(&gt;F)`[2], wald_cl$`Pr(&gt;F)`[2], wald_bl$`Pr(&gt;F)`[2]), ri_p = c(wald$ri_p[2], waldb$ri_p[2], wald_cl$ri_p[2], wald_bl$ri_p[2]) ) omnibus$wald_p &lt;- round(omnibus$wald_p, 3) omnibus$ri_p &lt;- round(omnibus$ri_p, 4) ## Review omnibus ** Prepare data for this exercise, save prior data tempfile restore save `restore&apos;, replace local treatlist z2armequal zcluster zblockv2 foreach l of local treatlist { replace `l&apos; = cond(`l&apos; == &quot;T&quot;, &quot;1&quot;, &quot;0&quot;) destring `l&apos;, replace } ** Complete RA omnibus balance check qui reg z2armequal cov1 cov2, vce(hc2) test cov1=cov2=0 local waldp = r(p) local waldf = r(F) ** Repeat using randomization inference capture program drop ri_draw program define ri_draw, rclass capture drop riZ complete_ra riZ qui reg riZ cov1 cov2, vce(hc2) qui test cov1=cov2=0 return scalar ri_F = r(F) end preserve simulate /// ri_F = r(ri_F), /// reps(1000) nodots: /// ri_draw gen equal_or_greater = abs(ri_F) &gt;= abs(`waldf&apos;) qui sum equal_or_greater, meanonly local waldp_ri = r(mean) restore ** Cluster RA omnibus balance check (error adjust only) * (Note: this is CR1, not CR2) qui reg zcluster cov1 cov2, cluster(buildingid) qui test cov1=cov2=0 local waldb_p = r(p) local waldb_f = r(F) ** Repeat using randomization inference capture program drop ri_draw program define ri_draw, rclass capture drop riZ qui sum zcluster local zsum = r(sum) cluster_ra riZ, cluster_var(buildingid) qui reg riZ cov1 cov2, cluster(buildingid) qui test cov1=cov2=0 return scalar ri_F = r(F) end preserve simulate /// ri_F = r(ri_F), /// reps(1000) nodots: /// ri_draw gen equal_or_greater = abs(ri_F) &gt;= abs(`waldb_f&apos;) qui sum equal_or_greater, meanonly local waldb_p_ri = r(mean) restore ** Cluster RA balance check: Wald test preserve collapse (mean) cov1 cov2 (first) zcluster, by(buildingid) qui reg zcluster cov1 cov2, vce(hc2) qui test cov1=cov2=0 restore local wald_clp = r(p) local wald_clf = r(F) ** Repeat using randomization inference capture program drop ri_draw program define ri_draw, rclass capture drop riZ qui sum zcluster local zsum = r(sum) cluster_ra riZ, cluster_var(buildingid) preserve collapse (mean) cov1 cov2 (first) riZ, by(buildingid) qui reg riZ cov1 cov2, vce(hc2) qui test cov1=cov2=0 restore return scalar ri_F = r(F) end preserve simulate /// ri_F = r(ri_F), /// reps(1000) nodots: /// ri_draw gen equal_or_greater = abs(ri_F) &gt;= abs(`wald_clf&apos;) qui sum equal_or_greater, meanonly local wald_clp_ri = r(mean) restore ** Blocked RA omnibus balance check encode blockv2, gen(fblockv2) qui reg zblockv2 cov1 cov2 i.fblockv2, vce(hc2) test cov1=cov2=0 local wald_blp = r(p) local wald_blf = r(F) ** Repeat using randomization inference capture program drop ri_draw program define ri_draw, rclass capture drop riZ block_ra riZ, block_var(blockv2) qui reg riZ cov1 cov2 i.fblockv2, vce(hc2) qui test cov1=cov2=0 return scalar ri_F = r(F) end preserve simulate /// ri_F = r(ri_F), /// reps(1000) nodots: /// ri_draw gen equal_or_greater = abs(ri_F) &gt;= abs(`wald_blf&apos;) qui sum equal_or_greater, meanonly local wald_blp_ri = r(mean) restore ** Organize matrix omnibus = J(4, 2, .) matrix rownames omnibus = &quot;complete RA&quot; &quot;clustered RA (se only)&quot; &quot;clustered RA&quot; &quot;blocked RA&quot; matrix colnames omnibus = &quot;wald_p&quot; &quot;ri_p&quot; matrix omnibus[1,1] = round(`waldp&apos;, 0.001) matrix omnibus[1,2] = round(`waldp_ri&apos;, 0.001) matrix omnibus[2,1] = round(`waldb_p&apos;, 0.001) matrix omnibus[2,2] = round(`waldb_p_ri&apos;, 0.001) matrix omnibus[3,1] = round(`wald_clp&apos;, 0.001) matrix omnibus[3,2] = round(`wald_clp_ri&apos;, 0.001) matrix omnibus[4,1] = round(`wald_blp&apos;, 0.001) matrix omnibus[4,2] = round(`wald_blp_ri&apos;, 0.001) ** Review matrix list omnibus design wald_p ri_p 1 complete RA 0.907 0.886 2 clustered RA (SE only) 0.680 0.703 3 clustered RA 0.834 0.814 4 blocked RA 0.242 0.261 In these examples, both standard asymptotic and randomization-based inference yield similar conclusions in terms of failing to reject the joint null of overall imbalance (where &#x201C;overall imbalance&#x201D; is quantified using the F-statistic from a Wald test). Given the small sample size in this example, though we do not see sufficient evidence to reject the joint null, concerns that the omnibus test could be under-powered might still support evaluating covariate-by-covariate statistics as we do above for supplementary evidence. Finally, while we do not report results in text, we also illustrate how to perform the d2d^2d2 omnibus test mentioned above for a few different randomization designs. This is implemented in R using the function balanceTest() from the package RItools (Hansen and Bowers 2008; Bowers, Fredrickson, and Hansen 2016). In Stata, this is implemented through a command that calls the R package and applies it to the data in memory (so it still requires an R installation on the user&#x2019;s computer). This function provides significance test results based on large-sample approximations to the &#x201C;permutation&#x201D; or &#x201C;randomization&#x201D; distribution of d2d^2d2 statistic, but its randomization distribution could be computed manually as in our examples above. You can think of the d2d^2d2 test statistic as a summary measure of mean differences across each covariate. R code Stata code Hide ## Complete RA balanceTest(Z~cov1+cov2, data=dat1) ## Blocked RA balanceTest(ZblockV3~cov1+cov2+strata(blocksV3), ## Blocked RA balanceTest(ZblockV3~cov1+cov2+strata(blocksV3)+cluster(clusterID), data=dat1) * ssc install xbalance. * See &quot;help xbalance&quot; for additional Stata setup instructions. * This is calling the RItools R package, so you will need R installed. * The necessary path to Rterm.exe may look something like this: global Rterm_path &quot;C:\\Program Files\\R\\R-4.2.1\\bin\\x64\\Rterm.exe&quot; ** Complete RA gen single_block = 1 // To force only unstratified balance testing label val zblockV2 // Remove value labels first label val z xbalance z single_block cov1 cov2 ** Block RA * zblockV2 instead of zblockV3 * zblockV3 is generated using blockTools, with no Stata equivalent xbalance zblockV2 blockV2 cov1 cov2 4.8.5 What to do with &#x201C;failed&#x201D; randomization assessments? Observing a ppp-value of less than .05 in an omnibus test ought to trigger extra scrutiny about implementation and/or how the data were recorded. For example, we might respond by contacting our agency partner to learn more about how random numbers were generated or how random assignment code was used (particularly if we didn&#x2019;t perform the random assignment ourselves). In many circumstances, this follow-up investigation might suggest that random assignment was implemented correctly, and that our understanding of the design or the data was simply incorrect (i.e., the balance test was not performed correctly). But sometimes, a follow-up investigation may not turn up any misunderstandings at all. In those situations, we will need to determine whether our rejection of the null hypothesis of appropriate random assignment is a false positive, or evidence of a more systematic problem. If our rejection of the null hypothesis appears to be driven by one or more covariates that are substantively important&#x2014;say, the variable age looks very imbalanced between treated and control groups in a health-related randomized trial&#x2014;then we might present both the unadjusted results and a separate set of results that adjust for the covariate(s) in question (e.g., through a stratified difference-in-means estimator, or by using them as controls in a linear regression). Large differences between the adjusted and unadjusted estimates might help us interpret our findings: estimating separate effects within different age groups, for example, might tell us something useful about the particular context of a study and inform the conclusions we draw. References "],
["analysis-choices.html", "Chapter 5 Analysis choices 5.1 Completely randomized trials 5.2 Multiple tests 5.3 Covariance adjustment 5.4 How to choose covariates for covariance adjustment? 5.5 Block-randomized trials 5.6 Cluster-randomized trials", " Chapter 5 Analysis choices We&#x2019;ve organized our discussion of analysis techniques around a study&#x2019;s randomization design. But there are a few general tactics that we use to ensure that we can make transparent, valid, and statistically precise statements about the results from our evaluations. We&#x2019;ll start this chapter by reviewing those. First, the nature of the data that we expect to see from a given experiment informs our analysis plans. For example, we may make some choices based on the nature of the outcome&#x2014;a binary outcome, a symmetrically distributed continuous outcome, and a heavily skewed continuous outcome each could each call for different analytical approaches. Second, we tend to ask three different questions in each of our studies, and we answer them with different statistical procedures: Can we detect an effect in our experiment? (We use hypothesis tests to answer this question.) What is our best guess about the size of the effect of the experiment? (We estimate the average treatment effect of our interventions to answer this question.) How precise is our guess? (We report confidence intervals or standard errors to answer this question.) Finally, in the Analysis Plans that we post online before receiving outcome data for a project, we try to anticipate many common decisions involved in data analysis&#x2014;how we will treat missing data, how we will rescale, recode, and combine columns of raw data, etc. We touch on some of these topics in more detail below, and will cover others further in the future. 5.1 Completely randomized trials 5.1.1 Two arms 5.1.1.1 Continuous outcomes In a completely randomized trial where outcomes take on many levels (units like times, counts of events, dollars, percentages, etc.) we generally assess the weak null hypothesis of no average treatment effect across units. We may also (or instead) assess the sharp null hypothesis of no effect for any unit. What we call the &#x201C;weak null&#x201D; here corresponds to the null evaluated by default in many software packages (like R and Stata) when implementing procedures like a difference-in-means test or a linear regression. Meanwhile, the sharp null is evaluated when relying on design-based randomization inference. Particularly in smaller samples, tests of the sharp null may be better powered (Reichardt and Gollob 1999). It may also be possible to justify a hypothesis test of the sharp null in some limited situations where justifying other procedures is more difficult (Rubin 1986).14 5.1.1.1.1 Estimating the average treatment effect and testing the weak null of no average effects We show the kind of code we use for these purposes here. Below, Y is the outcome variable and Z is an indicator of the assignment to treatment. R code Stata code Hide ## This function comes from the estimatr package estAndSE1 &lt;- difference_in_means(Y ~ Z,data = dat1) print(estAndSE1) ** Similarly to the R code, estimate the difference ** in means assuming unequal variance across treatment groups. ** There isn&apos;t a perfect Stata equivalent providing a ** design-based difference in means estimator. ttest y, by(z) unequal Design: Standard Estimate Std. Error t value Pr(&gt;|t|) CI Lower CI Upper DF Z 4.637 1.039 4.465 8.792e-05 2.524 6.75 33.12 Notice that the standard errors that we use are not the default OLS errors: R code Stata code Hide estAndSE1OLS &lt;- lm(Y~Z,data=dat1) summary(estAndSE1OLS)$coef reg y z Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 2.132 0.4465 4.775 6.283e-06 Z 4.637 0.8930 5.193 1.123e-06 The standard errors we prefer, HC2, reflect repeated randomization from a fixed experimental pool (chapter 3). Specifically, Lin (2013) and Samii and Aronow (2012) show that the standard error estimator of an unbiased average treatment effect within a &#x201C;finite-sample&#x201D; or design based framework is equivalent to the HC2 standard error. In R, these SEs are produced by default by the estimatr package&#x2019;s function difference_in_means() and lm_robust(). They can also be produced, for instance, using the vcovHC() function from the sandwich package or the lmtest package&#x2019;s coeftest() and coefci() functions. In Stata, when using regress, these can be estimated using the vce(hc2) option (they are not the default used by the robust option). Some other commands offer a similar option; check the help file to see. Our preference for HC2 errors follows from their design-based justification, but most researchers likely encounter them as one of several methods of correcting OLS standard errors for heteroscedasticity. Essentially, this means that the variance of the regression model&#x2019;s error term is not constant across observations. Classical OLS standard errors assume away heteroscedasticity, which may render them meaningfully inaccurate when heteroscedasticity is present. When using OLS to analyze data from a two-arm randomized trial, heteroscedasticity might appear because the variance of the outcome is different in the treatment and control groups. This is common. 5.1.1.1.2 Testing the sharp null of no effects We may assess the sharp null of no effects via treatment permutation as a check on the assumptions underlying the calculations and statistical inferences above (i.e., &#x201C;randomization inference&#x201D;). We could use a ttt-statistic as our test statistic here to parallel the above tests. But we could also use a rank-based test statistic instead if we were concerned about long-tails (i.e., skew) reducing statistical power. In the interests of simplificity, we can even use treatment effect estimates themselves. The basic procedure is as follows: Estimate the treatment effect of interest using the real treatment indicator: &#x3C4;^real\\hat{\\tau}_{real}&#x3C4;^real&#x200B;. As we note, you could use some other statistic here instead, such as a t-statistic or a rank-based statistic. Randomly reassign treatment RRR times, each time following the same assignment procedure used originally. Estimate a treatment effect using each of the simulated re-assignments: &#x3C4;^sim,r\\hat{\\tau}_{sim,r}&#x3C4;^sim,r&#x200B;. This yields a distribution of treatment effects under the sharp null (the &#x201C;randomization distribution&#x201D;). Compare the treatment effect to its randomization distribution to estimate a (two-sided) p-value: 1R&#x2211;r=1R1(&#x2223;&#x3C4;^real&#x2223;&#x2264;&#x2223;&#x3C4;^sim,r&#x2223;)\\frac{1}{R}\\sum_{r=1}^{R} \\mathcal{1}(|\\hat{\\tau}_{real}| \\leq |\\hat{\\tau}_{sim,r}|)R1&#x200B;&#x2211;r=1R&#x200B;1(&#x2223;&#x3C4;^real&#x200B;&#x2223;&#x2264;&#x2223;&#x3C4;^sim,r&#x200B;&#x2223;) Below, we show how to implement this approach using two different R packages and several Stata commands. First, lets look at the coin package (Hothorn et al. 2021) in R and permute in Stata: R code Stata code Hide ## The coin package set.seed(12345) # Compare means test1coinT &lt;- oneway_test(Y~factor(Z),data=dat1,distribution=approximate(nresample=1000)) test1coinT # Rank test 1 test1coinR&lt;- oneway_test(rankY~factor(Z),data=dat1,distribution=approximate(nresample=1000)) test1coinR # Rank test 2 test1coinWR &lt;- wilcox_test(Y~factor(Z),data=dat1,distribution=approximate(nresample=1000)) test1coinWR ** The permute command set seed 12345 * Compare means permute z z = _b[z], reps(1000) nodots: reg y z // OR: permtest2 y, by(z) simulate runs(1000) * Rank test 1 egen ranky = rank(y) permute z z = _b[z], reps(1000) nodots: reg ranky z * Rank test 2 permute z z = r(z), reps(1000) nodots: ranksum y, by(z) * OR: ranksum y, by(z) exact // exact, not approximate Approximative Two-Sample Fisher-Pitman Permutation Test data: Y by factor(Z) (0, 1) Z = -4.6, p-value &lt;0.001 alternative hypothesis: true mu is not equal to 0 Approximative Two-Sample Fisher-Pitman Permutation Test data: rankY by factor(Z) (0, 1) Z = -4.9, p-value &lt;0.001 alternative hypothesis: true mu is not equal to 0 Approximative Wilcoxon-Mann-Whitney Test data: Y by factor(Z) (0, 1) Z = -4.9, p-value &lt;0.001 alternative hypothesis: true mu is not equal to 0 Next, the ri2 R package (Coppock 2022b) and ritest in Stata: R code Stata code Hide ## The ri2 package thedesign1 &lt;- randomizr:::declare_ra(N=ndat1,m=sum(dat1$Z)) test1riT &lt;- conduct_ri(Y~Z,declaration=thedesign1,sharp_hypothesis=0,data=dat1,sims=1000) tidy(test1riT) test1riR &lt;- conduct_ri(rankY~Z,declaration=thedesign1,sharp_hypothesis=0,data=dat1,sims=1000) tidy(test1riR) ** The ritest command * ssc install ritest ritest z z = _b[z], nodots reps(1000): reg y z ritest z z = r(z), nodots reps(1000): ranksum y, by(z) Random assignment procedure: Complete random assignment Number of units: 100 Number of treatment arms: 2 The possible treatment categories are 0 and 1. The number of possible random assignments is approximately infinite. The probabilities of assignment are constant across units: prob_0 prob_1 0.75 0.25 term estimate p.value 1 Z 4.637 0 term estimate p.value 1 Z 30.8 0 It is also relatively common for OES evaluations to encounter situations where these canned approaches aren&#x2019;t viable. It is useful to be able to program randomization inference manually. We provide templates for how this could be done in the code below, focusing on complete random assignment. This yields similar results to the canned functions above. R code Stata code Hide ## Define a function to re-randomize a single time ## and return the desired test statistic. ri_draw &lt;- function() { ## Using randomizr dat1$riZ &lt;- complete_ra(N = nrow(dat1), m = sum(dat1$Z)) ## Or base R # dat1$riZ &lt;- sample(dat1$Z, length(dat1$Z), replace = F) ## Return the test statistic of interest. ## Simplest is the difference in means itself. return( with(dat1, lm(Y ~ riZ))$coefficients[&quot;riZ&quot;] ) } ## We&apos;ll use replicate to repeat this many times. ri_dist &lt;- replicate(n = 1000, expr = ri_draw(), simplify = T) ## A loop would also work. # i &lt;- 1 # ri_dist &lt;- matrix(NA, 1000, 1) # for (i in 1:1000) { # ri_dist[i] &lt;- ri_draw() # } ## Get the real test statistic and calculate the p-value. # How often do different possible treatment assignments, # under a sharp null, yield test statistics with a magnitude # at least as large as our real statistic? real_stat &lt;- with(dat1, lm(Y ~ Z))$coefficients[&quot;Z&quot;] ri_p_manual &lt;- mean(abs(ri_dist) &gt;= abs(real_stat)) ## Compare to test1riT ri_p_manual ** Define a program to re-randomize a single time ** and return a desired test statistic. capture program drop ri_draw program define ri_draw, rclass ** Using randomizr (Stata version) capture drop riZ qui sum z local zsum = r(sum) complete_ra riZ, m(`zsum&apos;) ** Or manually /* gen rand = runiform() sort rand qui sum z gen riZ = 1 in 1/r(sum) replace riZ = 0 if missing(riZ) drop rand */ ** Return the test statistic of interest. ** Simplest is the difference in means itself. qui reg y riZ return scalar riZ = _b[riZ] end ** We&apos;ll use simulate to repeat this many times. ** Nested within preserve/restore to return to our data after. preserve ** Get the real test statistic qui reg y z local real_stat = _b[z] ** Perform the simulation itself simulate /// riZ = r(riZ), /// reps(1000): /// ri_draw ** Calculate the p-value. * How often do different possible treatment assignments, * under a sharp null, yield test statistics with a magnitude * at least as large as our real statistic? gen equal_or_greater = abs(riZ) &gt;= abs(`real_stat&apos;) qui sum equal_or_greater local ri_p_manual = r(mean) ** Compare to output from permute or ritest above di `ri_p_manual&apos; restore [1] 0 5.1.1.2 Binary outcomes We tend to focus on differences in percentage points when we are working with binary outcomes, usually estimated via OLS linear regression. A statement like &#x201C;the effect was a 5 percentage point increase&#x201D; has made communication with partners easier than a discussion in terms of log odds or odds ratios. In addition to difficulties in interpretation and communication, we also avoid logistic regression coefficients because of a problem noticed by Freedman (2008b) in the case of covariance adjustment or more complicated research designs. 5.1.1.2.1 Estimating the average treatment effect and testing the weak null of no average effects We can estimate effects and produce standard errors for differences of proportions using the same process as above. The average treatment effect estimate here represents the difference in the proportions of positive responses (i.e., Y=1Y=1Y=1) between treatment conditions. The standard error is still valid because it is calculated using a procedure justified by the study&#x2019;s design. R code Stata code Hide ## Make some binary outcomes dat1$u &lt;- runif(ndat1) dat1$v &lt;- runif(ndat1) dat1$y0bin &lt;- ifelse(dat1$u&gt;.5, 1, 0) # control potential outcome dat1$y1bin &lt;- ifelse((dat1$u+dat1$v) &gt;.75, 1, 0) # treated potential outcomes dat1$Ybin &lt;- with(dat1, Z*y1bin + (1-Z)*y0bin) truePropDiff &lt;- mean(dat1$y1bin) - mean(dat1$y0bin) ## Estimate and view the difference in proportions estAndSE2 &lt;- difference_in_means(Ybin~Z,data=dat1) estAndSE2 ** Make some binary outcomes gen u = runiform() gen v = runiform() gen uv = u + v gen y0bin = cond(u &gt; 0.5, 1, 0) // control potential outcome gen y1bin = cond(uv &gt; 0.75, 1, 0) // treated potential outcome gen ybin = (z * y1bin) + ((1 - z) * y0bin) qui sum y1bin, meanonly local y1mean = r(mean) qui sum y0bin, meanonly global truePropDiff = `y1mean&apos; - r(mean) ** Estimate and view the difference in proportions ttest ybin, by(z) unequal Design: Standard Estimate Std. Error t value Pr(&gt;|t|) CI Lower CI Upper DF Z 0.1067 0.1112 0.959 0.343 -0.1177 0.331 42.84 When we have an experiment that includes a treatment and control group with binary outcomes, and when we are estimating the ATE, the standard error from a difference in proportions test is similar to the design based feasible standard error for an average treatment effect (and therefore similar to HC2 errors). In contrast, classical OLS standard errors with a binary outcome&#x2014;sometimes called a linear probability model&#x2014;will be at least slightly incorrect due to inherent heteroscecdasticity (Angrist and Pischke 2009). To see this in more detail, consider that difference-in-proportion standard errors are estimated with the following equation: SE^prop=p1(1&#x2212;p1)n1+p2(1&#x2212;p2)n2\\widehat{SE}_{prop} = \\sqrt{\\frac{p_{1}(1-p_{1})}{n_{1}}+\\frac{p_{2}(1-p_{2})}{n_{2}}}SEprop&#x200B;=n1&#x200B;p1&#x200B;(1&#x2212;p1&#x200B;)&#x200B;+n2&#x200B;p2&#x200B;(1&#x2212;p2&#x200B;)&#x200B;&#x200B; where n1n_1n1&#x200B; is the size of the group assigned treatment, n2n_2n2&#x200B; is the size of the group assigned control, p1p_1p1&#x200B; is the proportion of &#x201C;successes&#x201D; in the group assigned treatment, and p2p_2p2&#x200B;iss the proportion of &#x201C;successes&#x201D; in the group assigned control. The fractions above represent the variance of the proportion in each group. It&#x2019;s easy to compare this with the feasible standard error equation from chapter 3: SE^feasible=Var(Yt)n1+Var(Yc)n2\\widehat{SE}_{feasible} = \\sqrt{\\frac{\\mathrm{Var}(Y_{t})}{n_1}+\\frac{\\mathrm{Var}(Y_{c})}{n_2}}SEfeasible&#x200B;=n1&#x200B;Var(Yt&#x200B;)&#x200B;+n2&#x200B;Var(Yc&#x200B;)&#x200B;&#x200B; YcY_cYc&#x200B; is the vector of observed outcomes under control, and YtY_tYt&#x200B; is the vector of observed outcomes under treatment. This equation indicates that we use the observed variances in each treatment group to estimate the feasible design based standard error for an average treatment effect. The code below compares the various standard error estimators discussed here (we refer to the design based SEs as Neyman SEs). R code Stata code Hide nt &lt;- sum(dat1$Z) nc &lt;- sum(1-dat1$Z) ## Find SE for difference of proportions. p1 &lt;- mean(dat1$Ybin[dat1$Z==1]) p0 &lt;- mean(dat1$Ybin[dat1$Z==0]) frac1 &lt;- (p1*(1-p1))/nt frac0 &lt;- (p0*(1-p0))/nc se_prop &lt;- round(sqrt(frac1 + frac0), 4) ## Find Neyman SE varc_s &lt;- var(dat1$Ybin[dat1$Z==0]) vart_s &lt;- var(dat1$Ybin[dat1$Z==1]) se_neyman &lt;- round(sqrt((vart_s/nt) + (varc_s/nc)), 4) ## Find OLS SE simpOLS &lt;- lm(Ybin~Z,dat1) se_ols &lt;- round(coef(summary(simpOLS))[&quot;Z&quot;, &quot;Std. Error&quot;], 2) ## Find Neyman SE (which are the HC2 SEs) se_neyman2 &lt;- coeftest(simpOLS,vcov = vcovHC(simpOLS,type=&quot;HC2&quot;))[2,2] se_neyman3 &lt;- estAndSE2$std.error ## Show SEs se_compare &lt;- as.data.frame(cbind(se_prop, se_neyman, se_neyman2, se_neyman3, se_ols)) rownames(se_compare) &lt;- &quot;SE(ATE)&quot; colnames(se_compare) &lt;- c(&quot;diff in prop&quot;, &quot;neyman1&quot;,&quot;neyman2&quot;,&quot;neyman3&quot;, &quot;ols&quot;) print(se_compare) qui sum z global nt = r(sum) tempvar oneminus gen `oneminus&apos; = 1 - z qui sum `oneminus&apos; global nc = r(sum) ** Find SE for difference of proportions. qui ttest ybin, by(z) local p1 = r(mu_2) local p0 = r(mu_2) local se1 = (`p1&apos; * (1 - `p1&apos;))/$nt local se0 = (`p0&apos; * (1 - `p0&apos;))/$nc global se_prop = round(sqrt(`se1&apos; + `se0&apos;), 0.0001) local se_list $se_prop // Initialize a running list; used in the matrix below ** Find Neyman SE qui sum ybin if z == 0 local varc_s = r(sd) * r(sd) qui sum ybin if z == 1 local vart_s = r(sd) * r(sd) global se_neyman = round(sqrt((`vart_s&apos;/$nt) + (`varc_s&apos;/$nc)), 0.0001) local se_list `se_list&apos; $se_neyman ** Find OLS SE qui reg ybin z global se_ols = round(_se[z], 0.0001) // See also: r(table) (return list) or e(V) (ereturn list) local se_list `se_list&apos; $se_ols ** Find Neyman SE (which are the HC2 SEs) qui reg ybin z, vce(hc2) global se_neyman2 = round(_se[z], 0.0001) qui ttest ybin, by(z) unequal // See: return list global se_neyman3 = round(r(se), 0.0001) local se_list `se_list&apos; $se_neyman2 $se_neyman3 ** Show SEs matrix se_compare = J(1, 5, .) matrix colnames se_compare = &quot;diff in prop&quot; &quot;neyman1&quot; &quot;ols&quot; &quot;neyman2&quot; &quot;neyman3&quot; local i = 0 foreach l of local se_list { local ++i matrix se_compare[1, `i&apos;] = `l&apos; } matrix list se_compare diff in prop neyman1 neyman2 neyman3 ols SE(ATE) 0.1094 0.1112 0.1112 0.1112 0.11 5.1.1.2.2 Testing the sharp null of no effects With a binary treatment and a binary outcome, we could test hypothesis that outcomes are totally independent of treatment assignment using what is called Fisher&#x2019;s exact test. We can also use the permutation-based approaches above to avoid relying on asymptotic (large-sample) assumptions. Below we show how Fisher&#x2019;s exact test, the Exact Cochran-Mantel-Haenszel test, and the Exact &#x3C7;\\chi&#x3C7;-squared test produce the same answers. R code Stata code Hide test2fisher &lt;- fisher.test(x=dat1$Z,y=dat1$Ybin) print(test2fisher) test2chisq &lt;- chisq_test(factor(Ybin)~factor(Z),data=dat1,distribution=exact()) print(test2chisq) test2cmh &lt;- cmh_test(factor(Ybin)~factor(Z),data=dat1,distribution=exact()) print(test2cmh) tabulate z ybin, exact tabulate z ybin, chi2 * search emh emh z ybin Fisher&apos;s Exact Test for Count Data data: dat1$Z and dat1$Ybin p-value = 0.5 alternative hypothesis: true odds ratio is not equal to 1 95 percent confidence interval: 0.5586 4.7680 sample estimates: odds ratio 1.574 Exact Pearson Chi-Squared Test data: factor(Ybin) by factor(Z) (0, 1) chi-squared = 0.89, p-value = 0.5 Exact Generalized Cochran-Mantel-Haenszel Test data: factor(Ybin) by factor(Z) (0, 1) chi-squared = 0.88, p-value = 0.5 A difference-in-proportions test can also be performed directly (rather than relying on OLS to approximate this). In that case, the null hypothesis is tested while using a binomial distribution (rather than a Normal distribution) to approximate the underlying randomization distribution. In reasonably-sized samples, both approximations perform well. R code Stata code Hide mat &lt;- with(dat1,table(Z,Ybin)) matpt &lt;- prop.test(mat[,2:1]) matpt prtest ybin, by(z) 2-sample test for equality of proportions with continuity correction data: mat[, 2:1] X-squared = 0.5, df = 1, p-value = 0.5 alternative hypothesis: two.sided 95 percent confidence interval: -0.3477 0.1344 sample estimates: prop 1 prop 2 0.5733 0.6800 5.2 Multiple tests 5.2.1 Multiple arms Multiple treatment arms can be analyzed as above, except that we now have more than one comparison between a treated group and a control group. Such studies raise both substantive and statistical questions about multiple testing (or &#x201C;multiple comparisons&#x201D;). For example, the difference_in_means function asks which average treatment effect it should estimate, and it only presents one comparison at a time. We could compare the treatment T2 with the baseline outcome of T1. But we could also compare both of T2 and T3 with T1 at the same time, as in the second set of results (lm_robust implements the same standard errors as difference_in_means, but allows for more flexible model specification). R code Stata code Hide ## Comparing only conditions 1 and 2 estAndSE3 &lt;- difference_in_means(Y~Z4arms,data=dat1,condition1=&quot;T1&quot;,condition2=&quot;T2&quot;) print(estAndSE3) ## Compare each other arm to T1 estAndSE3multarms &lt;- lm_robust(Y~Z4arms,data=dat1) print(estAndSE3multarms) ** Comparing only conditions 1 and 2 ttest y if inlist(z4arms, &quot;T1&quot;, &quot;T2&quot;), by(z4arms) unequal ** Compare each other arm to T1 encode z4arms, gen(z4num) reg y ib1.z4num, vce(hc2) // Set 1 as the reference category Design: Standard Estimate Std. Error t value Pr(&gt;|t|) CI Lower CI Upper DF Z4armsT2 0.7329 1.298 0.5647 0.5749 -1.877 3.343 47.67 Estimate Std. Error t value Pr(&gt;|t|) CI Lower CI Upper DF (Intercept) 2.5541 0.8786 2.9070 0.004532 0.8101 4.298 96 Z4armsT2 0.7329 1.2979 0.5647 0.573593 -1.8433 3.309 96 Z4armsT3 0.1798 1.1582 0.1552 0.876956 -2.1192 2.479 96 Z4armsT4 2.0372 1.2353 1.6491 0.102393 -0.4149 4.489 96 In this case, we could make ((4&#xD7;4)&#x2212;4)/2)=6((4 \\times 4)-4)/2)=6((4&#xD7;4)&#x2212;4)/2)=6 different possible comparisons between pairs of treatment groups. Consider that if there were really no effect of any treatment, and if we chose to reject the null across treatments at the standard significance threshold of &#x3B1;=.05\\alpha=.05&#x3B1;=.05, we would incorrectly claim that there was at least one effect more than 5% of the time. 1&#x2212;(1&#x2212;.05)6=.271 - ( 1 - .05)^6 = .271&#x2212;(1&#x2212;.05)6=.27, or 27% of the time, we would make a false positive error, claiming an effect existed when it did not. Two points are worth emphasizing. First, the &#x201C;family-wise error rate&#x201D; (FWER) will differ from the individual error rate of any single test. In short, performing multiple tests to draw a conclusion about a single underlying hypothesis increases our chances of incorrectly rejecting a true null at least once. Suppose that we calculated two p-values and compared each to the conventional significance level &#x3B1;=0.05\\alpha = 0.05&#x3B1;=0.05. The probability of retaining (failing to reject) both hypotheses is (1&#x2212;&#x3B1;)2=.9025(1-\\alpha)^2 = .9025(1&#x2212;&#x3B1;)2=.9025 and the probability of rejecting at least one of these hypotheses is 1&#x2212;(1&#x2212;&#x3B1;)2=.09751-(1-\\alpha)^2 = .09751&#x2212;(1&#x2212;&#x3B1;)2=.0975&#x2014;almost double our intended significance threshold of &#x3B1;=0.05\\alpha = 0.05&#x3B1;=0.05 across tests. Second, multiple tests will often be correlated, and optimal corrections for multiple testing incorporate information about these relationships. Importantly, accounting for this correlation will penalize multiple testing less than an adjustment procedure developed under an assumption that tests are uncorrelated. When we say that tests are &#x201C;correlated,&#x201D; we mean that there is some relationship between the test statistics (e.g., a student&#x2019;s t-statistic, or a &#x3C7;2\\chi^{2}&#x3C7;2 statistic) used to perform statistical inference. In other words, the test statistics are jointly distributed&#x2014;when one test statistic is higher, the other will tend to be higher as well.15 Our default procedure for evaluating multi-arm trials is as follows: First, decide on an omnibus confirmatory comparison for the entire evaluation: say, control/status quo versus receiving any version of the treatment. Such a test would likely have more statistical power than a test that evaluates each arm separately, and would also have a correctly controlled false positive rate. This would then serve as the primary finding we report. Second, perform the rest of the comparisons as exploratory analyses without multiple testing adjustment&#x2014;i.e., as analyses that may inform future projects and suggest where we might be seeing more or less of an effect, but which cannot serve as a foundation for policy conclusions on their own. Alternatively, if we do want to make multiple confirmatory comparisons, we need to adjust their collective false positive rate.16 The most appropriate method varies. Options include: canned procedures like Holm-Bonferroni adjustment (which do not account for correlations between tests); the Tukey HSD procedure for pairwise comparisons of multiple treatment arms; simulations to calculate a significance threshold that properly controls the collective error rate;17 or performing the tests in a specific, theory-informed order to protect the familywise error rate (Rosenbaum 2008). 5.2.1.1 Adjusting for multiple comparisons We review some of the adjustment options listed above in more detail, with coded examples. First, if we want to rely on relatively simple procedures that only require the p-values themselves (i.e., not the underlying data), we might hold the familywise error rate at &#x3B1;\\alpha&#x3B1; through either a single step correction (e.g.&#xA0;Bonferroni) or a stepwise correction (such as Holm-Bonferroni). We could also use a procedure that controls the false discovery rate instead (e.g., the Benjamini-Hochberg correction). These adjustments are derived assuming the included tests are uncorrelated. It is still valid to apply them to correlated tests, they simply may be too conservative (i.e., they may be suboptimal in terms of power). Our default practice when dealing with multiple uncorrelated tests is to apply the Holm-Bonferroni correction. For more, see EGAP&#x2019;s 10 Things you need to know about multiple comparisons. R code Stata code Hide ## Get p-values but exclude intercept pvals &lt;- summary(estAndSE3multarms)$coef[2:4,4] ## Illustrate different corrections (or lack thereof). ## Many of these corrections can be applied either as ## p-value adjustments, or by constructing alternative ## significance thresholds. We focus on p-value adjustments: # None round(p.adjust(pvals, &quot;none&quot;), 3) # Bonferroni round(p.adjust(pvals, &quot;bonferroni&quot;), 3) # Holm round(p.adjust(pvals, &quot;holm&quot;), 3) # Hochberg round(p.adjust(pvals, &quot;hochberg&quot;), 3) # FDR instead of FWER ** Get p-values but exclude intercept * See also: search parmest matrix pvals = r(table)[&quot;pvalue&quot;, 2..4] // save in a matrix matrix pvalst = pvals&apos; // transpose svmat pvalst, names(col) // add matrix as data in memory ** Illustrate different corrections (or lack thereof). ** Many of these corrections can be applied either as ** p-value adjustments, or by constructing alternative ** significance thresholds. We focus on p-value adjustments: * None replace pvalue = round(pvalue, 0.0001) list pvalue if !missing(pvalue) * Bonferroni * ssc install qqvalue qqvalue pvalue if !missing(pvalue), method(bonferroni) qvalue(adj_p_bonf) replace adj_p_bonf = round(adj_p_bonf, 0.0001) list adj_p_bonf if !missing(pvalue) * Holm qqvalue pvalue if !missing(pvalue), method(holm) qvalue(adj_p_holm) replace adj_p_holm = round(adj_p_holm, 0.0001) list adj_p_holm if !missing(pvalue) * Hochberg qqvalue pvalue if !missing(pvalue), method(hochberg) qvalue(adj_p_hoch) replace adj_p_hoch = round(adj_p_hoch, 0.0001) list adj_p_hoch if !missing(pvalue) // FDR instead of FWER [1] &quot;None&quot; Z4armsT2 Z4armsT3 Z4armsT4 0.574 0.877 0.102 [1] &quot;Bonferroni&quot; Z4armsT2 Z4armsT3 Z4armsT4 1.000 1.000 0.307 [1] &quot;Holm&quot; Z4armsT2 Z4armsT3 Z4armsT4 1.000 1.000 0.307 [1] &quot;Hochberg&quot; Z4armsT2 Z4armsT3 Z4armsT4 0.877 0.877 0.307 Notice that simply adjusting ppp-values from this linear model ignores the fact that we may be interested in other pairwise comparisons, such as the difference in effects between receiving T3 vs T4. And as already emphasized, we could be leaving meaningful statistical power &#x201C;on the table&#x201D; by applying procedures that ignore correlations between our test statistics. A different option when dealing with multiple comparisons is to implement a Tukey Honestly Signficant Differences (HSD) test. The Tukey HSD test (sometimes called a Tukey range test or just a Tukey test) calculates multiple-comparison-adjusted ppp-values and simultaneous confidence intervals for all pairwise comparisons in a model, while taking into account possible correlations between test statistics. It is similar to a two-sample t-test, but with built in adjustment for multiple comparisons. The test statistic for any comparison between two equally-sized groups iii and jjj is: tij=yi&#x2C9;&#x2212;yj&#x2C9;s2nt_{ij} = \\frac{\\bar{y_i}-\\bar{y_j}}{s\\sqrt{\\frac{2}{n}}}tij&#x200B;=sn2&#x200B;&#x200B;yi&#x200B;&#x2C9;&#x200B;&#x2212;yj&#x200B;&#x2C9;&#x200B;&#x200B; yi&#x2C9;\\bar{y_i}yi&#x200B;&#x2C9;&#x200B; and yj&#x2C9;\\bar{y_j}yj&#x200B;&#x2C9;&#x200B; are the means in groups iii and jjj, respectively. sss is the pooled standard deviation of the outcome, and nnn is the common sample size. A critical value is then chosen for tijt_{ij}tij&#x200B; given the desired significance level, &#x3B1;\\alpha&#x3B1;, the number of groups being compared, kkk, and the degrees of freedom, n&#x2212;kn-kn&#x2212;k. We&#x2019;ll represent this critical value with t&#x3B1;,k,nt_{\\alpha,k,n}t&#x3B1;,k,n&#x200B;. The confidence interval for any difference between equally-sized groups is then:18 [yi&#x2C9;&#x2212;yj&#x2C9;&#x2212;t&#x3B1;,k,ns2n,yi&#x2C9;&#x2212;yj&#x2C9;+t&#x3B1;,k,ns2n]\\left[\\bar{y_i}-\\bar{y_j}-t_{\\alpha,k,n}s\\sqrt{\\frac{2}{n}}\\hspace{2mm},\\hspace{2mm} \\bar{y_i}-\\bar{y_j}+t_{\\alpha,k,n}s\\sqrt{\\frac{2}{n}}\\right][yi&#x200B;&#x2C9;&#x200B;&#x2212;yj&#x200B;&#x2C9;&#x200B;&#x2212;t&#x3B1;,k,n&#x200B;sn2&#x200B;&#x200B;,yi&#x200B;&#x2C9;&#x200B;&#x2212;yj&#x200B;&#x2C9;&#x200B;+t&#x3B1;,k,n&#x200B;sn2&#x200B;&#x200B;] We present an R implementation of the Tukey HSD test using the glht() function from the multcomp package, which offers more flexiblity than the TukeyHSD in the base stats package (at the price of a slightly more complicated syntax). We also illustrate a few options in Stata. But to perform this post-hoc test, we first need to fit an applicable model. R code Stata code Hide ## We can use aov() or lm() dat1$Z4factor &lt;- as.factor(dat1$Z4arms) aovmod &lt;- aov(Y~Z4factor, dat1) ##lmmod &lt;- lm(Y~Z4arms, dat1) anova y z4num In R, using the glht() function&#x2019;s linfcnt argument, we tell the function to conduct a Tukey test of all pairwise comparisons for our treatment indicator, ZZZ. In Stata, we can do this using the tukeyhsd command or pwcompare commands. R code Stata code Hide tukey_mc &lt;- glht(aovmod, linfct = mcp(Z4factor = &quot;Tukey&quot;)) summary(tukey_mc) * search tukeyhsd * search qsturng tukeyhsd z4num * Or: pwcompare z4arms, mcompare(tukey) effects Simultaneous Tests for General Linear Hypotheses Multiple Comparisons of Means: Tukey Contrasts Fit: aov(formula = Y ~ Z4factor, data = dat1) Linear Hypotheses: Estimate Std. Error t value Pr(&gt;|t|) T2 - T1 == 0 0.733 1.226 0.60 0.93 T3 - T1 == 0 0.180 1.226 0.15 1.00 T4 - T1 == 0 2.037 1.226 1.66 0.35 T3 - T2 == 0 -0.553 1.226 -0.45 0.97 T4 - T2 == 0 1.304 1.226 1.06 0.71 T4 - T3 == 0 1.857 1.226 1.51 0.43 (Adjusted p values reported -- single-step method) Focusing on the R results, we can then plot the 95% family wise confidence intervals for these comparisons. ## Save dfault ploting parameters op &lt;- par() ## Add space to left-hand outer margin par(oma = c(1, 3, 0, 0)) plot(tukey_mc) We can also obtain simultaneous confidence intervals at other levels of statistical significance using the confint() function. ## Generate and plot 90% confidence intervals tukey_mc_90ci &lt;- confint(tukey_mc, level = .90) plot(tukey_mc_90ci) ## Restore plotting defaults, now par(op) See also, in R: pairwise.prop.test for binary outcomes. Finally, when test statistics are correlated but a Tukey-styled test is not desirable, we could instead employ randomization inference. To do this, we would start by simulating the distribution of p-values observed for each test across many random permutations of treatment (estimating the &#x201C;randomization distribution&#x201D; of our correlated test statistics under the sharp null). After that, we would use the distribution of p-values to calculate an alternative significance threshold. By comparing our real p-values to this simulated threshold, we could evaluate statistical significance while properly controling the FWER. We provide examples below for R and Stata. See step 7 on this page for an alternative R code example. In the code below, we first define a function to randomly re-assign treatment and estimate a p-value for each regression coefficient (based on HC2 errors). We repeatedly call this function 1000 times and save the resulting distribution of p-values. We then define two more functions (find_threshold1 and find_threshold2) that illustrate different methods of calculating an appropriate significance threshold using those p-values (the first may be a little slower, but it walks through the logic in more detail). We also use this simulation to illustrate the advantages of an omnibus approach where we simply perform our confirmatory test using an alternative indicator for receipt of one of the treatment arms. Both calculation approaches yield similar significance thresholds. And the collective false positive rate when making multiple comparisons is noticably higher than when making a single, omnibus comparison. R code Stata code Hide ## Function to permute treatment and generate ## p-values / null rejection for each of the ## three comparisons (i.e., arm 1 vs each of the other 3) draw_multiple_arms &lt;- function() { ## Generate a new simulated treatment variable ## (same approach as in chapter 4 to produce the original) dat1$Z4arms_mt &lt;- complete_ra(nrow(dat1), m_each=rep(nrow(dat1)/4,4)) ## Generate a binary indicator for being either T2 or T3. dat1$treat_any_mt &lt;- ifelse(dat1$Z4arms_mt == &quot;T1&quot;, 0, 1) ## Regress the outcome on treatment dummies. mod_each &lt;- lm_robust(Y ~ as.factor(Z4arms_mt), data = dat1) ## Regress the outcome on the &quot;any treatment&quot; dummy mod_any &lt;- lm_robust(Y ~ treat_any_mt, data = dat1) ## Is there at least one null rejection ## for the treatment dummies in mod_each? each &lt;- sum(mod_each$p.value[2:4] &lt;= 0.05) &gt; 0 ## Is there a null rejection in mod_any? any &lt;- mod_any$p.value[2] &lt;= 0.05 ## Prepare output (the p-values are used for ## a simulation-based FWER adjustment below) out &lt;- c(each, any, mod_each$p.value[2:4]) names(out) &lt;- c(&quot;unadjusted&quot;, &quot;any&quot;, &quot;pT2&quot;, &quot;pT3&quot;, &quot;pT4&quot;) return(out) } ## Calculate the output of that function 1000 times multiple_arms &lt;- ## Default output will be a list replicate( n = 1000, draw_multiple_arms(), simplify = F ) %&gt;% ## Stack the results from each draw as a df instead purrr::reduce(bind_rows) ## A function to use the p-values produced by the above to ## calculate a simulated significance threshold ## that controls the FWER rate. ## METHOD 1: try many possible thresholds, ## and find the largest one that sets the FWER to ## at least X% (default = 5). Slower, but ## makes the logic of this procedure clearer. ## P-values is a df/matrix where each column is ## the p-value for a different test, and each row ## is a set of p-values from a different iteration. find_threshold1 &lt;- function( p_values, # Df of p-values (columns = tests, rows = iterations) from = 0.001, # Lowest possible threshold to try to = 0.999, # Highest to try by = 0.001, # Increments to try sig_level = 0.05 # Desired max FWER (FW significance level) ) { ## Generate the sequence of thresholds to test test_thresholds &lt;- seq(from, to, by) ## Create a df for each test_threshold: ## threshold and resulting FWER. candidates &lt;- lapply( ## To each test threshold... test_thresholds, ## ... Apply the following function ## (.x is the test threshold): function(.x) { ## Convert each p-value to a null rejection indicator significant &lt;- p_values &lt;= .x ## By row: at least one rejection in this family of tests? at_least_one &lt;- rowSums(significant) &gt;= 1 ## From this, we can get a FWER type_I_rate &lt;- mean(at_least_one) ## Prepare output out &lt;- c(.x, type_I_rate) names(out) &lt;- c(&quot;threshold&quot;, &quot;fwer&quot;) return(out) } ) %&gt;% purrr::reduce(bind_rows) ## Keep only candidate thresholds with less than or ## equal to the target set above (default 5%). candidates &lt;- candidates[ candidates$fwer &lt;= sig_level, ] ## Find the largest threshold satisfying that criterion. candidates &lt;- candidates[ order(-candidates$threshold), ] return(candidates$threshold[1]) } ## Apply this function to calculate the adjusted ## significance threshold sim_alpha1 &lt;- find_threshold1(multiple_arms[, 3:5]) ## Method 2: A less intuitive shortcut that runs faster find_threshold2 &lt;- function(p_values) { ## Get the minimum p-value in each family of tests min_p_infamily &lt;- apply(p_values, 1, min) ## The 5th percentile of these is (approx.) ## the simulated threshold we want. p5 &lt;- quantile(min_p_infamily, 0.05) return(p5) } ## Apply this function to calculate the adjusted ## significance threshold sim_alpha2 &lt;- as.numeric(find_threshold2(multiple_arms[, 3:5])) ## Compare the three simulation approaches: print(&quot;sim_alpha1&quot;) sim_alpha1 print(&quot;sim_alpha2&quot;) sim_alpha2 ## Notice: the false positive rate is too high ## across replications without accounting for ## multiple tests, but is closer to the right value ## when relying on the simpler &quot;omnibus&quot; indicator ## for receipt of some treatment. print(&quot;False positive rate: separate tests&quot;) mean(multiple_arms$unadjusted) print(&quot;False positive rate: omnibus test&quot;) mean(multiple_arms$any) ** Program to permute treatment and generate ** p-values / null rejection for each of the ** three comparisons (i.e., arm 1 vs each of the other 3) capture program drop draw_multiple_arms program define draw_multiple_arms, rclass ** Generate a new simulated treatment variable ** (same approach as in chapter 4 to produce the original) capture drop z4arms_mt qui count local count_list = r(N)/4 local count_list : di _dup(4) &quot;`count_list&apos; &quot; qui complete_ra z4arms_mt, m_each(`count_list&apos;) ** Generate a binary indicator for being either T2 or T3 capture drop treat_any_mt qui gen treat_any_mt = cond(z4arms_mt == 1, 0, 1) ** Regress the outcome on treatment dummies. ** Any of the p-values statistically significant? qui reg y i.z4arms_mt, vce(hc2) local each = (r(table)[4,2] &lt;= 0.05) | (r(table)[4,3] &lt;= 0.05) | (r(table)[4,4] &lt;= 0.05) return scalar each = `each&apos; return scalar pT2 = r(table)[4,2] return scalar pT3 = r(table)[4,3] return scalar pT4 = r(table)[4,4] ** Regress the outcome on the &quot;any treatment&quot; dummy qui reg y treat_any_mt, vce(hc2) local any = (r(table)[4,1] &lt;= 0.05) return scalar any = `any&apos; end ** Program to avoid some typing when specifying what we want from simulate capture program drop simlist program define simlist local rscalars : r(scalars) global sim_targets &quot;&quot; // must be a global foreach item of local rscalars { global sim_targets &quot;$sim_targets `item&apos; = r(`item&apos;)&quot; } end ** Calculate the output of that function 1000 times draw_multiple_arms simlist simulate $sim_targets, reps(1000) nodots: draw_multiple_arms ** A program to use the p-values produced by the above to ** calculate a simulated significance threshold ** that controls the FWER rate. ** METHOD 1: try many possible thresholds, ** and find the largest one that sets the FWER to ** at least X% (default = 5). Slower, but ** makes the logic of this procedure clearer. ** P-values is a varlist of vars storing p-values ** from different tests (rows are different iterations, ** columns are assumed to represent different tests) capture program drop find_threshold1 program define find_threshold1, rclass syntax, pvalues(varlist) /// P-value varnames [ to_val(real 0.005) /// Lowest possible threshold to try from_val(real 0.15) /// Highest to try (starts here) by_val(real 0.001) /// Increments to try sig_level(real 0.05) ] // Desired FWER (FW significance level) ** Make list of candidate thresholds numlist &quot;`from_val&apos;(-`by_val&apos;)`to_val&apos;&quot; local candidate_thresholds `r(numlist)&apos; ** Minimum p-value across tests tempvar minimum_p_value qui egen `minimum_p_value&apos; = rowmin(`pvalues&apos;) ** Loop through candidate thresholds foreach l of local candidate_thresholds { ** Indicator for at least one test in each randomization being significant ** under the candidate threshold in question. If the minimum ** is not significant, none of them will be. tempvar at_least_one qui gen `at_least_one&apos; = `minimum_p_value&apos; &lt;= `l&apos; ** Get the mean, representing the family-wise type I error rate ** for this threshold. It will be stored internally in r(). qui sum `at_least_one&apos;, meanonly ** Evaluate if we&apos;re at the target FW type I error rate yet. ** If we are, save the threshold and stop the loop. if (`r(mean)&apos; &lt;= `sig_level&apos;) { return scalar alpha_fwer = `l&apos; continue, break } } end ** Apply this program to calculate the adjusted ** significance threshold find_threshold1, pvalues(pT3 pT2 pT4) global sim_alpha1 = r(alpha_fwer) ** Method 2: A less intuitive shortcut that runs faster capture program drop find_threshold2 program define find_threshold2, rclass syntax, pvalues(varlist) ** Get the minimum p-value in each family of tests capture drop minp egen minp = rowmin(`pvalues&apos;) qui sum minp, d ** The 5th percentile of these is (approx.) ** the simulated threshold we want. local alpha_fwer = round(`r(p5)&apos;, 0.0001) return scalar alpha_fwer = `alpha_fwer&apos; end ** Apply this program to calculate the adjusted ** significance threshold find_threshold2, pvalues(pT3 pT2 pT4) global sim_alpha2 = r(alpha_fwer) ** Compare the two simulation approaches: macro list sim_alpha1 macro list sim_alpha2 ** Notice: the false positive rate is too high ** across replications without accounting for ** multiple tests, but is closer to the right value ** when relying on the simpler &quot;omnibus&quot; indicator ** for receipt of some treatment. qui sum each, meanonly di r(mean) qui sum any, meanonly di r(mean) [1] &quot;sim_alpha1&quot; [1] 0.019 [1] &quot;sim_alpha2&quot; [1] 0.01935 [1] &quot;False positive rate: separate tests&quot; [1] 0.133 [1] &quot;False positive rate: omnibus test&quot; [1] 0.058 5.2.2 Multiple outcomes Our studies also often involve more than one outcome measure. Assessing the effects of even a simple two-arm treatment across 10 different outcomes can raise the same kinds of questions that come up in the context of multi-arm trials. Classic adjustments such as the Bonferroni and Holm procedures (which ignore correlations between test statistics), testing hypotheses in a pre-specified order (Rosenbaum 2008), and randomization inference simulations could all be appropriate adjustments to apply when dealing with multiple outcomes (whereas the Tukey HSD procedure is specifically for multiple comparisons). Additionally, as we did with multiple comparisons, we can perform omnibus tests for an overall treatment effect across multiple outcomes. One method we&#x2019;ll highlight is creating a &#x201C;stacked&#x201D; regression model, where each observation has a separate row in the dataset for each outcome measure (Oberfichtner and Tauchmann 2021).19 A modified version of the original model with unit-clustered errors can then be fit to the stacked dataset, evaluating joint significance across outcomes using a Wald test (see chapter 4 for a summary of Wald tests). This approach usually yields the same or similar results to fitting separate tests for each outcome in a &#x201C;seemingly unrelated estimation&#x201D; (SUE) framework (Weesie 2000). Which method is more convenient depends on the situation. We focus on stacked regression, but see suest in Stata or systemfit in R for guidance on implementing SUE. The code below illustrates the stacked regression method using two simulated outcomes drawn from a multivariate normal distribution. First, we need to prepare the data. R code Stata code Hide ## Jointly draw correlated outcomes from a multivariate normal mu &lt;- c(1, 3) sigma &lt;- rbind( c(1, 0.2), c(0.2, 3) ) dat1[, c(&quot;y_mo_1&quot;, &quot;y_mo_2&quot;)] &lt;- as.data.frame(mvrnorm(n = nrow(dat1), mu = mu, Sigma = sigma)) ## Transform outcomes stored as separate variables into ## separate observations instead. dat1stack &lt;- pivot_longer(dat1, cols = c(&quot;y_mo_1&quot;, &quot;y_mo_2&quot;), names_to = &quot;outcome&quot;, values_to = &quot;stacky&quot;) ## Review head(dat1stack[, c(&quot;id&quot;, &quot;outcome&quot;, &quot;stacky&quot;)]) ** Jointly draw correlated outcomes from a multivariate normal matrix sigma = (1, 0.2 \\ .2, 3) drawnorm y_mo_1 y_mo_2, cov(sigma) ** Transform outcomes stored as separate variables into ** separate observations instead. tempfile return_to save `return_to&apos;, replace keep id y_mo_1 y_mo_2 z2armequal reshape long y_mo_, i(id) j(outcome) ** Review list in 1/6 # A tibble: 6 &#xD7; 3 id outcome stacky &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; 1 1 y_mo_1 0.667 2 1 y_mo_2 2.80 3 2 y_mo_1 -0.332 4 2 y_mo_2 2.32 5 3 y_mo_1 0.445 6 3 y_mo_2 1.43 Next, as above, we define a function to repeatedly permute treatment, estimate an omnibus p-value for an effect across outcomes using a stacked regression, and compare this to the collective false positive rate we observe when evaluating the tests separately. We perform the simulation by permuting treatment just to illustrate the collective false positive rate in a case where we know the null of no effect is true. R code Stata code Hide ## As before, define a function to repeatedly ## permute treatment and estimate p-value draw_multiple_outcomes &lt;- function() { ## Generate a new simulated treatment variable dat1$Z2armEqual_mo &lt;- complete_ra(nrow(dat1)) ## Merge into the stacked data stack_merge &lt;- merge(dat1stack, dat1[, c(&quot;id&quot;, &quot;Z2armEqual_mo&quot;)], by = &quot;id&quot;) stack_merge$out1 &lt;- ifelse(stack_merge$outcome == &quot;y_mo_1&quot;, 1, 0) ## Fit a stacked regression model (unrestricted) mod_stack &lt;- lm( stacky ~ Z2armEqual_mo * out1, data = stack_merge ) ## Fit restricted model for the Wald test mod_restr &lt;- lm( stacky ~ out1, data = stack_merge ) ## Variance-covariance matrix of unrestricted model vcov &lt;- vcovCL(mod_stack, type = &quot;HC2&quot;, cluster = stack_merge$id) ## Wald test: joint significance of treatment effect ## and difference across outcomes. Could also use ## linearHypothesis from car with the following constraints: ## c(&quot;Z2armEqual_mo&quot;, &quot;Z2armEqual_mo:out1&quot;) or ## c(&quot;Z2armEqual_mo&quot;, &quot;&quot;Z2armEqual_mo + Z2armEqual_mo:out1&quot;) waldp &lt;- waldtest(mod_stack, mod_restr, vcov = vcov)$`Pr(&gt;F)`[2] ## Model each outcome separately pO1 &lt;- summary(lm_robust(y_mo_1 ~ Z2armEqual_mo, data = dat1))$coefficients[2,4] pO2 &lt;- summary(lm_robust(y_mo_2 ~ Z2armEqual_mo, data = dat1))$coefficients[2,4] ## Prepare output out &lt;- c(waldp, pO1, pO2) names(out) &lt;- c(&quot;waldp&quot;, &quot;p01&quot;, &quot;p02&quot;) return(out) } ## Calculate the output of that function 1000 times multiple_outcomes &lt;- ## Default output will be a list replicate( n = 1000, draw_multiple_outcomes(), simplify = F ) %&gt;% ## Stack the results from each draw as a df instead purrr::reduce(bind_rows) ## False positive rate: stacked print(&quot;False positive rate: stacked&quot;) mean(multiple_outcomes$waldp &lt;= 0.05) ## False positive rate: separate print(&quot;False positive rate: separate tests&quot;) mean(rowSums(multiple_outcomes[,c(2:3)] &lt;= 0.05) &gt; 0) ** As before, define a function to repeatedly ** permute treatment and estimate p-value capture program drop draw_multiple_outcomes program define draw_multiple_outcomes, rclass ** Generate a new simulated treatment variable capture drop z2armequal_mo preserve qui keep id qui duplicates drop qui count local count_list = r(N)/2 local count_list : di _dup(2) &quot;`count_list&apos; &quot; qui complete_ra z2armequal_mo, m_each(`count_list&apos;) tempfile newtreat save `newtreat&apos;, replace restore ** Merge into the stacked data qui merge m:1 id using `newtreat&apos; assert _merge == 3 qui drop _merge ** Fit a stacked regression model qui reg y_mo_ z2armequal_mo##i.outcome, vce(cluster id) ** Wald test: joint significance of treatment effect ** and difference across outcomes. In this case we just specify ** restrictions/tests directly instead of fitting separate models. qui test ((1.z2armequal_mo#2.outcome+1.z2armequal_mo) = 0) (1.z2armequal_mo = 0) local waldp = r(p) &lt;= 0.05 return scalar waldp = `waldp&apos; ** Model each outcome separately preserve qui keep if outcome == 1 qui reg y_mo_ z2armequal_mo, vce(hc2) local p1 = r(table)[4,1] restore preserve qui keep if outcome == 2 qui reg y_mo_ z2armequal_mo, vce(hc2) local p2 = r(table)[4,1] restore local each = (`p1&apos; &lt;= 0.05) | (`p2&apos; &lt;= 0.05) return scalar each = `each&apos; end ** Calculate the output of that function 1000 times draw_multiple_outcomes simlist simulate $sim_targets, reps(1000) nodots: draw_multiple_outcomes ** False positive rate: stacked qui sum waldp, meanonly di r(mean) ** False positive rate: separate qui sum each, meanonly di r(mean) ** Return to primary data use `return_to&apos;, clear [1] &quot;False positive rate: stacked&quot; [1] 0.056 [1] &quot;False positive rate: separate tests&quot; [1] 0.104 5.2.3 When is this necessary? So far, we&#x2019;ve focused on the mechanics of performing different kinds of adjustments (for different outcomes, multiple comparisons, multiple model specifications we are agnostic about, etc.). But we&#x2019;ve avoided a more fundamental question: how do we determine when adjustment is necessary in the first place? Failing to apply a multiple testing correction when it is needed yields overconfident inferences. But adjusting for multiple testing when it is not needed yields overly conservative inferences! Both miscalculations work against our ability to correctly control error rates, one of our key design criteria (chapter 2). This is an area where our thinking at OES has evolved over time. To see the logic for our current approach, consider the two hypothetical examples below. In Example 1, we want to evaluate whether outreach increases enrollment in a public benefit program. Past research doesn&#x2019;t clarify which available outreach method is most effective, so we design a four-arm trial with a control group and three different intervention groups (texts, emails, and phone calls). We decide that if we see statistically significant evidence for the effectiveness of any treatment arm, this is sufficient to reject the joint null hypothesis of no effect across outreach methods, corresponding to H0,jointH_{0, joint}H0,joint&#x200B; below, where &#x2229;\\cap&#x2229; means &#x201C;and.&#x201D; Our primary concern is whether at least one intervention works. H0,text:&#x3C4;text=0H_{0, text} : \\tau_{text} = 0H0,text&#x200B;:&#x3C4;text&#x200B;=0 H0,email:&#x3C4;email=0H_{0, email} : \\tau_{email} = 0H0,email&#x200B;:&#x3C4;email&#x200B;=0 H0,phone:&#x3C4;phone=0H_{0, phone} : \\tau_{phone} = 0H0,phone&#x200B;:&#x3C4;phone&#x200B;=0 H0,joint=H0,text&#x2229;H0,email&#x2229;H0,phoneH_{0, joint} = H_{0, text} \\cap H_{0, email} \\cap H_{0, phone}H0,joint&#x200B;=H0,text&#x200B;&#x2229;H0,email&#x200B;&#x2229;H0,phone&#x200B; In contrast, in Example 2, we are evaluating two separate interventions to process benefit applications more quickly. This is a three-arm trial with a control group and two intervention groups, but the interventions are unrelated. We can justify each intervention separately, and there is no joint null we are interested in. Instead, we simply want to reject the constituent null hypotheses for each intervention, H0,int1H_{0, int1}H0,int1&#x200B; and H0,int2H_{0, int2}H0,int2&#x200B;. H0,int1:&#x3C4;int1=0H_{0, int1} : \\tau_{int1} = 0H0,int1&#x200B;:&#x3C4;int1&#x200B;=0 H0,int2:&#x3C4;int2=0H_{0, int2} : \\tau_{int2} = 0H0,int2&#x200B;:&#x3C4;int2&#x200B;=0 In Example 1, the joint null of no effect across outreach methods is the intersection of three constituent null hypotheses. Rejecting this null implies accepting the union of their alternative hypotheses (where &#x222A;\\cup&#x222A; means &#x201C;or,&#x201D; and we have HA,method:&#x3C4;method&#x2260;0H_{A, method} : \\tau_{method} \\neq 0HA,method&#x200B;:&#x3C4;method&#x200B;&#xE020;=0): HA,joint=HA,text&#x222A;HA,email&#x222A;HA,phoneH_{A, joint} = H_{A, text} \\cup H_{A, email} \\cup H_{A, phone}HA,joint&#x200B;=HA,text&#x200B;&#x222A;HA,email&#x200B;&#x222A;HA,phone&#x200B; Rejecting any one of the constituent null hypotheses is grounds to reject H0,jointH_{0, joint}H0,joint&#x200B;. This is sometimes called a union-intersection test (Rubin 2024). To maintain appropriate error rate control, we apply multiple testing adjustment whenever we base a conclusion on a union-intersection test. These are the situations where we are most concerned about being mislead by inflated error rates: where we are drawing one underlying conclusion based on the results of multiple tests. In contrast, in Example 2, the interventions are justified separately, and the truth of H0,int1H_{0, int1}H0,int1&#x200B; presumably has no bearing on the truth of H0,int2H_{0, int2}H0,int2&#x200B;. We have determined that it is not useful to only know that at least one of the two modifications decreased application processing times (i.e., rejecting the intersection of their nulls is not informative)&#x2014;we need to know whether each modification on its own was effective. Therefore, we would not adjust for multiple testing. The collective false positive rate is still inflated, but we are not drawing a collective conclusion across tests, and the Type I error rates for the separate tests remain uninflated (Rubin 2024). Returning to the three simulated p-values from the multiple arms adjustment example code, we can see that the estimated false positive rate under a true null is approximately correct when each test is considered alone. colMeans(multiple_arms[,c(3:5)] &lt;= 0.05) pT2 pT3 pT4 0.052 0.058 0.055 Lastly, consider the reverse of example 1: we wish to determine whether there is sufficient evidence that all three outreach methods work. This is sometimes called an intersection-union test. We would again not adjust for multiple testing when performing an intersection-union test. The risk of seeing at least one false positive is not compelling if our conclusion depends on seeing supportive results for all three outreach methods, rather than only one. In sum, at OES we prefer to adjust for multiple testing when drawing conclusions based on union-intersection tests, but not in other cases. Put more simply, are we drawing a conclusion after looking for statistical significance in at least one of several tests? If so, we adjust for multiple testing. But otherwise we often elect not to. Researchers sometimes seek to define &#x201C;families&#x201D; of related tests to help determine where multiple testing adjustments are or are not needed. By the logic outlined above, two tests might be thought of as in the same &#x201C;family&#x201D; if they contribute to the same joint null hypothesis, as defined above for union-intersection tests (H0,joint:H0,1&#x2229;H0,2&#x2229;...)(H_{0, joint} : H_{0, 1} \\cap H_{0, 2} \\cap ... )(H0,joint&#x200B;:H0,1&#x200B;&#x2229;H0,2&#x200B;&#x2229;...). Determining which case above best represents any given evaluation is a question of theory and policy relevance as much as it is a statistical question. It may depend on what we hope to learn from an evaluation, and what kinds of conclusions would be most useful for our partners. It helps to carefully lay out and justify the claims we wish to make or evaluate as a part of pre-registering our analysis plans. 5.3 Covariance adjustment When we have additional information about experimental units (covariates), we can use this to increase the the statistical power of our tests. When possible, we use this information during the design phase to randomize interventions within blocks. But we also often pre-specify covariance adjustment as a part of our analyses. Researchers typically do this by regressing the outcome on a treatment indicator and a set of linear, additive terms for each covariate (like XiX_{i}Xi&#x200B; in equation 3 below). This estimator of the average treatment effect can be biased, though that bias usually reduces quickly as sample size increases (see below). The unadjusted difference in means, in constrast, is unbiased. However, as Lin (2013) points out, this is not reassuring if we think adjusting for covariates is necessary (e.g., to account for meaningful imbalances that persist despite randomization). Moreover, standard linear covariance adjustment is actually counter-productively less efficient than an unadjusted difference in means in some situations (Freedman 2008a).20 An approach to covariance adjustment that we prefer called the Lin estimator or Lin adjustment performs better&#x2014;at worst, it will be no less asymptotically efficient than an unadjusted difference in means (Lin 2013). We explain how it is implemented and provide more intuition for its superior efficiency below. Like linear covariance adjustment, the Lin estimator can also be (often negligibly) biased in finite samples. But note that it will be unbiased if it correctly models the true conditional expectation of YYY given treatment and the covariates (Negi and Wooldridge 2021; Lin 2013). For instance, this condition is met when the model is &#x201C;saturated&#x201D; with controls for every combination of covariate and treatment values (Angrist and Pischke 2009), such as when using the Lin estimator to adjust for a set of dummies representing block fixed effects (Miratrix, Sekhon, and Yu 2013). Notice that we couldn&#x2019;t meet this condition under linear adjustment for block fixed effects, since we would by definition not be &#x201C;saturating&#x201D; our model with the terms that estimate separate block fixed effects in each treatment arm. The precision-loss attributable to standard linear covariance adjustment is sometimes small in large samples, especially relative to the loss of precision that would be neccesary to change our statistical conclusions. Yet it is frequently costless to use the Lin estimator in large samples, so it is still our default recommendation in these cases (see this page as well). That said, we do sometimes encounter situations where still prefer to stick with standard linear covariance adjustment, either on practical or statistical grounds. A few examples: The randomization design has only two arms, and the probability of receiving treatment is approximately 0.5 We do not expect the covariates we observe to be associated with meaningful treatment effect heterogeneity The number of model parameters is large enough relative to the sample size that the loss of degrees of freedom from using Lin adjustment may be substantial (the number of terms added is the number of controls multipled by the number of treatments); Lin adjustment is only more efficient asymptotically We expand on some of these issues in the subsections below, with more detail on how Lin estimation works. We also provide a simulated example illustrating hypothetical power differences between standard covariate adjustment and Lin adjustment in Chapter 6. 5.3.1 Possible bias in the least squares ATE estimator with covariates When we estimate the average treatment effect using least squares we tend to say that we &#x201C;regress&#x201D; some outcome for each unit iii, YiY_iYi&#x200B;, on (often binary) treatment assignment, ZiZ_iZi&#x200B;, where Zi=1Z_i=1Zi&#x200B;=1 if a unit is assigned to treatment and 0 if assigned to control. And we write a linear model relating ZZZ and YYY as below, where &#x3B2;1\\beta_1&#x3B2;1&#x200B; represents the difference in means of YYY between units with Z=1Z=1Z=1 and Z=0Z=0Z=0: Yi=&#x3B2;0+&#x3B2;1Zi+ei\\begin{equation} Y_i = \\beta_0 + \\beta_1 Z_i + e_i \\end{equation}Yi&#x200B;=&#x3B2;0&#x200B;+&#x3B2;1&#x200B;Zi&#x200B;+ei&#x200B;&#x200B;&#x200B; This is a common practice because we know that the formula to estimate &#x3B2;1\\beta_1&#x3B2;1&#x200B; in Equation (1) is the same as the difference-in-means when comparing YYY across the treatment and control groups: &#x3B2;^1=Y&#x203E;Z=1&#x2212;Y&#x203E;Z=0=Cov(Y,Z)Var(Z).\\begin{equation} \\hat{\\beta}_1 = \\overline{Y}_{Z=1} - \\overline{Y}_{Z=0} = \\frac{\\mathrm{Cov}(Y,Z)}{\\mathrm{Var}(Z)}. \\end{equation}&#x3B2;^&#x200B;1&#x200B;=YZ=1&#x200B;&#x2212;YZ=0&#x200B;=Var(Z)Cov(Y,Z)&#x200B;.&#x200B;&#x200B; This last term, expressed with covariances and variances, is the expression for the slope coefficient in a bivariate OLS regression model. This estimator of the average treatment effect has no systematic error (i.e., it is unbiased), so we can write ER(&#x3B2;^1)=&#x3B2;1&#x2261;ATEE_R(\\hat{\\beta}_1)=\\beta_1 \\equiv \\text{ATE}ER&#x200B;(&#x3B2;^&#x200B;1&#x200B;)=&#x3B2;1&#x200B;&#x2261;ATE, where ER(&#x3B2;^1)E_R(\\hat{\\beta}_1)ER&#x200B;(&#x3B2;^&#x200B;1&#x200B;) refers to the expectation of &#x3B2;^1\\hat{\\beta}_{1}&#x3B2;^&#x200B;1&#x200B; across randomizations consistent with the experimental design. Sometimes we have an additional (pre-treatment) covariate, XiX_iXi&#x200B;, commonly included in the analysis as follows: Yi=&#x3B2;0+&#x3B2;1Zi+&#x3B2;2Xi+ei\\begin{equation} Y_i = \\beta_0 + \\beta_1 Z_i + \\beta_2 X_i + e_i \\end{equation}Yi&#x200B;=&#x3B2;0&#x200B;+&#x3B2;1&#x200B;Zi&#x200B;+&#x3B2;2&#x200B;Xi&#x200B;+ei&#x200B;&#x200B;&#x200B; What is &#x3B2;1\\beta_1&#x3B2;1&#x200B; in this case? The matrix representation here is: (&#x3B2;XT&#x3B2;X)&#x2212;1&#x3B2;XT&#x3B2;y(\\beta X^{T}\\beta X)^{-1}\\beta X^{T}\\beta y(&#x3B2;XT&#x3B2;X)&#x2212;1&#x3B2;XT&#x3B2;y. But it will be more useful to examine the scalar formula: &#x3B2;^1=Var(X)Cov(Z,Y)&#x2212;Cov(X,Z)Cov(X,Y)Var(Z)Var(X)&#x2212;Cov(Z,X)2\\hat{\\beta}_1 = \\frac{\\mathrm{Var}(X)\\mathrm{Cov}(Z,Y) - \\mathrm{Cov}(X,Z)\\mathrm{Cov}(X,Y)}{\\mathrm{Var}(Z)\\mathrm{Var}(X) - \\mathrm{Cov}(Z,X)^2}&#x3B2;^&#x200B;1&#x200B;=Var(Z)Var(X)&#x2212;Cov(Z,X)2Var(X)Cov(Z,Y)&#x2212;Cov(X,Z)Cov(X,Y)&#x200B; In large experiments Cov(X,Z)&#x2248;0\\mathrm{Cov}(X,Z) \\approx 0Cov(X,Z)&#x2248;0 because ZZZ is randomly assigned and is thus independent of background variables like XXX. However in any given finite sized experiment Cov(X,Z)&#x2260;0\\mathrm{Cov}(X,Z) \\ne 0Cov(X,Z)&#xE020;=0, so this does not reduce to an unbiased estimator as it does in the bivariate case. Thus, Freedman (2008a) showed that there is a risk of bias in using the equation above to estimate the average treatment effect. In contrast, the unadjusted difference in means will be unbiased. As discussed above, this kind of covariance adjustment will also sometimes counterproductively decrease asmyptotic efficiency relative to an unadjusted difference in means, or may simply fail to improve efficiency as much as we hope. To resolve that efficiency issue, Lin (2013) suggests the following least squares approach&#x2014;regressing the outcome on binary treatment assignment ZiZ_iZi&#x200B; and its interaction with mean-centered covariates:21 Yi=&#x3B2;0+&#x3B2;1Zi+&#x3B2;2(Xi&#x2212;X&#x2C9;)+&#x3B2;3Zi(Xi&#x2212;X&#x2C9;)+ei\\begin{equation} Y_i = \\beta_0 + \\beta_1 Z_i + \\beta_2 ( X_i - \\bar{X} ) + \\beta_3 Z_i (X_i - \\bar{X}) + e_i \\end{equation}Yi&#x200B;=&#x3B2;0&#x200B;+&#x3B2;1&#x200B;Zi&#x200B;+&#x3B2;2&#x200B;(Xi&#x200B;&#x2212;X&#x2C9;)+&#x3B2;3&#x200B;Zi&#x200B;(Xi&#x200B;&#x2212;X&#x2C9;)+ei&#x200B;&#x200B;&#x200B; When implementing this covariance adjustment strategy, remember that every covariate, including binary indicators used to estimate fixed effects or to control for values of categorical covariates, should be mean-centered and interacted with treatment. For instance, imagine a design with two treatment arms (treatment vs control) and three covariates. Linear adjustment for these covariates would involve fitting an OLS regression with four slope coefficients (one for the treatment group, and one for each covariate). Lin adjustment for these covariates would instead involve fitting a regression with seven slope coefficients (one for the treatment group, one for each mean-centered covariate, and one for each interaction of treatment with a mean-centered covariate).22 See the Green-Lin-Coppock SOP for more examples of this approach to covariance adjustment. While both Lin (2013) and Freedman (2008a) focus on a design based framework, Negi and Wooldridge (2021) discuss the Lin estimator (&#x201C;full regression adjustment&#x201D;) in a more traditional sampling based framework. 5.3.2 Illustrating the Lin Approach to Covariance Adjustment Here, we show how typical covariance adjustment can lead to bias or inefficiency in estimation of the average treatment effect, increasing the overall RMSE (root mean squared error, which is influenced by both bias and variance)&#x2014;and how using the Lin procedure (or increasing sample size) can reduce the RMSE. In this case, we compare an experiment with 20 units to an experiement with 100 units, in each case with half of the units assigned to treatment by complete random assignment. We&#x2019;ll use the DeclareDesign package in the R code to make this process of writing a simulation to assess bias easier.23 Much of the R code that follows is providing instructions to the diagnose_design command, which repeats the design of the experiment many times, each time estimating an average treatment effect, and comparing the mean of those estimate to the truth (labeled &#x201C;Mean Estimand&#x201D; below). We also provide code illustrating how you could accomplish something similar in Stata using manually defined programs and the simulate command. The true potential outcomes in these example data (y1 and y0) were generated using one covariate, called cov2, with no treatment effect. In what follows, we compare the performance of (1) the simple estimator using OLS to (2) estimators that use Lin&#x2019;s procedure involving just the correct covariate, and also to (3) estimators that use incorrect covariates (since we rarely know exactly the covariates that help generate any given behavioral outcome). We&#x2019;ll break this code up into sections to help with legibility. First, in R, we prepare design objects (a class used by DeclareDesign) for the n=20n=20n=20 and n=100n=100n=100 designs. Meanwhile, in Stata, we write a program to randomly generate a sample dataset; this program will then be iterated below. In both the R and Stata simulations, we follow a design-based philosophy in which randomness in our estimates across simulations comes only from variation in treatment assignment. R code Stata code Hide ## Keep a dataframe of select variables wrkdat1 &lt;- dat1 %&gt;% dplyr::select(id,y1,y0,contains(&quot;cov&quot;)) ## Declare this as our larger population ## (an experimental sample of 100 units) popbigdat1 &lt;- declare_population(wrkdat1) ## A dataset to represent a smaller experiment, ## or a cluster randomized experiment with few clusters ## (an experimental sample of 20 units) set.seed(12345) smalldat1 &lt;- dat1 %&gt;% dplyr::select(id,y1,y0,contains(&quot;cov&quot;)) %&gt;% sample_n(20) ## Now declare the different inputs for DeclareDesign ## (declare the smaller population, and assign treatment in each) popsmalldat1 &lt;- declare_population(smalldat1) assignsmalldat1 &lt;- declare_assignment(Znew=complete_ra(N,prob=0.5)) assignbigdat1 &lt;- declare_assignment(Znew=complete_ra(N,prob=0.5)) ## No additional treatment effects ## (potential outcomes) po_functionNull &lt;- function(data){ data$Y_Znew_0 &lt;- data$y0 data$Y_Znew_1 &lt;- data$y1 data } ## A few additional declare design settings ysdat1 &lt;- declare_potential_outcomes(handler = po_functionNull) theestimanddat1 &lt;- declare_inquiry(ATE = mean(Y_Znew_1 - Y_Znew_0)) theobsidentdat1 &lt;- declare_reveal(Y, Znew) ## The smaller sample design thedesignsmalldat1 &lt;- popsmalldat1 + assignsmalldat1 + ysdat1 + theestimanddat1 + theobsidentdat1 ## The larger sample design thedesignbigdat1 &lt;- popbigdat1 + assignbigdat1 + ysdat1 + theestimanddat1 + theobsidentdat1 ** Preserve the running dataset used so far. save main.dta, replace ** Keep a dataframe of select variables. keep id y1 y0 cov* ** A dataset to represent a smaller experiment, ** or a cluster randomized experiment with few clusters ** (an experimental sample of 20 units). ** Only done for illustration here. In practice, we&apos;ll use the data ** generated in the R code for the sake of comparison. set seed 12345 gen rand = runiform() sort rand keep in 1/20 drop rand ** Define a program to load one of these datasets ** and then randomly re-assign treatment. capture program drop sample_from program define sample_from, rclass syntax[ , smaller /// propsimtreat(real 0.5) ] * Which dataset to use? if &quot;`smaller&apos;&quot; != &quot;&quot; import delimited using &quot;smalldat1.csv&quot;, clear else import delimited using &quot;popbigdat1.csv&quot;, clear * Make sure propsimtreat is a proportion if `propsimtreat&apos; &gt; 1 | `propsimtreat&apos; &lt; 0 { di as error &quot;Check input: propsimtreat&quot; exit } * Re-assign (simulated) treatment in this draw of the data complete_ra znew, prob(`propsimtreat&apos;) * No additional treatment effects * (assigning new potential outcomes) gen y_znew_0 = y0 gen y_znew_1 = y1 * Save the true ATE gen true_effect = y_znew_1 - y_znew_0 qui sum true_effect, meanonly return scalar ATE = r(mean) * Get the revealed outcome gen ynew = (znew * y_znew_1) + ((1 - znew) * y_znew_0) end Next, in R, we&#x2019;ll prepare a list of estimators (i.e., models) we want to compare. These include different numbers of (potentially incorrect) covariates, with and without Lin (2013) adjustment. Similarly, in Stata, we&#x2019;ll write another program that generates a single dataset and then applies the same list of estimators to it, saving key results from each. R code Stata code Hide ## Declare a selection of different estimation strategies estCov0 &lt;- declare_estimator(Y~Znew, inquiry=theestimanddat1, .method=lm_robust, label=&quot;CovAdj0: Lm, No Covariates&quot;) estCov1 &lt;- declare_estimator(Y~Znew+cov2, inquiry=theestimanddat1, .method=lm_robust, label=&quot;CovAdj1: Lm, Correct Covariate&quot;) estCov2 &lt;- declare_estimator(Y~Znew+cov1+cov2+cov3+cov4+cov5+cov6+cov7+cov8, inquiry=theestimanddat1, .method=lm_robust, label=&quot;CovAdj2: Lm, Mixed Covariates&quot;) estCov3 &lt;- declare_estimator(Y~Znew+cov1+cov3+cov4+cov5+cov6, inquiry=theestimanddat1, .method=lm_robust, label=&quot;CovAdj3: Lm, Wrong Covariates&quot;) estCov4 &lt;- declare_estimator(Y~Znew,covariates=~cov1+cov2+cov3+cov4+cov5+cov6+cov7+cov8, inquiry=theestimanddat1, .method=lm_lin, label=&quot;CovAdj4: Lin, Mixed Covariates&quot;) estCov5 &lt;- declare_estimator(Y~Znew,covariates=~cov2, inquiry=theestimanddat1, .method=lm_lin, label=&quot;CovAdj5: Lin, Correct Covariate&quot;) ## List them all together all_estimators &lt;- estCov0 + estCov1 + estCov2 + estCov3 + estCov4 + estCov5 ** Define a program to apply various estimation strategies ** to a dataset drawn using the program defined above. capture program drop apply_estimators program define apply_estimators, rclass ** Same arguments as above syntax[, smaller /// propsimtreat(real 0.5) ] ** Call the program above sample_from, `smaller&apos; propsimtreat(`propsimtreat&apos;) return scalar ATE = r(ATE) ** CovAdj0: Lm, No Covariates qui reg ynew znew, vce(hc2) return scalar CovAdj0_est = _b[znew] return scalar CovAdj0_p = r(table)[&quot;pvalue&quot;, &quot;znew&quot;] ** CovAdj1: Lm, Correct Covariate qui reg ynew znew cov2, vce(hc2) return scalar CovAdj1_est = _b[znew] return scalar CovAdj1_p = r(table)[&quot;pvalue&quot;, &quot;znew&quot;] ** CovAdj2: Lm, Mixed Covariates qui reg ynew znew cov1-cov8, vce(hc2) return scalar CovAdj2_est = _b[znew] return scalar CovAdj2_p = r(table)[&quot;pvalue&quot;, &quot;znew&quot;] ** CovAdj3: Lm, Wrong Covariates qui reg ynew znew cov1 cov3-cov6, vce(hc2) return scalar CovAdj3_est = _b[znew] return scalar CovAdj3_p = r(table)[&quot;pvalue&quot;, &quot;znew&quot;] ** CovAdj4: Lin, Mixed Covariates qui reg ynew znew cov1-cov8 // to make a sample indicator gen samp = e(sample) // ensure correct obs are used in mean-centering foreach var of varlist cov1-cov8 { qui sum `var&apos; if samp == 1, meanonly qui gen mc_`var&apos; = `var&apos; - `r(mean)&apos; if samp == 1 } qui reg ynew i.znew##c.(mc_*), vce(hc2) return scalar CovAdj4_est = _b[1.znew] return scalar CovAdj4_p = r(table)[&quot;pvalue&quot;, &quot;1.znew&quot;] drop mc_* samp ** CovAdj5: Lin, Correct Covariate qui reg ynew znew cov2 gen samp = e(sample) foreach var of varlist cov2 { qui sum `var&apos; if samp == 1, meanonly qui gen mc_`var&apos; = `var&apos; - `r(mean)&apos; if samp == 1 } qui reg ynew i.znew##c.(mc_*), vce(hc2) return scalar CovAdj5_est = _b[1.znew] return scalar CovAdj5_p = r(table)[&quot;pvalue&quot;, &quot;1.znew&quot;] drop mc_* samp end After this, as a last step in R, we&#x2019;ll add those estimators to our design class objects. ## Smaller sample thedesignsmalldat1PlusEstimators &lt;- thedesignsmalldat1 + all_estimators ## Larger sample thedesignbigdat1PlusEstimators &lt;- thedesignbigdat1 + all_estimators Now, let&#x2019;s simulate each design 200 times and evaluate their performance. First, the smaller sample: R code Stata code Hide ## Summarize characteristics of the smaller-sample designs sims &lt;- 200 set.seed(12345) thediagnosisCovAdj1 &lt;- diagnose_design( thedesignsmalldat1PlusEstimators, sims = sims, bootstrap_sims = 0 ) ** Call the program once just to make a list of scalars to save apply_estimators, smaller local rscalars: r(scalars) // Save all scalars in r() to a local local to_store &quot;&quot; // Update them in a loop to work properly in simulate foreach item of local rscalars { local to_store &quot;`to_store&apos; `item&apos; = r(`item&apos;)&quot; } ** Summarize characteristics of the smaller-sample designs set seed 12345 simulate /// `to_store&apos;, /// reps(200): /// apply_estimators, smaller ** Create summary matrix qui des, short // saves number of columns to r() matrix diagnosands = J((r(k) - 1)/2, 6, .) matrix rownames diagnosands = &quot;Lm, No Covariates&quot; &quot;Lm, Correct Covariate&quot; &quot;Lm, Mixed Covariates&quot; &quot;Lm, Wrong Covariates&quot; &quot;Lin, Mixed Covariates&quot; &quot;Lin, Correct Covariate&quot; matrix colnames diagnosands = &quot;Mean estimand&quot; &quot;Mean estimate&quot; &quot;Bias&quot; &quot;SD Estimate&quot; &quot;RMSE&quot; &quot;Power&quot; ** Calculate quantities to include ** (https://declaredesign.org/r/declaredesign/reference/declare_diagnosands.html) local row = 0 forvalues i = 0/5 { local ++row * Estimand qui sum ATE, meanonly matrix diagnosands[`row&apos;, 1] = r(mean) * Estimate qui sum CovAdj`i&apos;_est, meanonly matrix diagnosands[`row&apos;, 2] = r(mean) * Bias qui gen biascalc = CovAdj`i&apos;_est - ATE qui sum biascalc, meanonly matrix diagnosands[`row&apos;, 3] = r(mean) drop biascalc * SD estimate (based on population variance, no n-1 in the variance denom.) qui sum CovAdj`i&apos;_est qui gen sdcalc = (CovAdj`i&apos;_est - r(mean))^2 qui sum sdcalc matrix diagnosands[`row&apos;, 4] = sqrt(r(sum)/r(N)) drop sdcalc * RMSE gen atediff = (CovAdj`i&apos;_est - ATE)^2 qui sum atediff, meanonly matrix diagnosands[`row&apos;, 5] = sqrt(r(mean)) drop atediff * Power gen rejectnull = CovAdj`i&apos;_p &lt;= 0.05 qui sum rejectnull, meanonly matrix diagnosands[`row&apos;, 6] = r(mean) drop rejectnull } * View the results matrix list diagnosands Second, the larger sample: R code Stata code Hide ## Summarize characteristics of the larger-sample designs set.seed(12345) thediagnosisCovAdj2 &lt;- diagnose_design( thedesignbigdat1PlusEstimators, sims = sims, bootstrap_sims = 0 ) ** Summarize characteristics of the large-sample designs apply_estimators local rscalars : r(scalars) local to_store &quot;&quot; foreach item of local rscalars { local to_store &quot;`to_store&apos; `item&apos; = r(`item&apos;)&quot; } simulate /// `to_store&apos;, /// reps(200): /// apply_estimators ** Create summary matrix qui des, short // get number of columns in r() matrix diagnosands = J((r(k) - 1)/2, 6, .) matrix rownames diagnosands = &quot;Lm, No Covariates&quot; &quot;Lm, Correct Covariate&quot; &quot;Lm, Mixed Covariates&quot; &quot;Lm, Wrong Covariates&quot; &quot;Lin, Mixed Covariates&quot; &quot;Lin, Correct Covariate&quot; matrix colnames diagnosands = &quot;Mean estimand&quot; &quot;Mean estimate&quot; &quot;Bias&quot; &quot;SD Estimate&quot; &quot;RMSE&quot; &quot;Power&quot; ** Calculate quantities to include local row = 0 forvalues i = 0/5 { local ++row * Estimand qui sum ATE, meanonly matrix diagnosands[`row&apos;, 1] = r(mean) * Estimate qui sum CovAdj`i&apos;_est, meanonly matrix diagnosands[`row&apos;, 2] = r(mean) * Bias qui gen biascalc = CovAdj`i&apos;_est - ATE qui sum biascalc, meanonly matrix diagnosands[`row&apos;, 3] = r(mean) drop biascalc * SD estimate (based on population variance, no n-1 in the variance denom.) qui sum CovAdj`i&apos;_est qui gen sdcalc = (CovAdj`i&apos;_est - r(mean))^2 qui sum sdcalc matrix diagnosands[`row&apos;, 4] = sqrt(r(sum)/r(N)) drop sdcalc * RMSE gen atediff = (CovAdj`i&apos;_est - ATE)^2 qui sum atediff, meanonly matrix diagnosands[`row&apos;, 5] = sqrt(r(mean)) drop atediff * Power gen rejectnull = CovAdj`i&apos;_p &lt;= 0.05 qui sum rejectnull, meanonly matrix diagnosands[`row&apos;, 6] = r(mean) drop rejectnull } * View the results matrix list diagnosands From the small sample simulation (N=20), we can see that &#x201C;CovAdj1: Lm, Correct Covariate&#x201D; shows more bias compared than the estimator using no covariates at all. The Lin approach using only the correct covariate reduces the bias in this case, but does not erase it. Though it is unbiased, the unadjusted estimator has fairly low power where as the Lin approach with the correct covariate &#x201C;CovAdj5: Lin, Correct Covariate&#x201D; has excellent power to detect the 1 SD effect built into this experiment. One interesting result here is that the Lin approach is worst (in terms of RMSE) when a mixture of correct and incorrect covariates are added to the linear model following the interaction-and-centering based approach. This illustrates a point mentioned above: as the number of parameters becomes large relative to the sample size (e.g., in small samples, or in moderate sized samples with a complex model), the efficiency costs from reducing the degrees of of freedom may not be worth the asymptotic efficiency advantages of Lin adjustment, particularly when we do not know if the covariates are actually prognostic. Estimator Term Mean Estimand Mean Estimate Bias SD Estimate RMSE Power CovAdj0: Lm, No Covariates Znew 5.03 4.93 -0.10 1.89 1.89 0.64 CovAdj1: Lm, Correct Covariate Znew 5.03 5.32 0.29 1.22 1.25 0.97 CovAdj2: Lm, Mixed Covariates Znew 5.03 5.12 0.09 1.36 1.36 0.94 CovAdj3: Lm, Wrong Covariates Znew 5.03 5.07 0.04 1.78 1.78 0.80 CovAdj4: Lin, Mixed Covariates Znew 5.03 5.10 0.07 2.47 2.46 0.42 CovAdj5: Lin, Correct Covariate Znew 5.03 5.17 0.14 1.13 1.13 0.97 The experiment with N=100N=100N=100 shows much smaller bias than the other experiment above. Since all estimators allow us to detect the 1 SD effect (Power=1), we can look to the RMSE column to weigh the overall performance of these estimators. Bias is lower in general, and the unadjusted approach is the least precise (SD Estimate), followed by linear covariance adjustment for the wrong covariates. In this example, the rest of the estimators all perform similarly (illustrating a point above: treatment assignment here is balanced, so the precision benefits of Lin adjustment should be lower). Estimator Term Mean Estimand Mean Estimate Bias SD Estimate RMSE Power CovAdj0: Lm, No Covariates Znew 5.45 5.39 -0.06 0.73 0.73 1.00 CovAdj1: Lm, Correct Covariate Znew 5.45 5.44 -0.02 0.46 0.46 1.00 CovAdj2: Lm, Mixed Covariates Znew 5.45 5.46 0.01 0.47 0.46 1.00 CovAdj3: Lm, Wrong Covariates Znew 5.45 5.43 -0.02 0.60 0.60 1.00 CovAdj4: Lin, Mixed Covariates Znew 5.45 5.45 -0.01 0.47 0.47 1.00 CovAdj5: Lin, Correct Covariate Znew 5.45 5.42 -0.03 0.46 0.46 1.00 The Lin approach works well when covariates are few and sample sizes are larger, but these simulations show where the approach is weaker: when there are many covariates relative to the number of observations, some of which may not actually be correlated with the outcome. In this case, the estimator involving both correct and irrelevant covariates included 18 terms. Fitting a model with 18 terms using a small sample, e.g.&#xA0;N=20N=20N=20, allows nearly any observation to exert undue influence, increases the risk of serious multicollinearity, and leads to overfitting problems in general. Our team does not often run into such an extreme version of this problem because our studies have tended to involve many thousands of units and relatively few covariates. However, when we do encounter situations where the number of covariates (or fixed effects) we&#x2019;d like to account for is large relative to the sample size, we consider a few alternative approaches: (1) omitting some covariates; (2) standard linear covariate adjustment; (3) collapsing the covariates into fewer dimensions (e.g., using principal component analysis); or (4) working with a residualized version of the outcome, as described below. 5.3.3 Another way to think about Lin adjustment What we call Lin adjustment, after Lin (2013), produces equivalent point estimates to a different procedure that has been called things like the &#x201C;oaxaca-blinder estimator&#x201D; (Guo and Basse 2023), &#x201C;full regression adjustment&#x201D; (Negi and Wooldridge 2021), or &#x201C;G-computation&#x201D; (Snowden, Rose, and Mortimer 2011; Greifer 2023). In a simple two-arm randomized trial, it proceeds as follows: Create separate subsets of treated and control units Regress the outcome on controls separately within each subset Using the control subset model, calculated a predicted control potential outcome for treated units Using the treated subset model, calculated a predicted treatment potential outcome for control units Calculate the unit-level difference between the real and predicted potential outcomes Take the mean of these unit-level differences We demonstrate in the code chunk below that this yields equivalent point estimates to Lin estimation in a linear regression case (maybe with some negligible finite sample estimation error). R code Stata code Hide ## Perform Lin adjustment mod_lin &lt;- lm(Y ~ Z * I(cov2 - mean(cov2)), data = dat1) ## Perform G-computation mod_c &lt;- lm(Y ~ cov2, data = dat1[dat1$Z == 0,]) mod_t &lt;- lm(Y ~ cov2, data = dat1[dat1$Z == 1,]) dat1$g_y0[dat1$Z == 1] &lt;- predict(mod_c, newdata = dat1[dat1$Z == 1,]) dat1$g_y1[dat1$Z == 0] &lt;- predict(mod_t, newdata = dat1[dat1$Z == 0,]) dat1$g_y0[dat1$Z == 0] &lt;- dat1$Y[dat1$Z == 0] dat1$g_y1[dat1$Z == 1] &lt;- dat1$Y[dat1$Z == 1] ## Compare print(&quot;Lin estimation&quot;) as.numeric(mod_lin$coefficients[2]) print(&quot;G-computation&quot;) mean(dat1$g_y1 - dat1$g_y0) ** Perform Lin adjustment. ** Nested in preserve/restore to ** avoid interrupting the flow from ** our simulation in the last section ** to the one in our next simulation. preserve ** Perform Lin adjustment use main.dta, clear qui sum cov2, meanonly gen gmc_cov2 = cov2 - r(mean) qui reg y z gmc_cov2 c.z#c.gmc_cov2 global gcomp_lin = _b[z] ** Perform G-computation qui reg y cov2 if z == 0 predict g_y0 if z == 1, xb replace g_y0 = y if z == 0 qui reg y cov2 if z == 1 predict g_y1 if z == 0, xb replace g_y1 = y if z == 1 gen g_diff = g_y1 - g_y0 qui sum g_diff, meanonly global gcomp_gcomp = r(mean) ** Compare di &quot;Lin estimation&quot; di $gcomp_lin di &quot;G-computation&quot; di $gcomp_gcomp restore [1] &quot;Lin estimation&quot; [1] 5.284 [1] &quot;G-computation&quot; [1] 5.284 One advantage of G-computation is that it can be easily generalized to non-linear models like Logit regression (Negi and Wooldridge 2021; Guo and Basse 2023), whereas Lin estimation is specifically for OLS. But a downside of G-computation is that standard error calculation must still be performed afterwards, either using derived estimators or a procedure like a bootstrap. We prefer Lin estimation for our evaluations in practice: we typically use linear regression to analyze experimental data, we know that HC2 standard errors from a Lin-adjusted OLS model are already appropriately conservative (Lin 2013), and we like being able to get everything we need from a single model. Still, Lin adjustment&#x2019;s similarity to G-computation helps provide more intuition for its (potential) efficiency advantages. A model that does a better job of explaining unrelated variation in YYY will provide more precise treatment effect estimates. If covariates have a different relationship with the outcome in treated and control groups (e.g., treatment effect heterogeneity), Lin adjustment will obviously be better at explaining variation in YYY than standard linear covariate adjustment. On the other hand, when covariates do have the same outcome relationship in treatment and control groups, both covariance adjustment strategies model variation in YYY just as well, and the primary efficiency issue is whether we are concerned about the extra degrees of freedom Lin adjustment costs. Note that in situations where we have accurate estimates of a unit&#x2019;s propensity score or probability of receiving treatment (e.g., through repeated simulations of treatment assignment as in randomization inference), we can potentially reduce bias/inefficiency further by performing Lin adjustment in a weighted least squares regression.24 The weights are as follows, with TiT_{i}Ti&#x200B; representing a binary treatment and pip_{i}pi&#x200B; the probability that a unit was treated: Tipi+1&#x2212;Ti1&#x2212;pi\\frac{T_{i}}{p_{i}} + \\frac{1-T_{i}}{1-p_{i}}pi&#x200B;Ti&#x200B;&#x200B;+1&#x2212;pi&#x200B;1&#x2212;Ti&#x200B;&#x200B;. Researchers sometimes &#x201C;stabilize&#x201D; those weights (reducing the influence of a few observations with especially large weights) by either taking the square root (Imbens 2004) or by multiplying an observation&#x2019;s weight by the proportion of the sample with its observed treatment exposure (Robins, Hernan, and Brumback 2000). This estimation procedure is called inverse propensity score weighted regression adjustment (IPWRA). It is straightforward to implement manually, and software like teffects in Stata or WeightIt in R can perform IPWRA as well. 5.3.4 The Rosenbaum Approach to Covariance Adjustment Rosenbaum (2002) suggests an alternative approach in which the outcomes are regressed on covariates, ignoring treatment assignment, and then the residuals from that first regression are used as the outcome in a second regression to estimate an average treatment effect. Again, we evaluate this approach through simulation. First, code we use to generate Rosenbaum (2002) estimators: R code Stata code Hide ## The argument covs is a character vector of names of covariates. ## The function below generates a separate estimation command ## that performs Rosenbaum (2002) adjustment for the specified ## covariates, given a sample of data. This new function can ## then by used as an estimator in simulations below. This code ## takes advantage of the concept of &quot;function factories.&quot; ## See, e.g.: https://adv-r.hadley.nz/function-factories.html. make_est_fun&lt;-function(covs){ force(covs) ## Generate a formula from the character vector of covariates covfmla &lt;- reformulate(covs,response=&quot;Y&quot;) ## What the new function to-be-generated will do, ## given the model generated above: function(data){ data$e_y &lt;- residuals(lm(covfmla,data=data)) obj &lt;- lm_robust(e_y~Znew,data=data) res &lt;- tidy(obj) %&gt;% filter(term==&quot;Znew&quot;) return(res) } } ## Make Rosenbaum (2002) estimators for different groups of covariates est_fun_correct &lt;- make_est_fun(&quot;cov2&quot;) est_fun_mixed&lt;- make_est_fun(c(&quot;cov1&quot;,&quot;cov2&quot;,&quot;cov3&quot;,&quot;cov4&quot;,&quot;cov5&quot;,&quot;cov6&quot;,&quot;cov7&quot;,&quot;cov8&quot;)) est_fun_incorrect &lt;- make_est_fun(c(&quot;cov1&quot;,&quot;cov3&quot;,&quot;cov4&quot;,&quot;cov5&quot;,&quot;cov6&quot;)) ## Declare additional estimators, to add to the designs above estCov6 &lt;- declare_estimator(handler = label_estimator(est_fun_correct), inquiry=theestimanddat1, label=&quot;CovAdj6: Resid, Correct&quot;) estCov7 &lt;- declare_estimator(handler = label_estimator(est_fun_mixed), inquiry=theestimanddat1, label=&quot;CovAdj7: Resid, Mixed&quot;) estCov8 &lt;- declare_estimator(handler = label_estimator(est_fun_incorrect), inquiry=theestimanddat1, label=&quot;CovAdj8: Resid, Incorrect&quot;) ## Add the additional estimators thedesignsmalldat1PlusRoseEstimators &lt;- thedesignsmalldat1 + all_estimators + estCov6 + estCov7 + estCov8 thedesignbigdat1PlusRoseEstimators &lt;- thedesignbigdat1 + all_estimators + estCov6 + estCov7 + estCov8 ** Define a program to perform Rosenbaum estimation * (with all variables specified in a varlist as in regress). capture program drop rosenbaum_est program define rosenbaum_est, rclass * Assumed structure of varlist: outcome treatment list_of_covariates. syntax varlist * Alternative 1: syntax, outcome(varname) treatment(varname) covar(varlist) * Alternative 2: syntax varlist, outcome(varname) treatment(varname) * Pull out first var of varlist as outcome tokenize `varlist&apos; local outcome `1&apos; macro shift * Pull out second var of varlist as treatment local treat_cov `*&apos; tokenize `treat_cov&apos; local treatment `1&apos; macro shift * The rest are treated as covariates local covar `*&apos; * Estimation reg `outcome&apos; `covar&apos; tempvar yhat predict `yhat&apos;, xb tempvar resid gen `resid&apos; = `outcome&apos; - `yhat&apos; reg `resid&apos; `treatment&apos;, vce(hc2) * Output return scalar out_est = _b[`treatment&apos;] return scalar out_p = r(table)[&quot;pvalue&quot;, &quot;`treatment&apos;&quot;] end ** Program to apply this estimator to the generated data: capture program drop apply_estimators_2 program define apply_estimators_2, rclass syntax[, smaller /// propsimtreat(real 0.5) ] ** Call the sample generation / random assignment program sample_from, `smaller&apos; propsimtreat(`propsimtreat&apos;) return scalar ATE = r(ATE) ** CovAdj6: Resid, Correct qui rosenbaum_est ynew znew cov2 return scalar CovAdj6_est = r(out_est) return scalar CovAdj6_p = r(out_p) ** CovAdj7: Resid, Mixed qui rosenbaum_est ynew znew cov1-cov8 return scalar CovAdj7_est = r(out_est) return scalar CovAdj7_p = r(out_p) ** CovAdj8: Resid, Incorrect qui rosenbaum_est ynew znew cov1 cov3-cov6 return scalar CovAdj8_est = r(out_est) return scalar CovAdj8_p = r(out_p) end Second, we evaluate a design that applies those estimators to our smaller sample: R code Stata code Hide set.seed(12345) thediagnosisCovAdj3 &lt;- diagnose_design( thedesignsmalldat1PlusRoseEstimators, sims = sims, bootstrap_sims = 0 ) ** Summarize characteristics of the smaller-sample designs apply_estimators_2, smaller // Looking only at the rosenbaum estimators now local rscalars : r(scalars) local to_store &quot;&quot; foreach item of local rscalars { local to_store &quot;`to_store&apos; `item&apos; = r(`item&apos;)&quot; } simulate /// `to_store&apos;, /// reps(200): /// apply_estimators_2, smaller ** Create summary matrix qui des, short matrix diagnosands = J((`r(k)&apos; - 1)/2, 6, .) matrix rownames diagnosands = &quot;Resid, Correct&quot; &quot;Resid, Mixed&quot; &quot;Resid, Incorrect&quot; matrix colnames diagnosands = &quot;Mean estimand&quot; &quot;Mean estimate&quot; &quot;Bias&quot; &quot;SD Estimate&quot; &quot;RMSE&quot; &quot;Power&quot; ** Calculate quantities to include local row = 0 forvalues i = 6/8 { local ++row * Estimand qui sum ATE, meanonly matrix diagnosands[`row&apos;, 1] = r(mean) * Estimate qui sum CovAdj`i&apos;_est, meanonly matrix diagnosands[`row&apos;, 2] = r(mean) * Bias qui gen biascalc = CovAdj`i&apos;_est - ATE qui sum biascalc, meanonly matrix diagnosands[`row&apos;, 3] = r(mean) drop biascalc * SD estimate (based on population variance, no n-1 in the variance denom.) qui sum CovAdj`i&apos;_est qui gen sdcalc = (CovAdj`i&apos;_est - r(mean))^2 qui sum sdcalc matrix diagnosands[`row&apos;, 4] = sqrt(r(sum)/r(N)) drop sdcalc * RMSE gen atediff = (CovAdj`i&apos;_est - ATE)^2 qui sum atediff, meanonly matrix diagnosands[`row&apos;, 5] = sqrt(r(mean)) drop atediff * Power gen rejectnull = CovAdj`i&apos;_p &lt;= 0.05 qui sum rejectnull, meanonly matrix diagnosands[`row&apos;, 6] = r(mean) drop rejectnull } * View the results matrix list diagnosands Finally, we evaluate a design that applies those estimators (alongside the others above) to our larger sample: R code Stata code Hide set.seed(12345) thediagnosisCovAdj4 &lt;- diagnose_design( thedesignbigdat1PlusRoseEstimators, sims = sims, bootstrap_sims = 0 ) ** Summarize characteristics of the larger-sample designs apply_estimators_2 // Looking only at the rosenbaum estimators now local rscalars : r(scalars) local to_store &quot;&quot; foreach item of local rscalars { local to_store &quot;`to_store&apos; `item&apos; = r(`item&apos;)&quot; } di &quot;`to_store&apos;&quot; simulate /// `to_store&apos;, /// reps(200): /// apply_estimators_2 ** Create summary matrix qui des, short matrix diagnosands = J((`r(k)&apos; - 1)/2, 6, .) matrix rownames diagnosands = &quot;Resid, Correct&quot; &quot;Resid, Mixed&quot; &quot;Resid, Incorrect&quot; matrix colnames diagnosands = &quot;Mean estimand&quot; &quot;Mean estimate&quot; &quot;Bias&quot; &quot;SD Estimate&quot; &quot;RMSE&quot; &quot;Power&quot; ** Calculate quantities to include local row = 0 forvalues i = 6/8 { local ++row * Estimand qui sum ATE, meanonly matrix diagnosands[`row&apos;, 1] = r(mean) * Estimate qui sum CovAdj`i&apos;_est, meanonly matrix diagnosands[`row&apos;, 2] = r(mean) * Bias qui gen biascalc = CovAdj`i&apos;_est - ATE qui sum biascalc, meanonly matrix diagnosands[`row&apos;, 3] = r(mean) drop biascalc * SD estimate (based on population variance, no n-1 in the variance denom.) qui sum CovAdj`i&apos;_est qui gen sdcalc = (CovAdj`i&apos;_est - r(mean))^2 qui sum sdcalc matrix diagnosands[`row&apos;, 4] = sqrt(r(sum)/r(N)) drop sdcalc * RMSE gen atediff = (CovAdj`i&apos;_est - ATE)^2 qui sum atediff, meanonly matrix diagnosands[`row&apos;, 5] = sqrt(r(mean)) drop atediff * Power gen rejectnull = CovAdj`i&apos;_p &lt;= 0.05 qui sum rejectnull, meanonly matrix diagnosands[`row&apos;, 6] = r(mean) drop rejectnull } * View the results matrix list diagnosands * Return to main data from before the covariance adjustment simulations use main.dta, clear erase main.dta With a small sample (N=20), the Rosenbaum-style approach yields very little bias and quite high power using the correct covariate (&#x201C;CovAdj6: Resid, Correct&#x201D;), but performs poorly in terms of bias and precision with incorrect covariates. Estimator Term Mean Estimand Mean Estimate Bias SD Estimate RMSE Power 7 CovAdj6: Resid, Correct Znew 5.03 5.04 0.01 1.15 1.14 0.99 8 CovAdj7: Resid, Mixed Znew 5.03 3.01 -2.01 1.06 2.28 0.84 9 CovAdj8: Resid, Incorrect Znew 5.03 3.77 -1.25 1.38 1.86 0.76 With a larger experiment, the bias goes down, but coverage is still relatively worse when incorrect covariates are included. We speculate that performance might improve if we fit covariance adjustment models that produce residuals separately for the treated and control groups. Estimator Term Mean Estimand Mean Estimate Bias SD Estimate RMSE Power 7 CovAdj6: Resid, Correct Znew 5.45 5.38 -0.07 0.47 0.47 1.00 8 CovAdj7: Resid, Mixed Znew 5.45 5.03 -0.42 0.48 0.64 1.00 9 CovAdj8: Resid, Incorrect Znew 5.45 5.16 -0.30 0.60 0.67 1.00 5.4 How to choose covariates for covariance adjustment? Our analysis plans commonly specify a few covariates based on what we know about the mechanisms and context of the study. In general, if we have a measurement of the outcome before the treatment was assigned &#x2014; the baseline outcome &#x2014; we try to use it as a part of a blocking or covariance adjustment strategy. When we have access to many covariates, we may sometimes use simple machine learning methods to select variables that strongly predict the outcome, such as a lasso model. In particular, we tend to prefer the adaptive lasso rather than a simple lasso because it has better theoretical properties (Zou 2006), but also because the adaptive lasso tends to produce sparser results &#x2014; and the bias from covariance adjustment can sometimes be significance if we include too many non-prognostic covariates. 5.5 Block-randomized trials We design block-randomized trials by splitting units into groups based on predefined characteristics &#x2014; covariates that cannot be changed by the experimental treatment &#x2014; and then randomly assigning treatment within each group. We use this procedure when we want to increase our ability to detect signal from noise. It assumes that the noise, or random variation in the outcome measure, is associated with the variables that we use to assign blocks. For example, if we imagine that patterns of energy use will tend to differ according to size of family, we may create blocks or strata of different family sizes and randomly assign an energy saving intervention separately within those blocks. We also design block-randomized experiments when we want to assess effects within and across subgroups (for example, if we want to ensure that we have enough statistical power to detect a difference in effects between veterans and non-veterans). If we have complete random assignment, it is likely that the proportion of veterans assigned treatment will not be exactly same as the proportion of non-veterans receiving treatment. However, if we stratify or block the group on military status, and randomly assign treatment and control within each group, we can then ensure that equal proportions (or numbers) or veterans and non-veterans receive the treatment and control. Most of the general ideas that we demonstrated in the context of completely randomized trials have direct analogues in the case of block randomized trials. The only additional question that arises with block randomized trials is about how to weight the contributions of each individual block when calculating an overall average treatment effect or testing an overall hypothesis about treatment effects. We begin with the simple case of testing the sharp null of no effects when we have a binary outcome &#x2014; in the case of a Cochran-Mantel-Haenszel test the weighting of different blocks is handled automatically. 5.5.1 Testing binary outcomes under block randomization: Cochran-Mantel-Haenszel (CMH) test for K X 2 X 2 tables For block-randomized trials with a binary outcome, we might use the CMH test to evaluate the null of no effect.25 This is one way of keeping the outcomes for each strata separate rather than pooling them together &#x2014; under blocking, we are effectively repeating the same experiment separately within each stratum. The CMH test tells us if the odds ratios across strata support an association between the outcome and treatment (Cochran 1954; Mantel and Haenszel 1959). To set up the CMH test, we need k sets of 2x2 contingency tables. Suppose the table below represents outcomes from stratum i where A, B, C, and D are counts of observations: Assignment Response No response Total Treatment A B A+B Control C D C+D Total A+C B+D A+B+C+D = T The CMH test statistic takes the sum of the deviations between observed and expected outcomes within each stratum (OiO_{i}Oi&#x200B; and EiE_{i}Ei&#x200B;, respectively), squares this sum, and compares it to the sum of the variance within each strata: CMH=[&#x2211;i=1k(Oi&#x2212;Ei)]2&#x2211;i=1kVar[Oi]CMH = \\frac{\\big[\\sum_{i=1}^{k} (O_{i} - E_{i})\\big]^{2}}{\\sum_{i=1}^{k}{\\mathrm{\\mathrm{Var}}[O_{i}]}}CMH=&#x2211;i=1k&#x200B;Var[Oi&#x200B;][&#x2211;i=1k&#x200B;(Oi&#x200B;&#x2212;Ei&#x200B;)]2&#x200B; where Oi=(Ai+Bi)(Ai+Ci)TiO_{i} = \\frac{(A_i+B_i)(A_i+C_i)}{T_i}Oi&#x200B;=Ti&#x200B;(Ai&#x200B;+Bi&#x200B;)(Ai&#x200B;+Ci&#x200B;)&#x200B; and Var[Oi]=(Ai+Bi)(Ai+Ci)(Bi+Di)(Ci+Di)Ti2(Ti&#x2212;1)\\mathrm{Var}[O_{i}] = \\frac{(A_i+B_i)(A_i+C_i)(B_i+D_i)(C_i+D_i)}{{T_i}^2(T_i-1)}Var[Oi&#x200B;]=Ti&#x200B;2(Ti&#x200B;&#x2212;1)(Ai&#x200B;+Bi&#x200B;)(Ai&#x200B;+Ci&#x200B;)(Bi&#x200B;+Di&#x200B;)(Ci&#x200B;+Di&#x200B;)&#x200B; In large-enough samples, if there are no associations between treatment and outcomes across strata, we would expect to see an odds ratio equal to 1. Across randomizations of large-enough samples, this test statistic follows an asymptotic &#x3C7;2\\chi^2&#x3C7;2 distribution with degrees of freedom = 1. For a standard two-arm trial, the odds ratio for a given stratum would be: OR=A/BC/D=ADBCOR = \\frac{A/B}{C/D} = \\frac{AD}{BC}OR=C/DA/B&#x200B;=BCAD&#x200B; Across strata, could then find an overall common odds ratio as follows: ORCMH=&#x2211;i=1k(AiDi/Ti)&#x2211;i=1kBiCiTi\\begin{equation} OR_{CMH} = \\frac{\\sum_{i=1}^{k} (A_{i}D_{i}/T_{i})}{\\sum_{i=1}^{k}{B_{i}C_{i}}{T_{i}}} \\end{equation}ORCMH&#x200B;=&#x2211;i=1k&#x200B;Bi&#x200B;Ci&#x200B;Ti&#x200B;&#x2211;i=1k&#x200B;(Ai&#x200B;Di&#x200B;/Ti&#x200B;)&#x200B;&#x200B;&#x200B; If this common odds ratio is substantially greater than 1, then we should suspect that there is an association between the outcome and treatment across strata, and the CMH test statistic would be large. If ORCMH=1OR_{CMH} = 1ORCMH&#x200B;=1, this would instead support the null hypothesis that there is no association between treatment and the outcome, and the CMH test statistic would be small. We could also use the CMH test to compare odds ratios between experiments, rather than comparing against the null that the common odds ratio = 1. 5.5.2 Estimating an overall average treatment effect Regardless of how treatment is assigned, our team nearly always reports a single estimate of the average treatment effect. To review, we define the overall ATE (the theoretical estimand) as a simple average of the individual treatment effects. For two treatments, we might write this individual-level causal effect as &#x3C4;i=yi,0&#x2212;yi,1\\tau_i = y_{i,0} - y_{i,1}&#x3C4;i&#x200B;=yi,0&#x200B;&#x2212;yi,1&#x200B;. We&#x2019;d therefore express the overall (true) ATE across our sample as as &#x3C4;&#x2C9;=(1/n)&#x2211;i=1n&#x3C4;i\\bar{\\tau}=(1/n) \\sum_{i=1}^n \\tau_i&#x3C4;&#x2C9;=(1/n)&#x2211;i=1n&#x200B;&#x3C4;i&#x200B;. Unfortunately, this quantity is unobservable. In blocked designs, we have randomly assigned the intervention within each block independently. As we note above, this (1) increases precision and (2) supports more credible subgroup analysis. How might we &#x201C;analyze as we have randomized&#x201D; to learn about &#x3C4;&#x2C9;\\bar{\\tau}&#x3C4;&#x2C9; using observable data? Our approach is to think about this in terms of block-level treatment effects. See Gerber and Green (2012) for more context. Say, for example, that the unobserved ATE within a given block, bbb, was ATEb=&#x3C4;&#x2C9;b=(1/nb)&#x2211;i=1nb&#x3C4;i\\text{ATE}_{b}=\\bar{\\tau}_b=(1/n_b)\\sum_{i=1}^{n_b} \\tau_{i}ATEb&#x200B;=&#x3C4;&#x2C9;b&#x200B;=(1/nb&#x200B;)&#x2211;i=1nb&#x200B;&#x200B;&#x3C4;i&#x200B;. Here, we are averaging the individual level treatment effects (&#x3C4;i\\tau_{i}&#x3C4;i&#x200B;) across all nbn_bnb&#x200B; people in block bbb. Now, imagine that we had an experiment with blocks of different sizes (and perhaps with different proportions of units assigned to treatment &#x2014; e.g., certain blocks may be more expensive places in which to run an experiment). We can learn about &#x3C4;&#x2C9;\\bar{\\tau}&#x3C4;&#x2C9; by first reframing it as the block-size weighted average of the true within-block treatment effects:26 &#x3C4;&#x2C9;=&#x3C4;&#x2C9;nbwt=(1/B)&#x2211;b=1B(nb/n)&#x3C4;&#x2C9;b\\bar{\\tau} = \\bar{\\tau}_{\\text{nbwt}} = (1/B) \\sum_{b=1}^B (n_b/n) \\bar{\\tau}_b&#x3C4;&#x2C9;=&#x3C4;&#x2C9;nbwt&#x200B;=(1/B)b=1&#x2211;B&#x200B;(nb&#x200B;/n)&#x3C4;&#x2C9;b&#x200B; We can then estimate &#x3C4;&#x2C9;nbwt\\bar{\\tau}_{\\text{nbwt}}&#x3C4;&#x2C9;nbwt&#x200B; based on its observable analogue, just as we have with a completely randomized experiment. Blocks are essentially their own randomized experiments, so we can estimate each &#x3C4;&#x2C9;b\\bar{\\tau}_b&#x3C4;&#x2C9;b&#x200B; using the following unbiased estimator, where i&#x2208;ti \\in ti&#x2208;t means &#x201C;for iii in the treatment group&#x201D; and where mbm_bmb&#x200B; is the number of units assigned to treatment in block bbb: &#x3C4;^b=&#x2211;i&#x2208;tYib/mb&#x2212;&#x2211;i&#x2208;cYib/(nb&#x2212;mb)\\hat{\\tau}_b=\\sum_{i \\in t} Y_{ib}/m_b - \\sum_{i \\in c} Y_{ib}/(n_b - m_b)&#x3C4;^b&#x200B;=i&#x2208;t&#x2211;&#x200B;Yib&#x200B;/mb&#x200B;&#x2212;i&#x2208;c&#x2211;&#x200B;Yib&#x200B;/(nb&#x200B;&#x2212;mb&#x200B;) We would then plug the &#x3C4;^b\\hat{\\tau}_b&#x3C4;^b&#x200B; estimates into the expression for &#x3C4;&#x2C9;nbwt\\bar{\\tau}_{\\text{nbwt}}&#x3C4;&#x2C9;nbwt&#x200B; above, yielding &#x3C4;&#x2C9;^nbwt\\hat{\\bar{\\tau}}_{\\text{nbwt}}&#x3C4;&#x2C9;^nbwt&#x200B;. Note that many people do not use this unbiased estimator, in part because its precision is worse that those of another, slightly biased estimator. We illustrate both methods below, the block-size weighted estimator and what we call the &#x201C;precision-weighted&#x201D; estimator. We also offer some reflections on when a biased estimator that tends to produce answers closer to the truth might be preferred over an unbiased estimator where any given estimate may be farther from the truth. The precision-weighted estimator is based on harmonic-weights. We have also tended to call it a &#x201C;precision-weighted average.&#x201D; The weights combine block size, nbn_bnb&#x200B;, and the proportion of the block assigned to treatment, pb=(1/nb)&#x2211;i=1nbZibp_b = (1/n_b) \\sum_{i=1}^{n_b} Z_{ib}pb&#x200B;=(1/nb&#x200B;)&#x2211;i=1nb&#x200B;&#x200B;Zib&#x200B;, for a binary treatment, ZibZ_{ib}Zib&#x200B;. The harmonic weight for each block is hb=nbpb(1&#x2212;pb)h_b = n_b p_b (1 - p_b)hb&#x200B;=nb&#x200B;pb&#x200B;(1&#x2212;pb&#x200B;), and the theoretical estimand is then: &#x3C4;&#x2C9;hbwt=(1/B)&#x2211;b=1B(1/hb)&#x3C4;&#x2C9;b\\bar{\\tau}_{\\text{hbwt}}= (1/B) \\sum_{b=1}^B (1/h_b) \\bar{\\tau}_b&#x3C4;&#x2C9;hbwt&#x200B;=(1/B)b=1&#x2211;B&#x200B;(1/hb&#x200B;)&#x3C4;&#x2C9;b&#x200B; This approach may sound unfamiliar. But using precision weights to average a set of within-block treatment effect estimates is equivalent to the more common practice of regression adjustment for linear, additive blocked fixed effects (and we show an example of this in our code below). Two points are worth emphasizing. First, by weighting blocks proportionally to their contributions to overall sample size, &#x3C4;&#x2C9;nbwt\\bar{\\tau}_{\\text{nbwt}}&#x3C4;&#x2C9;nbwt&#x200B; effectively treats all units equally in terms of their contributions to the ATE. In contrast, &#x3C4;&#x2C9;hbwt\\bar{\\tau}_{\\text{hbwt}}&#x3C4;&#x2C9;hbwt&#x200B; weights treatment effects from blocks (and therefore the individuals within them) relatively less if the within-block treatment probability is closer to 0.5.27 Second, the expression for hbh_bhb&#x200B; above shows that the precision-weighted approach will give no weight to units in blocks where the probability of treatment is 0 or 1 (i.e., all units are eiter treated or untreated, due perhaps to treatment administration issues). What may not be obvious yet is that the unbiased, block-size weighted approach suffers from the same problem. Treatment probabilities may not factor into nb/nn_b/nnb&#x200B;/n, but these are block-level weights that we can&#x2019;t apply without a within-block treatment effect estimate (which we cannot calculate in the absence of within-block variation in treatment). The unit level expression for these unbiased weights is a function of the within-block treated proportion (pib)(p_{ib})(pib&#x200B;): nbwti=Zi/pib+(1&#x2212;Zi)/(1&#x2212;pib)nbwt_{i} = Z_{i}/p_{ib} + (1-Z_{i})/(1 - p_{ib})nbwti&#x200B;=Zi&#x200B;/pib&#x200B;+(1&#x2212;Zi&#x200B;)/(1&#x2212;pib&#x200B;) For reference, an expression for unit-level precision-weights is: hbwti=nbwti&#xD7;pib&#xD7;(1&#x2212;pib)hbwt_{i} = nbwt_{i} \\times p_{ib} \\times (1 - p_{ib})hbwti&#x200B;=nbwti&#x200B;&#xD7;pib&#x200B;&#xD7;(1&#x2212;pib&#x200B;) Let&#x2019;s review various approaches to applying these estimators. We&#x2019;ll demonstrate (1) that ignoring the blocks when estimating treatment effects can produce problems in both estimation and testing, and (2) that the block-size weighted approaches are unbiased but possibly less precise than the precision weighted approaches. R code Stata code Hide ## Create block sizes and create block weights B &lt;- 10 # Number of blocks dat &lt;- data.frame(b=rep(1:B,c(8,20,30,40,50,60,70,80,100,800))) dat$bF &lt;- factor(dat$b) ## x1 is a covariate that strongly predicts the outcome without treatment set.seed(2201) dat &lt;- group_by(dat,b) %&gt;% mutate( nb=n(), x1=rpois(n = nb,lambda=runif(1,min=1,max=2000)) ) ## The treatment effect varies by block size (sqrt(nb) because nb has such a large range.) dat &lt;- group_by(dat,b) %&gt;% mutate( y0=sd(x1)*x1+rchisq(n=nb,df=1), y0=y0*(y0&gt;quantile(y0,.05)), tauib = -(sd(y0))*sqrt(nb) + rnorm(n(),mean=0,sd=sd(y0)), y1=y0+tauib, y1=y1*(y1&gt;0) ) blockpredpower &lt;- summary(lm(y0~bF,data=dat))$r.squared ** Data simulation provided for illustration. ** In practice, will use data generated by the R code, ** for the sake of comparison. ** Create block sizes and create block weights clear local sizes 8 20 30 40 50 60 70 80 100 800 // Block sample sizes set obs 10 // Number of blocks qui gen bf = . // Categorical block var qui gen nb = . // Number in block local i = 0 foreach size of local sizes { local ++i replace nb = `size&apos; if _n == `i&apos; replace bf = `i&apos; if _n == `i&apos; } gen lambda = runiform(1, 2000) // For poisson generation below expand nb sort bf ** x1 is a covariate that strongly predicts the outcome without treatment set seed 2201 gen x1 = rpoisson(lambda) ** The treatment effect varies by block size (sqrt(nb) because nb has such a large range.) bysort bf: egen sd_x1 = sd(x1) gen y0 = (sd_x1 * x1) + rchi2(1) bysort bf: egen p5 = pctile(y0), p(5) replace y0 = 0 if y0 &lt;= p5 bysort bf: egen sd_y0 = sd(y0) gen tauib = -(sd_y0)*sqrt(nb) + rnormal(0, sd_y0) gen y1 = y0 + tauib replace y1 = 0 if y1 &lt;= 0 qui reg y0 i.bf di round(e(r2), 0.0001) drop sd_y0 sd_x1 p5 To make the differences between these estimation approaches more vivid, we&#x2019;ve create a dataset with blocks of widely varying sizes. Half of the blocks have half of the units assigned to treatment and the other half have 10% of the units assigned to treatment (i.e., varying assignment proabilities across blocks). Notice also that the baseline outcomes are strongly predicted by the blocks (R2R^2R2 around 0.870.870.87). We&#x2019;ll use DeclareDesign (in R) to assess bias, coverage and power (or precision) of several different approaches to applying each estimator. We&#x2019;ll also provide a parallel Stata example. The next code block sets up the simulation and also demonstrates different approaches to estimating an ATE (as usual, we can only do this because we are using simulation to compare these different statistical techniques in a setting where we know the true data generating process). R code Stata code Hide ## Using the Declare Design Machinery to ensure that ## the data here and the simulations below match ## Declare the population thepop &lt;- declare_population(dat) ## Represent the potential outcomes with a function po_function &lt;- function(data){ data$Y_Z_0 &lt;- data$y0 data$Y_Z_1 &lt;- data$y1 data } ## Use this function to declare the potential outcomes theys &lt;- declare_potential_outcomes(handler = po_function) ## Declare the desired inquiry theestimand &lt;- declare_inquiry(ATE = mean(Y_Z_1 - Y_Z_0)) ## Declare treatment assignment numtreated &lt;- sort(unique(dat$nb))/rep(c(2,10),B/2) theassign &lt;- declare_assignment(Z = block_ra(blocks = bF, block_m = numtreated)) ## Declare revealed data theobsident &lt;- declare_reveal(Y, Z) ## Combine these design elements together thedesign &lt;- thepop + theys + theestimand + theassign + theobsident # Draw data matching this design set.seed(2201) dat2 &lt;- draw_data(thedesign) ## Now add individual-level weights to the data ## (under complete_ra within blocks, these will ## be the same across re-randomizations; OK to ## not recalculate within the simulation). dat2 &lt;- dat2 %&gt;% group_by(b) %&gt;% mutate( nb = n(), # Size of block pib = mean(Z), # Prob. of treatment assignment nTb = sum(Z), # Number treated nCb = nb - nTb, # Number control nbwt = (Z/pib) + ((1-Z)/(1-pib)), # Unbiased regression weight hbwt = nbwt * (pib * (1 - pib)) # Precision regression weight ) ## Declare a new population, and generate another design thepop2 &lt;- declare_population(dat2) thedesign2 &lt;- thepop2 + theys + theestimand + theassign + theobsident ## Create the block level dataset, with block level weights. datB &lt;- group_by(dat2,b) %&gt;% summarize( taub = mean(Y[Z==1]) - mean(Y[Z==0]), # Effect (variance below) truetaub = mean(y1) - mean(y0), # True effect nb = n(), # Number in block nTb = sum(Z), # Number treated nCb = nb - nTb, # Number control estvartaub = (nb/(nb-1)) * (var(Y[Z==1])/nTb) + (var(Y[Z==0])/nCb), pb = mean(Z), # Proportion treated nbwt = unique(nb/nrow(dat2)), # Block-size weight pbwt = pb * ( 1 - pb), hbwt2 = nbwt * pbwt, # Precision weights hbwt3 = pbwt * nb, hbwt = (2*(nCb * nTb)/(nTb + nCb)) ) datB$greenlabrule &lt;- 20*datB$hbwt3/sum(datB$hbwt3) ## All of these expressions of the harmonic mean weight are the same datB$hbwt01 &lt;- datB$hbwt/sum(datB$hbwt) datB$hbwt02 &lt;- datB$hbwt2/sum(datB$hbwt2) datB$hbwt03 &lt;- datB$hbwt3/sum(datB$hbwt3) stopifnot(all.equal(datB$hbwt01, datB$hbwt02)) stopifnot(all.equal(datB$hbwt01, datB$hbwt03)) ## What is the &quot;true&quot; ATE? trueATE1 &lt;- with(dat2, mean(y1) - mean(y0)) trueATE2 &lt;- with(datB, sum(truetaub*nbwt)) stopifnot(all.equal(trueATE1, trueATE2)) ## We could define the following as an estimand, too. ## trueATE3 &lt;- with(datB, sum(truetaub*hbwt01)) ## c(trueATE1,trueATE2,trueATE3) ## We can get the same answer using R&apos;s weighted.mean command trueATE2b &lt;- weighted.mean(datB$truetaub, w=datB$nbwt) stopifnot(all.equal(trueATE2b, trueATE2)) ** Define a program to load base dataset and randomly re-assign treatment capture program drop sample_from program define sample_from, rclass * Load simulated data from R import delimited using &quot;blocksimdat.csv&quot;, clear * Get # treated for each block levelsof nb, local(nblevels) local numtreat local i = 0 foreach l of local nblevels { local ++i // block indices (1, 2, 3,... 10) * even block indices (2nd, 4th, etc.) get 10% treated if mod(`i&apos;,2) == 0 { local temp = `l&apos;/10 local numtreat `numtreat&apos; `temp&apos; } * odd get 50% treated else { local temp = `l&apos;/2 local numtreat `numtreat&apos; `temp&apos; } } * Re-assign (simulated) treatment in this draw of the data capture drop __0* block_ra znew, block_var(bf) block_m(`numtreat&apos;) replace * No additional treatment effects * (assigning new potential outcomes) gen y_znew_0 = y0 gen y_znew_1 = y1 * Get true ATE in r() gen true_effect = y_znew_1 - y_znew_0 sum true_effect, meanonly return scalar ATE = r(mean) * Revealed outcome gen ynew = (znew * y_znew_1) + ((1 - znew) * y_znew_0) * Now add individual-level weights to the data * (under complete_ra within blocks, these will * be the same across re-randomizations). bysort bf: egen pib = mean(znew) // Prob. of treatment assignment bysort bf: egen nTb = total(znew) // Number treated gen nCb = nb - nTb // Number control gen nbwt = (znew/pib) + ((1-znew)/(1-pib)) // Unbiased regression weight gen hbwt = nbwt * (pib * (1 - pib)) // Precision regression weight end ** Draw data using this program once and save the true ATE set seed 2201 sample_from global trueATE1 = round(r(ATE), 0.01) ** For illustration: how to prepare block-level data as in the R code. ** In practice, we&apos;ll use data from the R code for comparison. ** Prepare to create a block level dataset, with block level weights. bysort bf znew: egen treatmean = mean(ynew) if znew == 1 // Treat and control means bysort bf znew: egen controlmean = mean(ynew) if znew == 0 bysort bf znew: egen treatsd = sd(ynew) if znew == 1 // Treat and control SD bysort bf znew: egen controlsd = sd(ynew) if znew == 0 qui count gen total_n = r(N) ** Collapse to block-level collapse /// (mean) treatmean controlmean treatsd controlsd y1 y0 pb = znew /// (first) nb nTb nCb total_n, /// by(bf) ** Additional variable preparation gen taub = treatmean - controlmean // Observed effect (variance below) gen truetaub = y1 - y0 // True effect gen treatvar = treatsd * treatsd gen controlvar = controlsd * controlsd gen estvartaub = (nb/(nb-1)) * (treatvar/nTb) + (controlvar/nCb) gen nbwt = nb / total_n // Block size weight gen pbwt = pb * (1 - pb) // pb is proportion treated gen hbwt2 = nbwt * pbwt // Precision weights gen hbwt3 = pbwt * nb gen hbwt = (2*(nCb * nTb)/(nTb + nCb)) qui sum hbwt3 gen greenlabrule = 20 * hbwt3 / r(sum) ** All of these expressions of the harmonic mean weight are the same qui sum hbwt gen double hbwt01 = round(hbwt / r(sum), 0.000001) // Precision issues qui sum hbwt2 gen double hbwt02 = round(hbwt2 / r(sum), 0.000001) qui sum hbwt3 gen double hbwt03 = round(hbwt3 / r(sum), 0.000001) assert hbwt01 == hbwt02 assert hbwt01 == hbwt03 ** What is the &quot;true&quot; ATE? gen truetaubweighted = truetaub * nbwt qui sum truetaubweighted global trueATE2 = round(r(sum), 0.01) assert $trueATE1 == $trueATE2 // After rounding ** We could define the following as an estimand, too. * gen truetaubweighted2 = truetaub * hbwt01 * qui sum truetaubweighted2 * global trueATE3 = round(r(sum), 0.01) We can now review the design: blocknumber treatment 1 2 3 4 5 6 7 8 9 10 0 4 18 15 36 25 54 35 72 50 720 1 4 2 15 4 25 6 35 8 50 80 With those simulations prepared, we&#x2019;ll start by comparing multiple approaches to getting an unbiased block-size-weighted average treatment effect estimate, &#x3C4;&#x2C9;nbwt\\bar{\\tau}_{\\text{nbwt}}&#x3C4;&#x2C9;nbwt&#x200B;. Notice that we do not use linear, additive fixed effects adjustment in any of these.28 Two of these approaches do include fixed effects dummies, but they are mean-centered and interacted with treatment assignment &#x2014; in other words, we include the fixed effects via Lin (2013) adjustment.29 simple_block refers to calculating mean differences within blocks and then taking the block-size weighted average of them diffmeans uses the difference_in_means function from the estimatr package (Blair et al. 2022) lmlin applies Lin covariance adjustment via block indicators, using the lm_lin function lmlinbyhand verifies that function using matrix operations; see this entry from the Green lab&#x2019;s SOP. regwts uses the basic OLS function from R lm with appropriate weights. Let&#x2019;s compare the estimates themselves using a single dataset before running our simulation: R code Stata code Hide ## simple_block ate_nbwt1 &lt;- with(datB,sum(taub*nbwt)) ## diffmeans ate_nbwt2 &lt;- difference_in_means(Y~Z,blocks=b,data=dat2) ## lmlin ate_nbwt3 &lt;- lm_lin(Y~Z,covariates=~bF,data=dat2) ## regwts ate_nbwt5 &lt;- lm_robust(Y~Z,data=dat2,weights=nbwt) ate_nbwt5a &lt;- lm(Y~Z,data=dat2,weights=nbwt) ate_nbwt5ase &lt;- coeftest(ate_nbwt5a,vcov=vcovHC(ate_nbwt5a,type=&quot;HC2&quot;)) ## lmlinbyhand X &lt;- model.matrix(~bF-1,data=dat2) barX &lt;- colMeans(X) Xmd &lt;- sweep(X,2,barX) stopifnot(all.equal((X[,3]-mean(X[,3])), Xmd[,3])) ZXmd &lt;- sweep(Xmd,1,dat2$Z,FUN=&quot;*&quot;) stopifnot(all.equal(dat2$Z*Xmd[,3],ZXmd[,3])) bigX &lt;- cbind(Intercept=1,Z=dat2$Z,Xmd[,-1],ZXmd[,-1]) bigXdf &lt;- data.frame(bigX,Y=dat2$Y) ate_nbwt4 &lt;-lm(Y~.-1,data=bigXdf) ate_nbwt4se &lt;- coeftest(ate_nbwt4,vcov.=vcovHC(ate_nbwt4,type=&quot;HC2&quot;)) ## List all nbwtates&lt;-c( simple_block=ate_nbwt1, diffmeans=ate_nbwt2$coefficients[[&quot;Z&quot;]], lmlin=ate_nbwt3$coefficients[[&quot;Z&quot;]], lmlinbyhand=ate_nbwt4$coefficients[[&quot;Z&quot;]], regwts=ate_nbwt5$coefficients[[&quot;Z&quot;]] ) nbwtates ** Load block-level data from the R code import delimited using &quot;datB.csv&quot;, clear ** simple_block gen taubweighted = taub * nbwt qui sum taubweighted global ate_nbwt1 = r(sum) tempvar calc_se gen `calc_se&apos; = nbwt^2 * estvartaub qui sum `calc_se&apos; global ate_nbwt1se = sqrt(r(sum)) ** design-based diffmeans estimator (return to observation level data) * See the DeclareDesign package mathematical notes. import delimited using &quot;blocksimdat2.csv&quot;, clear // dat2 in the R code rename (y z ntb ncb) (ynew znew nTb nCb) // Adjust names to match illustrative code above egen blockmean = mean(ynew), by(bf znew) egen blocksd = sd(ynew), by(bf znew) bysort bf (znew): gen diffmean = blockmean[_N] - blockmean[1] // with sorting, treat - control bysort bf (znew): gen variance = ((blocksd[_N]^2)/nTb[_N]) + ((blocksd[1]^2)/nCb[1]) qui sum diffmean, meanonly global ate_nbwt2 = r(mean) qui count gen forblockedse = (nb/r(N))^2 * variance bysort bf: replace forblockedse = . if _n != 1 qui sum forblockedse global ate_nbwt2se = sqrt(r(sum)) ** lmlinbyhand tabulate bf, gen(block_) drop block_1 qui reg ynew znew i.bf gen samp = e(sample) // Ensure correct sample used foreach var of varlist block_* { qui sum `var&apos; if samp == 1, meanonly gen mc_`var&apos; = `var&apos; - `r(mean)&apos; } qui reg ynew i.znew##c.(mc_block_*), vce(hc2) global ate_nbwt3 = _b[1.znew] global ate_nbwt3se = _se[1.znew] ** regwts qui reg ynew znew [aw = nbwt], vce(hc2) global ate_nbwt5 = _b[znew] global ate_nbwt5se = _se[znew] ** List all di &quot;simple_block = $ate_nbwt1&quot; di &quot;diffmeans = $ate_nbwt2&quot; di &quot;lmlinbyhand = $ate_nbwt3&quot; di &quot;regwts = $ate_nbwt5&quot; simple_block diffmeans lmlin lmlinbyhand regwts -12823 -12823 -12823 -12823 -12823 Let&#x2019;s also quickly review their standard errors: R code Stata code Hide ## Comparing the Standard Errors ate_nbwt1se &lt;- sqrt(sum(datB$nbwt^2 * datB$estvartaub)) nbwtses &lt;- c( simple_block=ate_nbwt1se, diffmeans=ate_nbwt2$std.error, lmlin=ate_nbwt3$std.error[[&quot;Z&quot;]], lmlinbyhand=ate_nbwt4se[&quot;Z&quot;,&quot;Std. Error&quot;], regwts=ate_nbwt5$std.error[[&quot;Z&quot;]] ) nbwtses ** Comparing the Standard Errors di &quot;simple_block = $ate_nbwt1se&quot; di &quot;diffmeans = $ate_nbwt2se&quot; di &quot;lmlinbyhand = $ate_nbwt3se&quot; di &quot;regwts = $ate_nbwt5se&quot; simple_block diffmeans.Z lmlin lmlinbyhand regwts 182.0 181.3 181.3 181.3 441.0 As noted above, weighting by block size allows us to define the average treatment effect in a way that treats each unit equally. And we have just illustrated six different ways to estimate this effect. If we want to calculate standard errors for these estimators (e.g., to produce confidence intervals), we will, in general, be leaving statistical power on the table in exchange for an easier to interpret estimate, and an estimator that relates to its underlying target in an unbiased manner. Next, we show an approach to blocking adjustment that is optimal from the perspective of statistical power, or narrow confidence intervals. As discussed above, we call this the &#x201C;precision-weighted&#x201D; average treatment effect.30 In some literatures, it&#x2019;s typical to use OLS regression machinery to calculate an ATE that is weighted to account for blocking &#x2014; i.e., a &#x201C;Least Squared Dummy Variables&#x201D; (LSDV) approach to including block fixed effects in a regression. We show here that this is simply another version of the precision-weighted ATE estimator. simple_block calculates a simple differences of means within blocks and then takes a weighted average of those differences, using precision weights lm_fixed_effects1 uses lm_robust with binary indicators for blocks lm_fixed_effects2 uses lm_robust with the fixed_effects option including a factor variable recording block membership direct_wts uses lm_robust without block-indicators but with precision weights demeaned regresses a block-centered version of the outcome on a block-centered version of the treatment indicator (i.e., including block fixed effects through a within estimator). Let&#x2019;s compare the estimates themselves using a single dataset first: R code Stata code Hide ## simple_block ate_hbwt1 &lt;- with(datB, sum(taub*hbwt01)) ## lm_fixed_effects1 ate_hbwt2 &lt;- lm_robust(Y~Z+bF,data=dat2) ## lm_fixed_effects2 ate_hbwt3 &lt;- lm_robust(Y~Z,fixed_effects=~bF,data=dat2) ## direct_wts ate_hbwt4 &lt;- lm_robust(Y~Z,data=dat2,weights=hbwt) ## demeaned ate_hbwt5 &lt;- lm_robust(I(Y-ave(Y,b))~I(Z-ave(Z,b)),data=dat2) ## List all hbwtates &lt;- c( simple_block=ate_hbwt1, lm_fixed_effects1=ate_hbwt2$coefficients[[&quot;Z&quot;]], lm_fixed_effects2=ate_hbwt3$coefficients[[&quot;Z&quot;]], direct_wts=ate_hbwt4$coefficients[[&quot;Z&quot;]], demeaned=ate_hbwt5$coefficient[[2]] ) hbwtates ** simple_block (return to block level data) import delimited using &quot;datB.csv&quot;, clear capture drop taubweighted_prec gen taubweighted_prec = taub * hbwt01 qui sum taubweighted_prec global ate_hbwt1 = r(sum) capture drop __0* tempvar calc_se2 gen `calc_se2&apos; = hbwt01^2 * estvartaub qui sum `calc_se2&apos; global ate_hbwt1se = sqrt(r(sum)) ** lm_fixed_effects1 (return to observation level data) import delimited using &quot;blocksimdat2.csv&quot;, clear // dat2 in the R code rename (y z ntb ncb) (ynew znew nTb nCb) // Adjust names to match illustrative code above qui reg ynew znew i.bf, vce(hc2) global ate_hbwt2 = _b[znew] global ate_hbwt2se = _se[znew] ** lm_fixed_effects2 (SEs differ from R variant of this) qui areg ynew znew, absorb(bf) vce(hc2) global ate_hbwt3 = _b[znew] global ate_hbwt3se = _se[znew] ** direct_wts qui reg ynew znew [aw = hbwt], vce(hc2) global ate_hbwt4 = _b[znew] global ate_hbwt4se = _se[znew] ** demeaned bysort bf: egen meanz = mean(znew) bysort bf: egen meany = mean(ynew) gen demeany = ynew - meany gen demeanz = znew - meanz qui reg demeany demeanz, vce(hc2) global ate_hbwt5 = _b[demeanz] global ate_hbwt5se = _se[demeanz] ** List all di &quot;simple_block = $ate_hbwt1&quot; di &quot;lm_fixed_effects1 = $ate_hbwt2&quot; di &quot;lm_fixed_effects2 = $ate_hbwt3&quot; di &quot;direct_wts = $ate_hbwt4&quot; di &quot;demeaned = $ate_hbwt5&quot; simple_block lm_fixed_effects1 lm_fixed_effects2 direct_wts demeaned -13981 -13981 -13981 -13981 -13981 Let&#x2019;s also quickly review their standard errors: R code Stata code Hide ## Comparing the Standard Errors ate_hbwt1se &lt;- sqrt(sum(datB$hbwt01^2 * datB$estvartaub)) hbwtses &lt;- c( simple_block=ate_hbwt1se, lm_fixed_effects1=ate_hbwt2$std.error[[&quot;Z&quot;]], lm_fixed_effects2=ate_hbwt3$std.error[[&quot;Z&quot;]], direct_wts=ate_hbwt4$std.error[[&quot;Z&quot;]], demeaned=ate_hbwt5$std.error[[2]] ) hbwtses ** Comparing the Standard Errors di &quot;simple_block = $ate_hbwt1se&quot; di &quot;lm_fixed_effects1 = $ate_hbwt2se&quot; di &quot;lm_fixed_effects2 = $ate_hbwt3se&quot; di &quot;direct_wts = $ate_hbwt4se&quot; di &quot;demeaned = $ate_hbwt5se&quot; simple_block lm_fixed_effects1 lm_fixed_effects2 direct_wts demeaned 328.7 716.6 716.6 788.3 705.4 We claimed that the block size weighted estimator is unbiased but perhaps less precise than the precision-weighted estimator. The evidence from this one draw of data does not bear that prediction out. But this isn&#x2019;t a very rigorous comparison. To provide a better comparison, in R, we&#x2019;ll use DeclareDesign to evaluate the performance of these estimators. We implement them as functions passed to the diagnose_design function in the next code block. In Stata, we use the same approach as above, passing manually-written programs to simulate. R code Stata code Hide ## Define estimator functions employed in the simulation below # Two options that don&apos;t account for blocking estnowtHC2 &lt;- declare_estimator(Y~Z, inquiry=theestimand, .method=lm_robust, label=&quot;E1: Ignores Blocks, Design (HC2) SE&quot;) estnowtIID &lt;- declare_estimator(Y~Z, inquiry=theestimand, .method=lm, label=&quot;E0: Ignores Blocks, OLS SE&quot;) # Two options applying block-size weights estnbwt1 &lt;- declare_estimator(Y~Z, inquiry=theestimand, .method=difference_in_means, blocks=b, label=&quot;E2: Diff Means Block Size Weights, Design SE&quot;) estnbwt2 &lt;- declare_estimator(Y~Z, inquiry=theestimand, .method=lm_lin, covariates=~bF, label=&quot;E3: Treatment Interaction with Block Indicators, Design SE&quot;) # Another block-size-weighted option (via regression weights) nbwt_est_fun &lt;- function(data){ data$newnbwt &lt;- with(data, (Z/pib) + ((1-Z)/(1-pib))) obj &lt;- lm_robust(Y~Z,data=data, weights=newnbwt) res &lt;- tidy(obj) %&gt;% filter(term==&quot;Z&quot;) return(res) } estnbwt3 &lt;- declare_estimator(handler=label_estimator(nbwt_est_fun), inquiry=theestimand, label=&quot;E4: Least Squares with Block Size Weights, Design SE&quot;) # Two precision-weighted options esthbwt1 &lt;- declare_estimator(Y~Z+bF, inquiry=theestimand, .method=lm_robust, label=&quot;E5: Precision Weights via Fixed Effects, Design SE&quot;) esthbwt2 &lt;- declare_estimator(Y~Z, inquiry=theestimand, .method=lm_robust, fixed_effects=~bF, label=&quot;E6: Precision Weights via Demeaning, Design SE&quot;) # Another precision-weighted option (regression weights) hbwt_est_fun &lt;- function(data){ data$newnbwt &lt;- with(data, (Z/pib) + ((1-Z)/(1-pib))) data$newhbwt &lt;- with(data, newnbwt * (pib * (1 - pib))) obj &lt;- lm_robust(Y~Z,data=data, weights=newhbwt) res &lt;- tidy(obj) %&gt;% filter(term==&quot;Z&quot;) return(res) } esthbwt3 &lt;- declare_estimator(handler=label_estimator(hbwt_est_fun), inquiry=theestimand, label=&quot;E7: Direct Precision Weights, Design SE&quot;) # A final precision-weighted option (demeaning) direct_demean_fun &lt;- function(data){ data$Y &lt;- with(data, Y - ave(Y,b)) data$Z &lt;- with(data, Z - ave(Z,b)) obj &lt;- lm_robust(Y~Z, data=data) data.frame(term = &quot;Z&quot; , estimate = obj$coefficients[[2]], std.error = obj$std.error[[2]], statistic = obj$statistic[[2]], p.value=obj$p.value[[2]], conf.low=obj$conf.low[[2]], conf.high=obj$conf.high[[2]], df=obj$df[[2]], outcome=&quot;Y&quot;) } esthbwt4 &lt;- declare_estimator(handler=label_estimator(direct_demean_fun), inquiry=theestimand, label=&quot;E8: Direct Demeaning, Design SE&quot;) ## Vector of all the estimator function names just created ## (via regular expression) theestimators &lt;- ls(patt=&quot;^est.*?wt&quot;) theestimators ## Get results for each estimator checkest &lt;- sapply( theestimators, function(x){ get(x)(as.data.frame(dat2))[c(&quot;estimate&quot;,&quot;std.error&quot;)] } ) ## Combine all of these design objects thedesignPlusEstimators &lt;- thedesign2 + estnowtHC2 + estnowtIID + estnbwt1 + estnbwt2 + estnbwt3 + esthbwt1 + esthbwt2 + esthbwt3 + esthbwt4 ## Update the default diagnosands for this simulation diagnosands &lt;- declare_diagnosands( mean_estimand = mean(estimand), mean_estimate = mean(estimate), bias = mean(estimate - estimand), sd_estimate = sqrt(pop.var(estimate)), mean_se = mean(std.error), rmse = sqrt(mean((estimate - estimand) ^ 2)), power = mean(p.value &lt;= 0.05), coverage = mean(estimand &lt;= conf.high &amp; estimand &gt;= conf.low) ) ## Perform the simulation sims &lt;- 200 set.seed(12345) thediagnosis &lt;- diagnose_design(thedesignPlusEstimators, sims=sims, bootstrap_sims = 0, diagnosands = diagnosands) ** Define a program to apply various estimation strategies ** to a dataset drawn via sample_from. capture program drop apply_estimators program define apply_estimators, rclass ** Call the data generation program defined above sample_from return scalar ATE = r(ATE) ** E0: Ignores Blocks, OLS SE qui reg ynew znew return scalar estnowtIID_est = _b[znew] return scalar estnowtIID_se = _se[znew] return scalar estnowtIID_p = r(table)[&quot;pvalue&quot;, &quot;znew&quot;] ** E1: Ignores Blocks, Design (HC2) SE qui reg ynew znew, vce(hc2) return scalar estnowtHC2_est = _b[znew] return scalar estnowtHC2_se = _se[znew] return scalar estnowtHC2_p = r(table)[&quot;pvalue&quot;, &quot;znew&quot;] ** E2: Design-based difference in means, Design SE egen blockmean = mean(ynew), by(bf znew) egen blocksd = sd(ynew), by(bf znew) bysort bf (znew): gen diffmean = blockmean[_N] - blockmean[1] // with sorting, treat - control bysort bf (znew): gen variance = ((blocksd[_N]^2)/nTb[_N]) + ((blocksd[1]^2)/nTb[1]) qui sum diffmean, meanonly local est = r(mean) return scalar estnbwt1_est = `est&apos; qui count local N = r(N) gen forblockedse = (nb/`N&apos;)^2 * variance bysort bf: replace forblockedse = . if _n != 1 qui sum forblockedse local se = sqrt(r(sum)) return scalar estnbwt1_se = `se&apos; qui levelsof bf, local(b) local J: word count `b&apos; local df = `N&apos; - (2*`J&apos;) return scalar estnbwt1_p = (2 * ttail(`df&apos;, abs(`est&apos;/`se&apos;))) ** E3: Treatment Interaction with Block Indicators, Design SE qui tabulate bf, gen(block_) drop block_1 qui reg ynew znew i.bf gen samp = e(sample) foreach var of varlist block_* { qui sum `var&apos; if samp == 1, meanonly gen mc_`var&apos; = `var&apos; - r(mean) } qui reg ynew i.znew##c.(mc_block_*), vce(hc2) return scalar estnbwt2_est = _b[1.znew] return scalar estnbwt2_se = _se[1.znew] return scalar estnbwt2_p = r(table)[&quot;pvalue&quot;, &quot;1.znew&quot;] ** E4: Least Squares with Block Size Weights, Design SE qui reg ynew znew [aw = nbwt], vce(hc2) return scalar estnbwt4_est = _b[znew] return scalar estnbwt4_se = _se[znew] return scalar estnbwt4_p = r(table)[&quot;pvalue&quot;, &quot;znew&quot;] ** E5: Precision Weights via Fixed Effects, Design SE qui reg ynew znew i.bf, vce(hc2) return scalar esthbwt1_est = _b[znew] return scalar esthbwt1_se = _se[znew] return scalar esthbwt1_p = r(table)[&quot;pvalue&quot;, &quot;znew&quot;] ** E6: Precision Weights via Demeaning, Design SE qui areg ynew znew, absorb(bf) vce(hc2) return scalar esthbwt2_est = _b[znew] return scalar esthbwt2_se = _se[znew] return scalar esthbwt2_p = r(table)[&quot;pvalue&quot;, &quot;znew&quot;] ** E7: Direct Precision Weights, Design SE qui reg ynew znew [aw = hbwt], vce(hc2) return scalar esthbwt3_est = _b[znew] return scalar esthbwt3_se = _se[znew] return scalar esthbwt3_p = r(table)[&quot;pvalue&quot;, &quot;znew&quot;] ** E8: Direct Demeaning, Design SE bysort bf: egen meanz = mean(znew) bysort bf: egen meany = mean(ynew) gen demeany = ynew - meany gen demeanz = znew - meanz qui reg demeany demeanz, vce(hc2) return scalar esthbwt4_est = _b[demeanz] return scalar esthbwt4_se = _se[demeanz] return scalar esthbwt4_p = r(table)[&quot;pvalue&quot;, &quot;demeanz&quot;] end ** Perform the simulation apply_estimators local rscalars : r(scalars) local to_store &quot;&quot; foreach item of local rscalars { local to_store &quot;`to_store&apos; `item&apos; = r(`item&apos;)&quot; } simulate /// `to_store&apos;, /// reps(200): /// apply_estimators ** Create summary matrix qui des, short di (`r(k)&apos; - 1)/3 matrix diagnosands = J((`r(k)&apos; - 1)/3, 6, .) matrix rownames diagnosands = &quot;E0: Ignores Blocks, OLS SE&quot; &quot;E1: Ignores Blocks, Design (HC2) SE&quot; &quot;E2: Diff Means BW, Design SE&quot; &quot;E3: Lin BW, Design SE&quot; &quot;E4: Direct BW, Design SE&quot; &quot;E5: FE PW, Design SE&quot; &quot;E6: Demean PW, Design SE&quot; &quot;E7: Direct PW, Design SE&quot; &quot;E8: Direct Demeaning, Design SE&quot; matrix colnames diagnosands = &quot;Mean estimand&quot; &quot;Mean estimate&quot; &quot;Bias&quot; &quot;SD Estimate&quot; &quot;MeanSE&quot; &quot;Coverage&quot; ** Get variable name roots to loop through local roots estnowtIID estnowtHC2 estnbwt1 estnbwt2 estnbwt4 esthbwt1 esthbwt2 esthbwt3 esthbwt4 ** Calculate quantities to include local row = 0 foreach l of local roots { local ++row * Estimand qui sum ATE, meanonly matrix diagnosands[`row&apos;, 1] = `r(mean)&apos; * Estimate qui sum `l&apos;_est, meanonly matrix diagnosands[`row&apos;, 2] = `r(mean)&apos; * Bias qui gen biascalc = `l&apos;_est - ATE qui sum biascalc, meanonly matrix diagnosands[`row&apos;, 3] = r(mean) drop biascalc * SD estimate (based on population variance, no n-1 in the variance denom.) qui sum `l&apos;_est qui gen sdcalc = (`l&apos;_est - r(mean))^2 qui sum sdcalc matrix diagnosands[`row&apos;, 4] = sqrt(r(sum)/r(N)) drop sdcalc * Mean SE qui sum `l&apos;_se matrix diagnosands[`row&apos;, 5] = `r(mean)&apos; * Coverage (z-stat CIs to limit estimates stored above) qui gen conflow = `l&apos;_est - (1.96 * `l&apos;_se) qui gen confhigh = `l&apos;_est + (1.96 * `l&apos;_se) qui gen inrange = ATE &lt;= confhigh &amp; ATE &gt;= conflow qui sum inrange matrix diagnosands[`row&apos;, 6] = r(mean) drop conf* inrange } * View the results matrix list diagnosands We can see in the diagnostic output below that the estimators using block-size weights (E2, E3, E4, or E5) all eliminate bias (within simulation error).31 The estimators ignoring blocks (E0 and E1), are biased, and in this particular simulation the precision weighed estimators (E6&#x2013;E9) are also highly biased &#x2014; with some of them also producing poor &#x201C;coverage&#x201D; or false positive rates (E7 and E9). This output also shows us the &#x201C;SD Estimate&#x201D; (an estimate of the true standard error) and the &#x201C;Mean SE&#x201D; (which is the average of the analytic estimates of the standard error). In the case of E2, E3, E4 or E5 the Mean SE is larger than the SD Estimate &#x2014; this is good in that it means that our analytic standard errors will be conservative rather than overly liberal. However, we also would prefer that our analytic standard errors not be too conservative. Estimator Mean Estimand Mean Estimate Bias SD Estimate Mean Se Coverage E0: Ignores Blocks, OLS SE -12900.57 -12355.37 545.20 105.75 714.11 1.00 E1: Ignores Blocks, Design (HC2) SE -12900.57 -12355.37 545.20 105.75 352.66 0.94 E2: Diff Means Block Size Weights, Design SE -12900.57 -12900.11 0.46 115.36 165.80 0.99 E3: Treatment Interaction with Block Indicators, Design SE -12900.57 -12900.11 0.46 115.36 165.80 0.99 E4: Least Squares with Block Size Weights, Design SE -12900.57 -12900.11 0.46 115.36 444.82 1.00 E5: Precision Weights via Fixed Effects, Design SE -12900.57 -14143.05 -1242.48 208.11 713.30 0.74 E6: Precision Weights via Demeaning, Design SE -12900.57 -14143.05 -1242.48 208.11 713.30 0.74 E7: Direct Precision Weights, Design SE -12900.57 -14143.05 -1242.48 208.11 791.40 0.94 E8: Direct Demeaning, Design SE -12900.57 -14143.05 -1242.48 208.11 702.44 0.73 Why is the performance of the precision-weighted approach so poor so far, even just in terms of precision? Well, the simulated outcome data used in this example is highly skewed by design. All else equal, the precision-weighted approach makes a reasonable choice to allow some bias in return for reduced variance. But it&#x2019;s performance can suffer dramatically in the presence of common problems in administrative data like skewed outcomes or zero-inflation (i.e., many 0s). With skewed or zero-inflated outcomes, the block-size weighted approach may be safer. One solution we could apply to allow more confident use of the precision-weighted approach is rank-transforming the outcome measure. We apply this change in the code chunk below. This transformation nearly erases the bias from the block-size weighted estimators, and the precision-weighted approaches now perform closer to our expectations (they produce identical effect estiates after rounding here, with a slightly lower standard error). Ignoring the blocking design is still a problem&#x2014;E0 and E1 continue to show substantial bias. Moreover, notice here and above that although using regression weights to apply these blocking adjustments (rather than, e.g., an appropriate fixed effects specification) produces the right estimates, it may do so at the cost of more conservative standard error estimates. R code Stata code Hide ## Define a function to rank-transform the potential outcomes po_functionNorm &lt;- function(data){ data &lt;- data %&gt;% group_by(b) %&gt;% mutate( Y_Z_0=rank(y0), Y_Z_1=rank(y1) ) return(as.data.frame(data)) } ## Redefine relevant DeclareDesign objects theysNorm &lt;- declare_potential_outcomes(handler = po_functionNorm) thedesignNorm &lt;- thepop2 + theysNorm + theestimand + theassign + theobsident datNorm &lt;- draw_data(thedesignNorm) thedesignPlusEstimatorsNorm &lt;- thedesignNorm + estnowtHC2 + estnowtIID + estnbwt1 + estnbwt2 + estnbwt3 + esthbwt1 + esthbwt2 + esthbwt3 + esthbwt4 ## Perform a new simulation sims &lt;- 200 thediagnosisNorm &lt;- diagnose_design(thedesignPlusEstimatorsNorm, sims = sims, bootstrap_sims = 0, diagnosands = diagnosands) ** Perform this simulation again with rank-transformed potential outcomes ** Redefine sampling program to consider ranked potential outcomes capture program drop sample_from program define sample_from, rclass * Load simulated data from R import delimited using &quot;blocksimdat.csv&quot;, clear * Get # treated for each block levelsof nb, local(nblevels) local numtreat local i = 0 foreach l of local nblevels { local ++i // block indices (1, 2, 3,... 10) * even block indices (2nd, 4th, etc.) get 10% treated if mod(`i&apos;,2) == 0 { local temp = `l&apos;/10 local numtreat `numtreat&apos; `temp&apos; } * odd get 50% treated else { local temp = `l&apos;/2 local numtreat `numtreat&apos; `temp&apos; } } * Re-assign (simulated) treatment in this draw of the data capture drop __0* block_ra znew, block_var(bf) block_m(`numtreat&apos;) replace * No additional treatment effects, but rank POs egen y_znew_0 = rank(y0), unique // Different than how ties are handled in R. egen y_znew_1 = rank(y1), unique // R rank command averages by default. * Get true ATE in r() gen true_effect = y_znew_1 - y_znew_0 sum true_effect, meanonly return scalar ATE = r(mean) * Revealed outcome gen ynew = (znew * y_znew_1) + ((1 - znew) * y_znew_0) * Now add individual-level weights to the data * (under complete_ra within blocks, these will * be the same across re-randomizations). bysort bf: egen pib = mean(znew) // Prob. of treatment assignment bysort bf: egen nTb = total(znew) // Number treated gen nCb = nb - nTb // Number control gen nbwt = (znew/pib) + ((1-znew)/(1-pib)) // Unbiased regression weight gen hbwt = nbwt * (pib * (1 - pib)) // Precision regression weight end ** Estimators program above can then be re-used apply_estimators local rscalars : r(scalars) local to_store &quot;&quot; foreach item of local rscalars { local to_store &quot;`to_store&apos; `item&apos; = r(`item&apos;)&quot; } simulate /// `to_store&apos;, /// reps(200): /// apply_estimators ** Create summary matrix qui des, short di (`r(k)&apos; - 1)/3 matrix diagnosands = J((`r(k)&apos; - 1)/3, 6, .) matrix rownames diagnosands = &quot;E0: Ignores Blocks, OLS SE&quot; &quot;E1: Ignores Blocks, Design (HC2) SE&quot; &quot;E2: Diff Means BW, Design SE&quot; &quot;E3: Lin BW, Design SE&quot; &quot;E5: Direct BW, Design SE&quot; &quot;E6: FE PW, Design SE&quot; &quot;E7: Demean PW, Design SE&quot; &quot;E8: Direct PW, Design SE&quot; &quot;E9: Direct Demeaning, Design SE&quot; matrix colnames diagnosands = &quot;Mean estimand&quot; &quot;Mean estimate&quot; &quot;Bias&quot; &quot;SD Estimate&quot; &quot;MeanSE&quot; &quot;Coverage&quot; ** Get variable name roots to loop through local roots estnowtIID estnowtHC2 estnbwt1 estnbwt2 estnbwt4 esthbwt1 esthbwt2 esthbwt3 esthbwt4 ** Calculate quantities to include local row = 0 foreach l of local roots { local ++row * Estimand qui sum ATE, meanonly matrix diagnosands[`row&apos;, 1] = `r(mean)&apos; * Estimate qui sum `l&apos;_est, meanonly matrix diagnosands[`row&apos;, 2] = `r(mean)&apos; * Bias qui gen biascalc = `l&apos;_est - ATE qui sum biascalc, meanonly matrix diagnosands[`row&apos;, 3] = r(mean) drop biascalc * SD estimate (based on population variance, no n-1 in the variance denom.) qui sum `l&apos;_est qui gen sdcalc = (`l&apos;_est - r(mean))^2 qui sum sdcalc matrix diagnosands[`row&apos;, 4] = sqrt(r(sum)/r(N)) drop sdcalc * Mean SE qui sum `l&apos;_se matrix diagnosands[`row&apos;, 5] = `r(mean)&apos; * Coverage (z-stat CIs to limit estimates stored above) qui gen conflow = `l&apos;_est - (1.96 * `l&apos;_se) qui gen confhigh = `l&apos;_est + (1.96 * `l&apos;_se) qui gen inrange = ATE &lt;= confhigh &amp; ATE &gt;= conflow qui sum inrange matrix diagnosands[`row&apos;, 6] = r(mean) drop conf* inrange } * View the results matrix list diagnosands Estimator Mean Estimand Mean Estimate Bias SD Estimate Mean Se Coverage E0: Ignores Blocks, OLS SE 0.00 -127.08 -127.08 1.84 17.81 0.00 E1: Ignores Blocks, Design (HC2) SE 0.00 -127.08 -127.08 1.84 14.05 0.00 E2: Diff Means Block Size Weights, Design SE 0.00 0.11 0.11 1.69 5.49 1.00 E3: Treatment Interaction with Block Indicators, Design SE 0.00 0.11 0.11 1.69 5.49 1.00 E4: Least Squares with Block Size Weights, Design SE 0.00 0.11 0.11 1.69 15.66 1.00 E5: Precision Weights via Fixed Effects, Design SE 0.00 0.11 0.11 1.35 4.09 1.00 E6: Precision Weights via Demeaning, Design SE 0.00 0.11 0.11 1.35 4.09 1.00 E7: Direct Precision Weights, Design SE 0.00 0.11 0.11 1.35 15.28 1.00 E8: Direct Demeaning, Design SE 0.00 0.11 0.11 1.35 4.09 1.00 Lastly, note that when the probability of treatment is the same in each block (e.g., with a pair-randomized design), ignoring the blocking structure during estimation should not lead to meaningful bias. But we can still improve precision by taking blocking into account (Bowers 2011). This final simulation changes the design to still have unequal sized blocks, but now with a uniform probability of treatment assignment in each block (instead of a mix of 10% and 50%). Here, only two estimators show appreciable bias (E4 and E7). However, ignoring the blocks during estimation leads to overly conservative standard errors. R code Stata code Hide ## Re-define relevant declare design objects theassignEqual &lt;- declare_assignment(Z=block_ra(blocks = bF)) thedesignNormEqual &lt;- thepop2 + theysNorm + theestimand + theassignEqual + theobsident datNormEqual &lt;- draw_data(thedesignNormEqual) thedesignPlusEstimatorsNormEqual &lt;- thedesignNormEqual + estnowtHC2 + estnowtIID + estnbwt1 + estnbwt2 + estnbwt3 + esthbwt1 + esthbwt2 + esthbwt3 + esthbwt4 ## Perform a new simulation sims &lt;- 200 set.seed(12345) thediagnosisNormEqual &lt;- diagnose_design(thedesignPlusEstimatorsNormEqual, sims = sims, bootstrap_sims = 0, diagnosands = diagnosands) ** Perform this simulation again with equal assignment probabilities. ** Redefine sampling program once more. capture program drop sample_from program define sample_from, rclass * Load simulated data from R import delimited using &quot;blocksimdat.csv&quot;, clear * Re-assign (simulated) treatment in this draw of the data. capture drop __0* block_ra znew, block_var(bf) replace * No additional treatment effects, but rank POs egen y_znew_0 = rank(y0), unique // Different than how ties are handled in R. egen y_znew_1 = rank(y1), unique // R rank command averages by default. * Get true ATE in r() gen true_effect = y_znew_1 - y_znew_0 sum true_effect, meanonly return scalar ATE = r(mean) * Revealed outcome gen ynew = (znew * y_znew_1) + ((1 - znew) * y_znew_0) * Now add individual-level weights to the data * (under complete_ra within blocks, these will * be the same across re-randomizations). bysort bf: egen pib = mean(znew) // Prob. of treatment assignment bysort bf: egen nTb = total(znew) // Number treated gen nCb = nb - nTb // Number control gen nbwt = (znew/pib) + ((1-znew)/(1-pib)) // Unbiased regression weight gen hbwt = nbwt * (pib * (1 - pib)) // Precision regression weight end ** Estimators program above can then be re-used apply_estimators local rscalars : r(scalars) local to_store &quot;&quot; foreach item of local rscalars { local to_store &quot;`to_store&apos; `item&apos; = r(`item&apos;)&quot; } simulate /// `to_store&apos;, /// reps(200): /// apply_estimators ** Create summary matrix qui des, short di (`r(k)&apos; - 1)/3 matrix diagnosands = J((`r(k)&apos; - 1)/3, 6, .) matrix rownames diagnosands = &quot;E0: Ignores Blocks, OLS SE&quot; &quot;E1: Ignores Blocks, Design (HC2) SE&quot; &quot;E2: Diff Means BW, Design SE&quot; &quot;E3: Lin BW, Design SE&quot; &quot;E5: Direct BW, Design SE&quot; &quot;E6: FE PW, Design SE&quot; &quot;E7: Demean PW, Design SE&quot; &quot;E8: Direct PW, Design SE&quot; &quot;E9: Direct Demeaning, Design SE&quot; matrix colnames diagnosands = &quot;Mean estimand&quot; &quot;Mean estimate&quot; &quot;Bias&quot; &quot;SD Estimate&quot; &quot;MeanSE&quot; &quot;Coverage&quot; ** Get variable name roots to loop through local roots estnowtIID estnowtHC2 estnbwt1 estnbwt2 estnbwt4 esthbwt1 esthbwt2 esthbwt3 esthbwt4 ** Calculate quantities to include local row = 0 foreach l of local roots { local ++row * Estimand qui sum ATE, meanonly matrix diagnosands[`row&apos;, 1] = `r(mean)&apos; * Estimate qui sum `l&apos;_est, meanonly matrix diagnosands[`row&apos;, 2] = `r(mean)&apos; * Bias qui gen biascalc = `l&apos;_est - ATE qui sum biascalc, meanonly matrix diagnosands[`row&apos;, 3] = r(mean) drop biascalc * SD estimate (based on population variance, no n-1 in the variance denom.) qui sum `l&apos;_est qui gen sdcalc = (`l&apos;_est - r(mean))^2 qui sum sdcalc matrix diagnosands[`row&apos;, 4] = sqrt(r(sum)/r(N)) drop sdcalc * Mean SE qui sum `l&apos;_se matrix diagnosands[`row&apos;, 5] = `r(mean)&apos; * Coverage (z-stat CIs to limit estimates stored above) qui gen conflow = `l&apos;_est - (1.96 * `l&apos;_se) qui gen confhigh = `l&apos;_est + (1.96 * `l&apos;_se) qui gen inrange = ATE &lt;= confhigh &amp; ATE &gt;= conflow qui sum inrange matrix diagnosands[`row&apos;, 6] = r(mean) drop conf* inrange } * View the results matrix list diagnosands Estimator Mean Estimand Mean Estimate Bias SD Estimate Mean Se Coverage E0: Ignores Blocks, OLS SE 0.00 -0.34 -0.34 4.74 12.40 1.00 E1: Ignores Blocks, Design (HC2) SE 0.00 -0.34 -0.34 4.74 12.40 1.00 E2: Diff Means Block Size Weights, Design SE 0.00 -0.34 -0.34 4.74 7.36 1.00 E3: Treatment Interaction with Block Indicators, Design SE 0.00 -0.34 -0.34 4.74 7.36 1.00 E4: Least Squares with Block Size Weights, Design SE 0.00 77.57 77.57 4.09 11.89 0.00 E5: Precision Weights via Fixed Effects, Design SE 0.00 -0.34 -0.34 4.74 7.36 1.00 E6: Precision Weights via Demeaning, Design SE 0.00 -0.34 -0.34 4.74 7.36 1.00 E7: Direct Precision Weights, Design SE 0.00 127.05 127.05 2.71 10.95 0.00 E8: Direct Demeaning, Design SE 0.00 -0.34 -0.34 4.74 7.36 1.00 5.5.2.1 Summary of Approaches to the Analysis of Block Randomized Trials Our team often prefers block randomized trials because of their potential to increase statistical power, help us avoid bias, and help us better highlight subgroup effects. And we generally prefer to &#x201C;analyze as we randomize.&#x201D; But this can mean a number of things with blocked designs, and different designs will require different analytical decisions. Sometimes we may be willing to trade a small amount of bias to make our estimates more precise. Other studies will be so large or small that one or another estimation strategy will be obviously superior. We use our Analysis Plans to explain our choices, and generally rely on simulation studies when we are uncertain about the applicability of statistical rules of thumb to any given design. 5.6 Cluster-randomized trials In a cluster randomized trial, treatment is assigned at the level of larger groups of units, called &#x201C;clusters,&#x201D; instead of at the individual level.32 These studies tend to distinguish signal from noise less effectively than an experiment where we assign treatment directly to individuals &#x2014; i.e., they have less precision. The number of independent pieces of information available to learn about the treatment effect will generally be closer to the number of clusters (each of which tends to be assigned to treatment independently of each other) than the number of dependent observations within a cluster (See EGAP&#x2019;s 10 Things You Need to Know about Cluster Randomization).33 Since we analyze as we randomize, a cluster randomized experiment may require that we (1) weight cluster-level average treatment effects by cluster size if we are trying to estimate the average of the individual level causal effects (Middleton and Aronow 2015). They also require that we (2) change how we calculate standard errors and ppp-values to account for the fact that uncertainty is generated at the level of the cluster and not at the level of the individual (Hansen and Bowers 2008; Gerber and Green 2012). For example, imagine that we had 10 clusters (administrative offices, physicians groups, etc.) with half assigned to treatment and half assigned to control. R code Stata code Hide ## Create a copy of dat2 to use for our clustering examples dat3 &lt;- dat2 dat3$cluster &lt;- dat3$b dat3$clusterF &lt;- factor(dat3$cluster) ndat3 &lt;- nrow(dat3) ## Randomly assign half of the clusters to treatment and half to control set.seed(12345) dat3$Zcluster &lt;- cluster_ra(cluster=dat3$cluster) with(dat3,table(Zcluster,cluster)) ** Modify dat2 to use for our clustering examples import delimited using &quot;blocksimdat2.csv&quot;, clear // dat2 in the R code gen cluster = b ** Randomly assign half of the clusters to treatment and half to control set seed 12345 cluster_ra zcluster, cluster_var(cluster) table zcluster cluster cluster Zcluster 1 2 3 4 5 6 7 8 9 10 0 8 0 30 40 0 0 0 0 100 800 1 0 20 0 0 50 60 70 80 0 0 Although our example data has 1258 observations, we do not have 1258 pieces of independent information about the effect of the treatment because people were assigned in groups. Rather we have some amount of information in between 1258 and the number of clusters (in this case, 10). We can see above that the number of people within each cluster &#x2014; and notice that all of the people are coded as either control or treatment because assignment is at the level of the cluster. Before continuing, in R, let&#x2019;s set up some DeclareDesign elements for a simulation of clustered experimental designs. We&#x2019;ll do some comparable preparation in Stata. R code Stata code Hide ## Get weights for one estimation strategy used below library(ICC) iccres &lt;- ICCest(x=clusterF,y=Y,data=dat3) dat3$varweight &lt;- 1/(iccres$vara + (iccres$varw/dat3$nb)) ## Define various DeclareDesign elements thepop3 &lt;- declare_population(dat3) po_functionCluster &lt;- function(data){ data$Y_Zcluster_0 &lt;- data$y0 data$Y_Zcluster_1 &lt;- data$y1 data } theysCluster &lt;- declare_potential_outcomes(handler = po_functionCluster) theestimandCluster &lt;- declare_inquiry(ATE = mean(Y_Zcluster_1 - Y_Zcluster_0)) theassignCluster &lt;- declare_assignment(Zcluster=cluster_ra(clusters=cluster)) theobsidentCluster &lt;- declare_reveal(Y, Zcluster) thedesignCluster &lt;- thepop3 + theysCluster + theestimandCluster + theassignCluster + theobsidentCluster datCluster &lt;- draw_data(thedesignCluster) ** Update the sampling program for a basic clustered design capture program drop sample_from program define sample_from, rclass * Load baseline data import delimited using &quot;blocksimdat2.csv&quot;, clear // dat2 in the R code gen cluster = b * Re-assign (simulated) treatment in this draw of the data cluster_ra zcluster, cluster_var(cluster) * No additional treatment effects gen y_zcluster_0 = y0 gen y_zcluster_1 = y1 * Get true ATE in r() gen true_effect = y_zcluster_1 - y_zcluster_0 sum true_effect, meanonly return scalar ATE = r(mean) * Revealed outcome gen ynew = (zcluster * y_zcluster_1) + ((1 - zcluster) * y_zcluster_0) ** Get weights for one estimation strategy used below qui loneway ynew cluster gen varweight = 1 / ( r(sd_b)^2 + ( r(sd_w)^2 / nb ) ) end In everyday practice, with more than about 50 (equally-sized) clusters, we often produce estimates using more or less the same kinds estimators as those above, but changing our standard error calculations to instead rely on CR2 cluster-robust standard error (Pustejovsky 2019). We show two approaches to such adjustment. First, the design-based difference_in_means() function in R:34 ## CR2 via difference in means estAndSE4a &lt;- difference_in_means(Y~Zcluster, data=datCluster, clusters=cluster) estAndSE4a Design: Clustered Estimate Std. Error t value Pr(&gt;|t|) CI Lower CI Upper DF Zcluster -15597 8336 -1.871 0.1232 -37356 6162 4.76 Second, via linear regression with an appropriate error adjustment: R code Stata code Hide ## CR2 via linear regression estAndSE4b &lt;- lm_robust(Y~Zcluster, data=datCluster, clusters=cluster, se_type=&quot;CR2&quot;) estAndSE4b ** Get the treatment assignment used in the R code import delimited using &quot;datCluster.csv&quot;, clear ** Stata&apos;s default clustered errors reg y zcluster, vce(cluster cluster) local CR1 = _se[zcluster] ** CR2 errors via reg_sandwich * ssc install reg_sandwich reg_sandwich y zcluster, cluster(cluster) local CR2 = _se[zcluster] ** Notice: Stata&apos;s default is CR1, not CR2 macro list _CR1 _CR2 assert `CR1&apos; != `CR2&apos; Estimate Std. Error t value Pr(&gt;|t|) CI Lower CI Upper DF (Intercept) 15707 8332 1.885 0.1409 -8542 39955 3.578 Zcluster -15597 8336 -1.871 0.1232 -37356 6162 4.760 5.6.1 Bias when cluster size is correlated with potential outcomes When clusters have unequal sizes, in addition to adjusting our standard error calculations, we might worry about bias as well (Middleton and Aronow 2015) (see also this Declare Design blog post). Here, we demonstrate how bias could emerge in the analysis of a cluster randomized trial, as well as how to reduce that bias. R code Stata code Hide ## Define estimators that can be repeated in the simulation below ## No adjustment for clustering estC0 &lt;- declare_estimator(Y~Zcluster, inquiry=theestimand, .method=lm, label=&quot;C0: Ignores Clusters, IID SE&quot;) ## HC2 SEs estC1 &lt;- declare_estimator(Y~Zcluster, inquiry=theestimand, .method=lm_robust, label=&quot;C1: Ignores Clusters, HC22 SE&quot;) ## Clustered SEs (CR1; Stata default) estC2 &lt;- declare_estimator(Y~Zcluster, inquiry=theestimand, .method=lm_robust, clusters=cluster, se_type=&quot;stata&quot;, label=&quot;C2: OLS, CR1 SE (Stata)&quot;) ## CR2 SEs (difference in means) estC3 &lt;- declare_estimator(Y~Zcluster, inquiry=theestimand, .method=difference_in_means, clusters = cluster, label=&quot;C3: Diff Means, CR2 SE&quot;) ## CR2 SEs (linear regression) estC4 &lt;- declare_estimator(Y~Zcluster, inquiry=theestimand, .method=lm_robust, clusters = cluster, se_type=&quot;CR2&quot;, label=&quot;C4: OLS, CR2 SE&quot;) ## Horvitz-Thompson estimator with clustering estC5 &lt;- declare_estimator(Y~Zcluster, inquiry=theestimand, .method=horvitz_thompson, clusters=cluster, simple=FALSE, condition_prs=.5, label=&quot;C5: Horvitz-Thompson Cluster, Young SE&quot;) ## Linear regression, CR2 errors, weight by cluster size estC6 &lt;- declare_estimator(Y~Zcluster, inquiry=theestimand, .method=lm_robust, weights = nb, clusters=cluster, se_type=&quot;CR2&quot;, label=&quot;C6: OLS with Cluster Size Weights, CR2 SE&quot;) ## Linear regression, CR2 errors, control for cluster size, variance weights estC7 &lt;- declare_estimator(Y~Zcluster+nb, inquiry=theestimand, .method=lm_robust, weights=varweight, clusters=cluster, se_type=&quot;CR2&quot;, label=&quot;C7: OLS Clusters with Weights, CR2 RCSE&quot;) ## Linear regression, CR2 errors, control for cluster size estC8 &lt;- declare_estimator(Y~Zcluster+nb, inquiry=theestimand, .method=lm_robust, clusters=cluster, se_type=&quot;CR2&quot;, label=&quot;C8: OLS Clusters with adj for cluster size, CR2 RCSE&quot;) ## Linear regression, CR2 errors, lin estimation for cluster size estC9 &lt;- declare_estimator(Y~Zcluster, inquiry=theestimand, .method=lm_lin, covariates=~nb, clusters=cluster, se_type=&quot;CR2&quot;, label=&quot;C9: OLS Clusters with adj for cluster size, CR2 RCSE&quot;) ## Add all these estimators to the design thedesignClusterPlusEstimators &lt;- thedesignCluster + estC0 + estC1 + estC2 + estC3 + estC4 + estC5 + estC6 + estC7 + estC8 + estC9 ## Simulate the performance of these estimators sims &lt;- 200 set.seed(12345) thediagnosisCluster &lt;- diagnose_design(thedesignClusterPlusEstimators, sims = sims, bootstrap_sims = 0) ** Define estimators that can be repeated in the simulation below capture program drop apply_estimators program define apply_estimators, rclass syntax[ , datCluster] if &quot;`datCluster&apos;&quot; == &quot;&quot; { ** Call the data generation program defined above sample_from return scalar ATE = r(ATE) } else { ** Get the treatment assignment used in the R code import delimited using &quot;datCluster.csv&quot;, clear rename y ynew } ** C0: Ignores Clusters, IID SE qui reg ynew zcluster return scalar estC0_est = _b[zcluster] return scalar estC0_se = _se[zcluster] return scalar estC0_p = r(table)[&quot;pvalue&quot;, &quot;zcluster&quot;] ** C1: Ignores Clusters, HC2 SE qui reg ynew zcluster, vce(hc2) return scalar estC1_est = _b[zcluster] return scalar estC1_se = _se[zcluster] return scalar estC1_p = r(table)[&quot;pvalue&quot;, &quot;zcluster&quot;] ** C2: OLS, CR1 SE (Stata) qui reg ynew zcluster, vce(cluster cluster) return scalar estC2_est = _b[zcluster] return scalar estC2_se = _se[zcluster] return scalar estC2_p = r(table)[&quot;pvalue&quot;, &quot;zcluster&quot;] ** C3: OLS, CR2 SE qui reg_sandwich ynew zcluster, cluster(cluster) return scalar estC3_est = _b[zcluster] return scalar estC3_se = _se[zcluster] local p = (2 * ttail(e(dfs)[1,1], abs(_b[zcluster]/_se[zcluster]))) return scalar estC3_p = `p&apos; ** C6: OLS with Cluster Size Weights, CR2 SE qui reg_sandwich ynew zcluster [pw = nb], cluster(cluster) return scalar estC6_est = _b[zcluster] return scalar estC6_se = _se[zcluster] local p = (2 * ttail(e(dfs)[1,1], abs(_b[zcluster]/_se[zcluster]))) return scalar estC6_p = `p&apos; ** C7: OLS with Cluster Control and Var. Weights, CR2 SE qui reg_sandwich ynew zcluster nb [pw = varweight], cluster(cluster) return scalar estC7_est = _b[zcluster] return scalar estC7_se = _se[zcluster] local p = (2 * ttail(e(dfs)[1,1], abs(_b[zcluster]/_se[zcluster]))) return scalar estC7_p = `p&apos; ** C8: OLS Clusters with adj for cluster size, CR2 RCSE qui reg_sandwich ynew zcluster nb, cluster(cluster) return scalar estC8_est = _b[zcluster] return scalar estC8_se = _se[zcluster] local p = (2 * ttail(e(dfs)[1,1], abs(_b[zcluster]/_se[zcluster]))) return scalar estC8_p = `p&apos; ** C9: OLS Clusters with adj for cluster size, CR2 RCSE qui reg ynew zcluster nb gen sample = e(sample) qui sum nb if sample == 1 gen mc_nb = nb - r(mean) if sample == 1 gen interaction = zcluster * mc_nb qui reg_sandwich ynew zcluster mc_nb interaction, cluster(cluster) return scalar estC9_est = _b[zcluster] return scalar estC9_se = _se[zcluster] local p = (2 * ttail(e(dfs)[1,1], abs(_b[zcluster]/_se[zcluster]))) return scalar estC9_p = `p&apos; end ** Simulate the performance of these estimators apply_estimators local rscalars : r(scalars) local to_store &quot;&quot; foreach item of local rscalars { local to_store &quot;`to_store&apos; `item&apos; = r(`item&apos;)&quot; } simulate /// `to_store&apos;, /// reps(200): /// apply_estimators ** Create summary matrix qui des, short di (`r(k)&apos; - 1)/3 matrix diagnosands = J((`r(k)&apos; - 1)/3, 6, .) matrix rownames diagnosands = &quot;C0: IID SE&quot; &quot;C1: HC2 SE&quot; &quot;C2: CR1 SE&quot; &quot;C3: CR2 SE&quot; &quot;C6: Cluster Size Weights&quot; &quot;C7: Var Weights&quot; &quot;C8: Cluster size control&quot; &quot;C9: Lin cluster size adjust&quot; matrix colnames diagnosands = &quot;Mean estimand&quot; &quot;Mean estimate&quot; &quot;Bias&quot; &quot;SD Estimate&quot; &quot;Power&quot; &quot;Coverage&quot; ** Get variable name roots to loop through local roots estC0 estC1 estC2 estC3 estC6 estC7 estC8 estC9 ** Calculate quantities to include local row = 0 foreach l of local roots { local ++row * Estimand qui sum ATE, meanonly matrix diagnosands[`row&apos;, 1] = `r(mean)&apos; * Estimate qui sum `l&apos;_est, meanonly matrix diagnosands[`row&apos;, 2] = `r(mean)&apos; * Bias qui gen biascalc = `l&apos;_est - ATE qui sum biascalc, meanonly matrix diagnosands[`row&apos;, 3] = r(mean) drop biascalc * SD estimate (based on population variance, no n-1 in the variance denom.) qui sum `l&apos;_est qui gen sdcalc = (`l&apos;_est - r(mean))^2 qui sum sdcalc matrix diagnosands[`row&apos;, 4] = sqrt(r(sum)/r(N)) drop sdcalc * Power gen rejectnull = `l&apos;_p &lt;= 0.05 qui sum rejectnull, meanonly matrix diagnosands[`row&apos;, 5] = r(mean) drop rejectnull * Coverage (z-stat CIs to limit estimates stored above) qui gen conflow = `l&apos;_est - (1.96 * `l&apos;_se) qui gen confhigh = `l&apos;_est + (1.96 * `l&apos;_se) qui gen inrange = ATE &lt;= confhigh &amp; ATE &gt;= conflow qui sum inrange matrix diagnosands[`row&apos;, 6] = r(mean) drop conf* inrange } * View the results matrix list diagnosands The results of our simulation show how certain approaches can yield very biased estimates, while other approaches can reduce the bias. With variaion in cluster size, the C5 and C6 estimators have the lowest bias in this particular example (with very few clusters). We also see evidence of problems with some of these standard errors &#x2014; the actual standard errors (in &#x201C;SD Estimate&#x201D;) should be much higher than those estimated by the approaches ignoring the clustered design (C0 and C1). Estimator Mean Estimand Mean Estimate Bias SD Estimate Power Coverage C0: Ignores Clusters, IID SE -12900.57 -15166.65 -2266.08 5916.55 1.00 0.22 C1: Ignores Clusters, HC22 SE -12900.57 -15166.65 -2266.08 5916.55 1.00 0.20 C2: OLS, CR1 SE (Stata) -12900.57 -15166.65 -2266.08 5916.55 0.76 0.85 C3: Diff Means, CR2 SE -12900.57 -15166.65 -2266.08 5916.55 0.32 0.94 C4: OLS, CR2 SE -12900.57 -15166.65 -2266.08 5916.55 0.32 0.94 C5: Horvitz-Thompson Cluster, Young SE -12900.57 -12710.34 190.23 5595.09 0.32 0.86 C6: OLS with Cluster Size Weights, CR2 SE -12900.57 -12670.05 230.51 5931.54 0.24 0.92 C7: OLS Clusters with Weights, CR2 RCSE -12900.57 -19047.82 -6147.26 6717.76 0.24 0.92 C8: OLS Clusters with adj for cluster size, CR2 RCSE -12900.57 -19120.43 -6219.86 6709.44 0.26 0.92 C9: OLS Clusters with adj for cluster size, CR2 RCSE -12900.57 61051.64 73952.20 124497.09 0.06 0.96 5.6.2 Incorrect false positive rates from tests and confidence intervals When we have few clusters, analytic standard errors could lead to incorrect false positive rates for our hypothesis tests or confidence intervals, even after we adjust our standard errors for clustering (e.g., using the default adjustment in Stata, CR1). The CR2 errors OES prefers tend to perform slightly better in settings with fewer clusters (Pustejovsky 2019). We demonstrate below using CR2 errors rather than CR1 can sometimes help ensure that the false positive rates of our hypothesis tests is controlled correctly. To distinguish between (1) the problems of bias arising from unequal sized clusters and (2) problems of false positive rates or poor covereage arising from the presence of few clusters, we use a design with equal numbers of units per cluster: buildingID Zcluster 1 2 3 4 5 6 7 8 9 10 C 10 0 10 10 0 0 0 0 10 10 T 0 10 0 0 10 10 10 10 0 0 R code Stata code Hide ## Break any relationship between treatment and outcomes by permuting ## or shuffling the treatment variable. This means that H0, the null, ## of no effects is true. checkFP &lt;- function(dat, setype=&quot;CR2&quot;){ dat$newZ &lt;- cluster_ra(cluster=dat$buildingID) newest &lt;- lm_robust(Y~newZ, dat=dat, clusters=buildingID, se_type=setype) return(nullp = newest$p.value[&quot;newZ&quot;]) } ## Construct a separate dataset for this example smalldat &lt;- dat1[, c(&quot;Y&quot;,&quot;buildingID&quot;)] ## Apply the function above 1000 times (CR2) set.seed(123) fpresCR2 &lt;- replicate(1000, checkFP(dat=smalldat)) ## Apply the function above 1000 times (CR1, Stata) set.seed(123) fpresStata &lt;- replicate(1000, checkFP(dat=smalldat, setype=&quot;stata&quot;)) ## Summarize the results fprateCR205 &lt;- mean(fpresCR2 &lt;= .05) paste0(&quot;fprateCR205 = &quot;, fprateCR205) fprateStata05 &lt;- mean(fpresStata &lt;= .05) paste0(&quot;fprateStata05 = &quot;, fprateStata05) ** Return to data from Chapter 4 import delimited using &quot;dat1_with_designs.csv&quot;, clear ** Break any relationship between treatment and outcomes by permuting ** or shuffling the treatment variable. This means that H0, the null, ** of no effects is true. capture program drop checkFP program define checkFP, rclass ** Specify if you want CR2, rather than CR1 (Stata) syntax[, se_CR2] capture drop newZ cluster_ra newZ, cluster_var(buildingid) if &quot;`se_CR2&apos;&quot; == &quot;&quot; { reg_sandwich y newZ, cluster(buildingid) local p = (2 * ttail(e(dfs)[1,1], abs(_b[newZ]/_se[newZ]))) return scalar nullp = `p&apos; } else { reg y newZ, vce(cluster buildingid) local p = (2 * ttail(e(df_r), abs(_b[newZ]/_se[newZ]))) return scalar nullp = `p&apos; } end ** Apply the function above 1000 times (CR2) preserve set seed 123 simulate /// nullp = r(nullp), /// reps(1000) nodots: /// checkFP, se_CR2 qui gen reject = nullp &lt;= 0.05 qui sum reject di &quot;fprateCR205 `r(mean)&apos;&quot; restore ** Apply the function above 1000 times (Stata) preserve set seed 123 simulate /// nullp = r(nullp), /// reps(1000) nodots: /// checkFP qui gen reject = nullp &lt;= 0.05 qui sum reject di &quot;fprateStata05 `r(mean)&apos;&quot; restore [1] &quot;fprateCR205 = 0.045&quot; [1] &quot;fprateStata05 = 0.054&quot; In this case, with 10 equal sized clusters, a simple outcome, and equal numbers of clusters in treatment and control, we see that the CR2 standard error controls the false positive rate (less than 5% of the 1000 simulations testing a true null hypothesis of no effect return a ppp-value of less than .05) while the default &#x201C;Stata&#x201D; (CR1) standard error has a slightly too high false positive rate of 0.054 at the 5% error level. If a simulation like the one above shows false positive rate problems with the CR2 standard error as well, we may prefer to rely on permutation-based randomization inference. For more discussion of issues with standard error calculation in the presence of few clusters, and examples of some other possible approaches to testing with few clusters, see Esarey and Menger (2019). References "],
["poweranalysis.html", "Chapter 6 Power analysis 6.1 An example of the off-the-shelf approach 6.2 An example of the simulation approach 6.3 When to use which approach 6.4 Additional examples of the simulation approach", " Chapter 6 Power analysis This chapter provides examples of how we assess statistical power for different experimental research designs. We often employ simulation here because we rarely have designs that fit easily into the assumptions made by analytic tools (though when we do, these tools are an invaluable reference). By &#x201C;analytic tools,&#x201D; we refer to methods of calculating power (or a minimum required sample size, etc.) based on mathematical derivations under particular assumptions. For instance, in an i.i.d. two-arm design with random assignment of half the sample, no covariates, a roughly normally distributed outcome, and equal variance in each treatment group, it&#x2019;s possible show that we would have 80% power to estimate a difference in means of &#x394;\\Delta&#x394; if we collect data on approximately n=(5.6&#x3C3;/&#x394;)2n = (5.6 \\sigma /\\Delta)^{2}n=(5.6&#x3C3;/&#x394;)2 observations, where &#x3C3;\\sigma&#x3C3; is the overall standard deviation of the outcome.35 But we frequently consider situations where such derivations are not readily available. 6.1 An example of the off-the-shelf approach To demonstrate how analytical power analysis works in principle, consider the R function power.t.test(). This can be used for power calculations in designs where a two-sample t-test is an appropriate estimation strategy (with no adjustment for blocking, clustering, or covariates). When using this function, there are three parameters that we&#x2019;re most concerned with, two of which must be specified by the user. The third is then calculated and returned by the function. These are: n = sample size, or number of observations per treatment group delta = the target effect size, or a minimum detectable effect (MDE) power = the probability of detecting an effect if in fact there is a true effect of size delta Note that there is also the parameter sd, representing the standard deviation of the outcome. This is set to 1 by default unless power.t.test() is instructed otherwise. Say, for example, you want to know the MDE for a two-arm study with 1,000 participants, of which half are assigned to treatment. Using power.t.test() you would specify: power.t.test( n = 500, # The number of observations per treatment arm power = 0.8 # The traditional power threshold of 80% ) Two-sample t test power calculation n = 500 delta = 0.1774 sd = 1 sig.level = 0.05 power = 0.8 alternative = two.sided NOTE: n is number in *each* group If all we wanted to extract was the MDE, we could instead write: power.t.test(n = 500, power = 0.8)$delta [1] 0.1774 If we request the sample size instead, we can illustrate that this is applying an expression like the one we mention above: n=(5.6/0.1773605)2&#x2248;1000n = (5.6/0.1773605)^{2} \\approx 1000n=(5.6/0.1773605)2&#x2248;1000 And now via R code: power.t.test(delta = 0.1773605, power = 0.8)$n * 2 [1] 1000 If you need to, you can adjust other parameters, like the standard deviation of the outcome, the level of the test, or whether the test is one-sided rather than two-sided. There are also other functions available for different types of outcomes. For example, if you have a binary response, you can use power.prop.test() to calculate power for a similar kind of simple difference in proportions test. An equivalent approach in Stata would be as follows: power twomeans 0, power(0.8) n(1000) sd(1) di r(delta) // See: return list Stata users can learn more about available off-the-shelf tools by checking out Stata&#x2019;s plethora of relevant help files. Meanwhile, R users could start by consulting the pwrss&#x2019;s packages guide. 6.2 An example of the simulation approach We can compare the output of power.t.test() to the output from a simulation-based (i.e., computational) approach, which we illustrate below. Our example relies on manually-written functions that could be copied and adapted by OES team members to the needs of different projects. This code is more detailed than we need for simple power analysis problems, but it provides useful flexibility for simulating some more complicated designs. R code Stata code Hide ## OES Power Simulation Toolkit (R): ## ## replicate_design(...) ---: Generate replicates of a simulated dataset ## estimate(...) -----------: Estimate the null test-stat for treatment(s). ## evaluate_power(...) -----: Evaluate power to detect non-zero effects. ## evaluate_mde(...) -------: Find MDE, searching over range of effect sizes. ## evaluate_bias(...) ------: Compute bias and other diagnostics. ## Required packages require(magrittr) require(fabricatr) require(foreach) ##### REPLICATE a hypothetical design R times ##### # Inputs: # - (1) number of replicates desired # - (2) additional arguments that are passed to fabricate() (see the ...). # - Generally, each argument is a separate variable to generate. # - Later vars can be a function of earlier ones. # - See our examples below, this is often simpler than it sounds! # - A built-in simulated treatment effect is not generally needed. replicate_design &lt;- function(R = 200, ...) { # Function: produce one draw of the simulated dataset/design design &lt;- function() { fabricatr::fabricate( ... ) %&gt;% list } # Use replicate() to replicate that design R times rep &lt;- replicate( n = R, expr = design() ) # Output will be a list of dataframes. # For each, add a variable indicating which sim# it is for(i in 1:length(rep)) { rep[[i]] &lt;- rep[[i]] %&gt;% dplyr::mutate( sim = i ) } return(rep) } ##### ESTIMATE results using those replicated dfs ##### # Inputs: # - (1) Estimation formula (y ~ x1 + x2 + ...) # - (2) Variables(s) we want to be powered to estimate the effects of # - Generally just the treatment var(s) # - (3) Data the estimator should be applied to (list of dfs) # - (4) The estimator (the default is OLS with HC2 errors) estimate &lt;- function( formula, vars, data = NULL, estimator = estimatr::lm_robust ) { # Pass the list of dfs to map(). # map() applies the procedure specified below to each df in the list. data %&gt;% purrr::map( # For each dataframe in the list, apply the specified estimator, # using the specified formula. ~ estimator( formula, data = . ) %&gt;% # tidy up the results and specify the rows of estimates to keep estimatr::tidy() %&gt;% dplyr::filter( .data$term %in% vars ) ) %&gt;% # Append the results from each sim replicate into a single dataframe dplyr::bind_rows() %&gt;% # Add some more useful labels dplyr::mutate( sim = rep(1:length(data), each = n() / length(data)), term = factor(.data$term, levels = vars) ) } ##### EVALUATE power of the design ##### # Inputs: # - (1) Results produced by estimate() above # - (2) Hypothetical effects we want power estimates for # - (3) Desired alpha (significance) level evaluate_power &lt;- function(data, delta, level = 0.05) { # Make sure delta (may be scalar or vector) was specified if (missing(delta)) { stop(&quot;Specify &apos;delta&apos; to proceed.&quot;) } # Apply the following (i.e., after %do%) to each delta separately, # appending the results with bind_rows at the end. foreach::foreach( i = 1:length(delta), .combine = &quot;bind_rows&quot; ) %do% { # Start with the df of estimates data %&gt;% # Create variables storing the relevant delta and new test stat dplyr::mutate( delta = delta[i], new_statistic = (.data$estimate + .data$delta) / .data$std.error ) %&gt;% # Similar to group_by, result here is list of dfs for each term dplyr::group_split(.data$term) %&gt;% # Separately for the df for each term, get p for each replicate purrr::map( ~ { tibble::tibble( term = .$term, delta = .$delta, p.value = foreach( j = 1:length(.$new_statistic), .combine = &quot;c&quot; ) %do% mean(abs(.$statistic) &gt;= abs(.$new_statistic[j])) ) } ) } %&gt;% # Organize by term and delta group_by(.data$term, .data$delta) %&gt;% # Average over repliacates to get power for each term/delta combination summarize( power = mean(.data$p.value &lt;= level), .groups = &quot;drop&quot; ) } ##### EVALUATE the min. detectable effect ##### # Helps summarize the results of evaluate_power() above, # basically a wrapper for evaluate_power() # Inputs: # - (1) Results produced by estimate() above # - (2) Range of hypothetical effects we want to consider (delta above) # - (3) How fine-grained do we want changes in delta to be? # - (4) Alpha (significance) level # - (5) Minimum power we want to accept evaluate_mde &lt;- function( data, delta_range = c(0, 1), how_granular = 0.01, level = 0.05, min_power = 0.8 ) { # Use the function designed above to get power estimates eval &lt;- evaluate_power( data = data, delta = seq(delta_range[1], delta_range[2], how_granular), level = level ) %&gt;% # Organize data by term dplyr::group_by( .data$term ) %&gt;% # Get the MDE at our desired power level for each term dplyr::summarize( MDE = min(.data$delta[.data$power &gt;= min_power]), .groups = &quot;drop&quot; ) return(eval) } ##### EVALUATE Bias ##### # Helps summarize the results of estimate() above. # Pass results of estimate() to this function. # Inputs: # - (1) Data produced by estimate() above # - (2) True parameter value (generally true ATE) evaluate_bias &lt;- function( data, ATE = 0 ) { # Start with the estimates for each replicated dataset smry &lt;- data %&gt;% # Add a variable representing the true ATE dplyr::mutate( ATE = rep(ATE, len = n()) ) %&gt;% # Organize estimates by term dplyr::group_by( .data$term ) %&gt;% # Summarize across replicates, within each term dplyr::summarize( &quot;True ATE&quot; = unique(.data$ATE), &quot;Mean Estimate&quot; = mean(.data$estimate), Bias = mean(.data$estimate - .data$ATE), MSE = mean((.data$estimate - .data$ATE)^2), Coverage = mean( .data$conf.low &lt;= .data$ATE &amp; .data$conf.high &gt;= .data$ATE ), &quot;SD of Estimates&quot; = sd(.data$estimate), &quot;Mean SE&quot; = mean(.data$std.error), .groups = &quot;drop&quot; ) return(smry) } ** OES Power Simulation Toolkit (Stata): ** ** draw_from_design ---: Generate a simulated dataset (NOT RUN DIRECTLY) ** single_estimator ---: Draw data once and estimate results (NOT RUN DIRECTLY) ** replicate ----------: Repeat (generate -&gt; estimate) many times ** evaluate_power -----: Evaluate power to detect non-zero effects. ** evaluate_mde -------: Find MDE, searching over range of effect sizes. ** evaluate_bias ------: Compute bias and other diagnostics. ***** DRAW FROM a hypothetical design ***** ** Note: Must be modified by user * Required set-up: * - (1) Write code within this program to generate one draw of a simulated dataset. * - See our examples below, this is often simpler than it sounds! * - A built-in simulated treatment effect is not generally needed. capture program drop draw_from_design program define draw_from_design, nclass * Clear existing data clear ** Replace the rest of the code inside this program with your own code * Sample size of 1000 observations set obs 1000 * Generate simulated outcome gen y = rnormal(0, 1) * Generate simulated treatment (complete random assignment) qui count local ntreat = r(N)/2 complete_ra x, m(`ntreat&apos;) end **** ESTIMATE results for a single simulated dataset **** ** Note: Must be modified by user * Required set-up: * - (1) Define the data generationprogram above. * - (2) Write out the test you want to run in this program (using the simulated data). capture program drop single_estimator program define single_estimator, rclass * Check that design program exists quietly capture draw_from_design if _rc != 0 { di as error &quot;Error: define data generation program (draw_from_design) first&quot; exit } * Call the design program draw_from_design * Apply the desired estimation strategy * (Code below, as written, will ignore any variable specified with factor * notation; i.x. You may need to tweak the next program more if you need * power estimates for terms that have to be factorials. This behavior is * intended, as an easy way to silently ignore fixed effects.) reg y x, vce(hc2) end **** REPEAT (generation -&gt; estimation) many times **** ** Note: Modification by user NOT NEEDED (just copy into your .do file) * Required set-up: * - (1) Define both programs above (data generation and a single_estimator) * Inputs on use: * - (1) Number of replicates (default = 200) * - (2) Variables(s) we want to be powered to estimate the effects of * - Generally just the treatment var(s) * - Specify like a normal varlist capture program drop replicate program define replicate, rclass syntax[, reps(integer 200) vars(string) ] * Check that design program exists quietly capture draw_from_design if _rc != 0 { di as error &quot;Error: define data generation program (draw_from_design) first&quot; exit } * Check that single_estimator program exists quietly capture single_estimator if _rc != 0 { di as error &quot;Error: define estimation program (single_estimator) first&quot; exit } * Save coefficients and SEs from each draw to memory. simulate /// _b _se, /// reps(`reps&apos;) nodots: /// single_estimator * (Optional): subset to specified explanatory variables. local updatenames foreach var of local vars { local updatenames `updatenames&apos; _b_`var&apos; _se_`var&apos; } capture confirm variable `updatenames&apos; if _rc == 0 { keep `updatenames&apos; } * Either way, keep only needed vars (e.g.: drop coefs/ses for factorial terms) else { keep _b_* _se_* } * Simulation indicator gen sim = _n * Modify var names of coefficients/SEs slightly foreach var of varlist _b* _se* { qui rename `var&apos; `=substr(&quot;`var&apos;&quot;, 2, .)&apos; } * Reshape to a format that makes the desired power calculation easier. qui reshape long b_ se_, i(sim) j(term) string end **** EVALUATE power of the design **** ** Note: Modification by user NOT NEEDED (just copy into your .do file) * Required setup: * - (1) Define all programs above * - (2) Run replicate to get simulated coef/SE estimates in memory. * Inputs on use: * - (1) Hypothetical effects we want power estimates for (min, steps, and max) * - (Default: from 0 to 1 in steps of 0.01) * - (2) Desired alpha (significance) level (default = 0.05) capture program drop evaluate_power program define evaluate_power, nclass syntax[, /// delta_min(real 0) /// delta_steps(real 0.01) /// delta_max(real 1) /// alpha(real 0.05) ] * Data to return to after each iteration tempfile restore_dat qui save `restore_dat&apos;, replace * Loop over specified effect sizes local i = 0 forvalues n = `delta_min&apos;(`delta_steps&apos;)`delta_max&apos; { qui use `restore_dat&apos;, clear local ++i * Real and simulated statistics for each a given effect size gen delta = `n&apos; gen real_t = b_/se_ gen sim_t = (b_ + delta)/se_ * Generate a p-value for each value of sim_t qui gen sim_p = . qui count forvalues v = 1/`r(N)&apos; { // Loop over observations qui gen greaterequal = abs(real_t) &gt;= abs(sim_t[`v&apos;]) qui sum greaterequal if term == term[`v&apos;], meanonly qui replace sim_p = r(mean) if _n == `v&apos; qui drop greaterequal } * Use these to get power for the given delta qui gen reject = sim_p &lt;= `alpha&apos; bysort term: egen power = mean(reject) collapse (mean) power, by(term delta) label var power &quot;&quot; * Save, and advance to the next delta if `i&apos; == 1 { qui tempfile running_dat qui save `running_dat&apos;, replace } else { append using `running_dat&apos; qui save `running_dat&apos;, replace } } * Open the result, replacing the data in memory qui use `running_dat&apos;, clear end **** EVALUATE the min. detectable effect **** ** Note: Modification by user NOT NEEDED (just copy into your .do file) * Required setup: * - (1) Define programs above * - (2) Run replicate and evaluate_power * Inputs on use: * - (1) Minimum power desired capture program drop evaluate_mde program define evaluate_mde, nclass syntax[, min_power(real 0.8)] quietly { bysort term (power): gen above_min = power &gt;= `min_power&apos; drop if above_min == 0 bysort term (delta): gen min = _n == 1 drop if min == 0 drop min above_min } end **** EVALUATE Bias for a particular term **** ** Note: Modification by user NOT NEEDED (just copy into your .do file) * Required setup: * - (1) Define programs above * - (2) Run replicate * Inputs on use: * - (1) The name of the term to provide diagnosics for * - (2) True parameter value (generally true ATE) capture program drop evaluate_bias program define evaluate_bias, nclass syntax, true_value(real) term(string) [restore_data] * Save data to return to in temporary file * (program includes option to turn this off) if &quot;`restore_data&apos;&quot; != &quot;&quot; { tempfile restore qui save `restore_data&apos;, replace } * Subset to only the term in question qui keep if term == &quot;`term&apos;&quot; * True parameter as variable qui gen true_value = `true_value&apos; * Prepare variables to summarizetrue qui gen bias = b_ - true_value qui gen MSE = (b_ - true_value)^2 qui gen conflow = b_ - (1.96 * se_) // normal approximation qui gen confhigh = b_ + (1.96 * se_) // normal approximation qui gen coverage = true_value &gt;= conflow &amp; true_value &lt;= confhigh collapse /// (first) True = true_value /// (mean) Mean_Estimate = b_ Bias = bias MSE Coverage = coverage Mean_SE = se_ /// (sd) SD_Estimate = b_, /// by(term) list * Return to data? if &quot;`restore_data&apos;&quot; != &quot;&quot; { qui use `restore_data&apos;, clear } end Results are shown in the subsequent figure. Though the computational estimates are slightly different, they comport quite well with the analytic estimates. R code Stata code Hide ## Parameters used for both sets of calculations n &lt;- 1000 # Sample size d &lt;- 0.2 # Effect size to consider ## Analytical power estimates power_data &lt;- tibble( d = seq(0, 0.5, len = 200), power = power.t.test(n = n / 2, delta = d)$power ) ## Save initial plot; add simulation results below g &lt;- ggplot(power_data) + geom_line(aes(d, power, linetype = &quot;power.t.test()&quot;)) + labs( x = expression(delta), y = &quot;Power&quot;, title = &quot;Power for Simple Difference in Means Test&quot; ) + scale_y_continuous( n.breaks = 6 ) + geom_hline( yintercept = 0.8, col = &quot;grey25&quot;, alpha = 08 ) + ggridges::theme_ridges( center_axis_labels = TRUE, font_size = 10 ) ## Comutational power estimates, using the functions above sim_power_data &lt;- replicate_design( N = n, y = rnorm(N), x = randomizr::complete_ra( N, m = N / 2 ) ) %&gt;% estimate( form = y ~ x, vars = &quot;x&quot; ) %&gt;% evaluate_power( delta = seq(0, 0.5, len = 200) ) ## Add results from the simulation to the plot and compare g + geom_line( data = sim_power_data, aes(delta, power, linetype = &quot;simulation&quot;), color = &quot;grey25&quot; ) + labs( linetype = &quot;Method:&quot; ) + theme( legend.position = &quot;bottom&quot; ) ** Analytical power estimates * View power estimates for a range of effect sizes as a table power twomeans 0, n(1000) diff(0.005(0.005)0.505) * Or view as a graph instead power twomeans 0, n(1000) diff(0.005(0.005)0.505) graph * It&apos;s also possible to get the estimates as data in memory * and write your own plotting code (e.g.: using twoway). clear svmat r(pss_table), names(col) list in 1/5 // Illustration: power estimates are data in memory keep diff power rename diff delta tempfile analytical save `analytical&apos;, replace ** Computational power estimates, using the programs as defined above replicate, reps(200) // Replicate data generation and estimation 200 times evaluate_power, delta_min(0.005) delta_max(0.500) delta_steps(0.005) // Consider a range of effect sizes ** Merge computational with analytic estimates rename power power_comp keep if term == &quot;x&quot; merge 1:1 delta using `analytical&apos; keep if _merge == 3 drop _merge ** Manual line plot label var power &quot;Analytical&quot; label var power_comp &quot;Computational&quot; twoway /// (line power delta) /// (line power_comp delta), /// legend(pos(6) rows(1)) /// xtitle(&quot;Effect size&quot;) ytitle(&quot;Power&quot;) /// yline(0.8) title(&quot;Power for Simple Difference in Means Test&quot;) As mentioned above, we produced those computational estimates using some pre-written functions laid step-by-step in the code chunk above. These tools are designed around a simple workflow, and they should help remove some of the programming that may otherwise be a barrier to project teams calculating power computationally. The workflow proceeds as follows (we&#x2019;ll focus on explaining the R code here in text): Replicate Estimate Evaluate The first step, Replicate, entails specifying an example data-generation process (which may include only an outcome variable and treatment assignment) and simulating it multiple times to create a series of randomly generated datasets. Each individual dataset produced is a sample replicate. The next step, Estimate, entails estimating treatment effects within each sample replicate. We can use those estimates to produce a distribution of test statistics for each effect size (i.e., hypothetical true treatment effect) of interest. Finally, the last step, Evaluate, entails using those test statistics to evaluate our power to detect a range of different possible true effect sizes. This workflow is supported by three functions: replicate_design(), estimate(), and evaluate_power(). Here&#x2019;s the simulation code used to generate Figure 1 in R in more detail (alongside a similar illustration of the Stata version of our power simulation toolkit): R code Stata code Hide ## 1. Replicate: # Output is a list of dfs rep &lt;- replicate_design( R = 200, # Number of sample replicates N = 1000, # Sample size of each replicate y = rnorm(N), # Normally distributed response x = rbinom(N, 1, 0.5) # Binary treatment indicator ) ## 2. Estimate: # Output is a dataframe of estimates from each sample replicate est &lt;- estimate( y ~ x, # Regression formula vars = &quot;x&quot;, # Treatment variable(s) data = rep # Sample replicates ) ## 3. Evaluate: # Output is a list of dfs pwr_eval_sim &lt;- evaluate_power( data = est, # Estimates, from estimate() above delta = seq(0, 0.5, len = 200) # Effect sizes to consider ) ** 0. Setup * 0a: Simulate (update program below as needed) * Output is a dataset in memory capture program drop draw_from_design program define draw_from_design, nclass * Clear existing data clear * Sample size of 1000 observations set obs 1000 * Generate simulated outcome gen y = rnormal(0, 1) * Generate simulated treatment (complete random assignment) qui count local ntreat = r(N)/2 complete_ra x, m(`ntreat&apos;) end * 0b: Analysis strategy (update program below as needed) * Output is a dataset in memory and stored estimates capture program drop single_estimator program define single_estimator, rclass * Call the design program draw_from_design * Write out the desired estimation strategy reg y x, vce(hc2) end ** 1/2. Replicate/Estimate: * Output is a dataset of coefficients and SEs from each simulation. replicate, reps(200) // Number of replications ** 3. Evaluate * Output is a set of power estimates in memory, one for each delta evaluate_power, /// delta_min(0.005) /// Smallest delta to consider delta_max(0.500) /// Largest delta to consider delta_steps(0.005) // Increments to apply The final product&#x2014;pwr_eval_sim above in the R code&#x2014;reports the power for each of the user-specified effect sizes (delta) and model terms (vars) specified when calling estimate(). The output can be used to plot power curves or to compute minimum detectable effects. These functions help make the process of performing computational power analysis easier, while still providing ample room for flexibility in both design and estimation strategy. For example, replicate_design() in the R code is a wrapper for fabricate() in the fabricatr package. This gives users the ability to generate multi-level or nested data-generating processes, specify additional covariates, or determine whether treatment randomization is done within blocks or by clusters. By default, estimates in R are returned using lm_robust() from the estimatr package, but alternative estimators can be specified through further modifications to the code. Say, for example, you have a binary response and a set of covariates, and your design calls for using logistic regression. You could generate estimates for such a design as follows: ## Define logit estimator function logit &lt;- function(...){ glm(..., family = binomial)} ## Pass this to the estimate() function above est &lt;- estimate( y ~ x + z1 + z2, data = rep, estimator = logit ) Other tools for power simulation exist as well. For instance, throughout this SOP, we have used DeclareDesign to simulate hypothetical research designs and compare their performance. And there is no shortage of further simulation examples that can be found online (or in our internal records of past OES project code) for more specialized use-cases. 6.3 When to use which approach For a simple difference in means test, the programming required for an analytical power analysis is much much less involved. In cases where we&#x2019;re interested in the power to detect a simple difference in means, or a difference in proportions for binary responses, it is probably sufficient to use power.t.test() (for means) or power.prop.test() (for proportions). However, OES projects often involve design features or analytic strategies that are difficult to account for using off-the-shelf tools. For example, we often include covariates in our statistical models to enhance the precision of our treatment effect estimates. If the gain in precision is small, then it might not be important to account for this in power calculations. But if we expect a substantial gain in precision due to including covariates, then we probably want to account for this when estimating power. The natural way to do this is by simulation, including the covariates in the &#x201C;replicate&#x201D; and &#x201C;estimate&#x201D; steps above. More complex design features or analytic strategies may make investing in the simulation approach even more worthwhile, or downright necessary. Examples include heterogeneity in treatment effects, a multi-arm or factorial design, or block randomization with differing probabilities of treatment between blocks &#x2013; none of which is usually easily accounted for with off-the-shelf tools. In the next section, we provide some additional examples of simulations for more complex designs or analytic strategies. 6.4 Additional examples of the simulation approach Here we provide two examples of research designs where simulation is well worth the extra effort. Attendant R code is included to illustrate how we could use the functions above in these cases. 6.4.1 A two-by-two design with interaction One instance where computational power analysis may be worth the investment is in assessing power for a two-by-two factorial design with an interaction. In such a design, the goal is to assess not only the power to detect main effects (the average effect of each individual treatment), but also power to detect a non-zero interaction effect between the treatments. Say we have a design with 1,000 observations and we would like to know the effect of two treatments on a binary outcome with a baseline of 0.25. Each treatment is assigned to M=500M = 500M=500 individuals at random, resulting in four roughly equal sized groups of observations after randomization: (1) a control group, (2) those assigned to treatment 1 but not treatment 2, (3) those assigned to treatment 2 but not treatment 1, and (4) those assigned to both treatment 1 and 2. We can easily calculate power to detect the main effect of each treatment as follows: R code Stata code Hide two_by_two &lt;- ## Basic elements of each simulated sample replicate replicate_design( N = 1000, y = rbinom(N, 1, 0.25), x1 = complete_ra(N, m = N / 2), x2 = complete_ra(N, m = N / 2) ) %&gt;% ## Estimate main and interaction effects estimate( form = y ~ x1 + x2 + x1:x2, vars = c(&quot;x1&quot;, &quot;x2&quot;, &quot;x1:x2&quot;) ) %&gt;% ## Evaluate power evaluate_power( delta = seq(0, 0.25, len = 200) ) ** Basic elements of each simulated sample replicate * Redefine data generation capture program drop draw_from_design program define draw_from_design, nclass * Clear existing data clear * Sample size of 1000 observations set obs 1000 * Generate simulated outcome gen y = rbinomial(1, 0.25) * Generate simulated treatments qui count local ntreat = r(N)/2 complete_ra x, m(`ntreat&apos;) complete_ra x2, m(`ntreat&apos;) end ** Estimate main and interaction effects * Redefine estimation capture program drop single_estimator program define single_estimator, rclass * Call the design program draw_from_design * Write out the desired estimation strategy * (note: the program currently does not correctly handle factor notation) gen x_int = x*x2 reg y x x2 x_int, vce(hc2) end ** Replicate estimates replicate, reps(200) ** Evaluate power evaluate_power, delta_min(0) delta_max(0.25) delta_steps(0.002) Using the output reported in the object two_by_two, we can plot the power curves for each of the main effects and the interaction effect, as shown in Figure 2. R code Stata code Hide ggplot(two_by_two) + ## Add a line representing power for each effect/term geom_line( aes(delta, power, linetype = term) ) + ## Choose linetypes that are easy to distinguish scale_linetype_manual(values = c(&quot;solid&quot;, &quot;longdash&quot;, &quot;dotted&quot;)) + ## Horizontal line for 80% power geom_hline( yintercept = 0.8, color = &quot;grey25&quot;, alpha = 0.8 ) + ## y-axis scale scale_y_continuous( n.breaks = 6 ) + ## Adding labels labs( x = expression(delta), y = &quot;Power&quot;, title = &quot;Power for a 2x2 Design&quot;, linetype = &quot;Effect for...&quot; ) + ## Update some visual settings with the ridges theme ggridges::theme_ridges( font_size = 10, center_axis_labels = TRUE ) + ## Other settings (here: just legend location) theme( legend.position = &quot;bottom&quot; ) ** Reshape to apply plotting code similar to above reshape wide power, i(delta) j(term) string ** Line plot label var powerx &quot;x1&quot; label var powerx2 &quot;x2&quot; label var powerx_int &quot;x1 * x2&quot; twoway /// (line powerx delta, lcolor(black) lpattern(solid)) /// x line (line powerx2 delta, lcolor(black) lpattern(dash)) /// x2 line (line powerx_int delta, lcolor(black) lpattern(dot)), /// interaction line legend(pos(6) rows(1)) /// xtitle(&quot;Effect size&quot;) ytitle(&quot;Power&quot;) /// yline(0.8) title(&quot;Power for a 2x2 Design&quot;) Of course, in this simple example, we could still have relied on some reasonable analytical assumptions to arrive at these estimates (see a helpful discussion here). But running a simulation saves us the trouble. 6.4.2 Covariate adjustment with the Lin estimator Another scenario where computational power analysis is worth the investment is if a design calls for covariate adjustment. This is common in OES projects, and, in many instances, the Lin (2013) saturated regression estimator is the solution we choose. Employing an off-the-shelf method here is possible, but would likely require investing time doing background research. Alternatively, we could simply replicate, estimate, and evaluate such a design computationally. The results will be roughly just as accurate, without requiring a review of the methods literature. Suppose we have a sample of 100 observations and a continuous outcome variable. We wish to assess the effect of some policy intervention on this continuous outcome. Our design calls for randomly assigning only M=25M = 25M=25 individuals to receive the intervention&#x2014;perhaps because it is very expensive to implement&#x2014;and the rest to control. In addition to having data on the outcome and on treatment assignment, let&#x2019;s say that we also anticipate obtaining covariates. These data contain two variables we suspect to be associated with treatment effect heterogeneity. We&#x2019;ll call these z1 and z2. The first is a continuous measure and the latter is a binary indicator. We&#x2019;re considering adjusting for these covariates to improve the precision of our estimated average treatment effect. We can simulate such a design to illustrate the possible benefits of different approaches to covariate adjustment in terms of improved statistical power. In the other examples in this chapter, we simulate a distribution of 200 test statistics under a true null, and then estimate power by performing some calculations on them. When feasible, this approach speeds up power simulation. But it&#x2019;s not as easy to rely on that approach (in a straightforward way) when we want to build in treatment effect heterogeneity. We illustrate a different approach to power simulation in the following code chunk, where we build different average effect sizes (and effect heterogeneity) into the simulated data itself. In short, we simulate 200 datasets for each of a sequence of possible effect sizes (200 &#xD7;\\times&#xD7; 48 = 9,600 total), and then estimate power among the 200 replicates for each effect size. Coding the simulation this way gives us even more control, though it often does take longer to run. R code Stata code Hide ## Simulate a single dataset with the given avg effect size ## and heterogenous effects (fixed het effect for z1, ## but proportional het effect for z2) sim_given_delta &lt;- function(delta) { fabricate( N = 100, # Sample size of 100 observations delta = delta, z1 = rnorm(N, mean = 0, sd = 3), # Continuous covariate z2 = rbinom(N, 1, 0.25), # Binary covariate cz1 = z1 - mean(z1), # Mean center z1 cz2 = z2 - mean(z2), # Mean center z2 x = complete_ra(N, m = N * 0.25), # 25% treatment error = rnorm(N, 0, 1.75), y_0 = z2*0.5 + z1*0.5 + error, # Control potential outcome effect = delta + z2*delta*2 + z1*1.5, # Effect y_1 = y_0 + effect, # Treated potential outcome y = x*y_1 + (1-x)*y_0 # Observed ) } ## Repeat data generation for a given delta reps times. repeat_sim_delta &lt;- function(delta, reps = 200) { lapply(1:reps, function(.x) sim_given_delta(delta)) } ## Apply the three estimators to the list of datasets. ## Still for only a single given delta. ## Estimate() above could work here, but ## adds some processing time overhead ## we can avoid by rewriting estimation. get_estimator_pvalues &lt;- function(delta, reps = 200) { # Get list of datasets, one for each delta simdat &lt;- repeat_sim_delta(delta, reps) # Lin adjustment # (HC1 errors to sidestep estimation issues for this e.g.) lin_f &lt;- y ~ x + cz1 + cz2 + x:cz1 + x:cz2 lin &lt;- simdat %&gt;% lapply(function(.x) lm(lin_f, data = .x)) %&gt;% lapply(function(.x) coeftest(.x, vcovHC(.x, &quot;HC1&quot;))[&quot;x&quot;,&quot;Pr(&gt;|t|)&quot;]) # Standard linear, additive adjustment stand_f &lt;- y ~ x + z1 + z2 standard &lt;- simdat %&gt;% lapply(function(.x) lm(stand_f, data = .x)) %&gt;% lapply(function(.x) coeftest(.x, vcovHC(.x, &quot;HC1&quot;))[&quot;x&quot;,&quot;Pr(&gt;|t|)&quot;]) # No adjustment no_f &lt;- y ~ x none &lt;- simdat %&gt;% lapply(function(.x) lm(no_f, data = .x)) %&gt;% lapply(function(.x) coeftest(.x, vcovHC(.x, &quot;HC1&quot;))[&quot;x&quot;,&quot;Pr(&gt;|t|)&quot;]) # Prepare output out &lt;- data.frame( delta = delta, lin = do.call(c, lin), additive = do.call(c, standard), none = do.call(c, none) ) return(out) } ## Function to repeat that process and estimate ## power across multiple possible effect sizes. across_deltas &lt;- function(deltas, reps = 200) { res &lt;- lapply( deltas, function(.d) { pvals &lt;- get_estimator_pvalues(.d, reps) pvals[,-1] &lt;- pvals[,-1] &lt;= 0.05 return(colMeans(pvals)) } ) res &lt;- do.call(rbind, res) return(res) } ## Run the sim cov_adjust_sim &lt;- across_deltas(seq(0.025, 1.2, 0.025), reps = 200) ** Simulate a single dataset with the given avg effect size ** and heterogenous effects (fixed het effect for z1, ** but proportional het effect for z2) capture program drop draw_from_design program define draw_from_design, nclass syntax, delta(real) // required argument, specify effect size * Clear existing data clear * Sample size of 100 observations set obs 100 * Continuous covariate gen z1 = rnormal(0, 3) * Binary covariate gen z2 = rbinomial(1, 0.25) * Mean centered versions qui sum z1 gen cz1 = z1 - r(mean) qui sum z2 gen cz2 = z2 - r(mean) * Generate simulated treatment (25%) complete_ra x, prob(0.25) * Simulate observed y gen error = rnormal(0, 1.75) gen y_0 = z2*0.5 + z1*0.5 + error // Control potential outcome gen effect = `delta&apos; + z2*`delta&apos;*2 + z1*1.5 // Effect gen y_1 = y_0 + effect // Treated potential outcome gen y = x*y_1 + (1-x)*y_0 // Observed outcome end ** Program to avoid some typing when specifying what we want from simulate capture program drop simlist program define simlist local rscalars : r(scalars) global sim_targets &quot;&quot; // must be a global foreach item of local rscalars { global sim_targets &quot;$sim_targets `item&apos; = r(`item&apos;)&quot; } end ** Apply the three estimators to the list of datasets, ** using the estimate() function defined above. ** Still for only a single given delta. capture program drop single_estimator program define single_estimator, rclass syntax, delta(real) // required argument, specify effect size * Check that design program exists quietly capture draw_from_design, delta(`delta&apos;) if _rc != 0 { di as error &quot;Error: define data generation program (draw_from_design) first&quot; exit } * Call the design program draw_from_design, delta(`delta&apos;) * Apply the desired estimation strategies, save p-values * (output will appear in &quot;return list&quot;); * normal robust errors to sidestep estimation issues in this e.g. qui reg y c.x##c.cz1 c.x#c.cz2 cz2, r return scalar p_lin = r(table)[&quot;pvalue&quot;,&quot;x&quot;] qui reg y x z1 z2, r return scalar p_stand = r(table)[&quot;pvalue&quot;,&quot;x&quot;] qui reg y x, r return scalar p_none = r(table)[&quot;pvalue&quot;,&quot;x&quot;] return scalar delta = `delta&apos; end ** Replicate p-value draws reps (default 200) times capture program drop replicate program define replicate, rclass syntax, delta(real) /// required argument [ reps(integer 200) ] // optional argument, with a default * Check that design program exists quietly capture draw_from_design, delta(`delta&apos;) if _rc != 0 { di as error &quot;Error: define data generation program (draw_from_design) first&quot; exit } * Check that single_estimator program exists quietly capture single_estimator, delta(`delta&apos;) if _rc != 0 { di as error &quot;Error: define estimation program (single_estimator) first&quot; exit } * Save coefficients and SEs from each draw to memory. qui single_estimator, delta(`delta&apos;) simlist // pull scalar names returned by single_estimator, save as macro $sim_targets simulate /// $sim_targets, /// reps(`reps&apos;) nodots: /// single_estimator, delta(`delta&apos;) gen sim = _n end ** Function to repeat that process and estimate ** power across multiple possible effect sizes. capture program drop across_deltas program define across_deltas syntax, deltas(numlist) /// required argument [ reps(integer 200) ] // optional argument, with a default * Loop across the specified effect sizes local i = 0 foreach d of numlist `deltas&apos; { local ++i * Run replicate for a given delta qui replicate, delta(`d&apos;) reps(`reps&apos;) * Get power for each estimator, est across reps foreach var of varlist p_* { qui replace `var&apos; = `var&apos; &lt;= 0.05 } qui drop sim collapse (mean) * * If first iteration, init temp file to store results. * Otherwise, append to that running tempfile if (`i&apos; == 1) { qui tempfile results qui save `results&apos;, replace } else { append using `results&apos; qui save `results&apos;, replace } } end ** Run the sim. across_deltas, deltas(0.025(0.025)1.2) This simulation yields power estimates across a range of effect sizes for three different estimators: (1) covariate adjustment via the Lin estimator; (2) without covariate adjustment; and (3) with standard linear, additive covariate adjustment. Figure 3 shows the power curves for each. R code Stata code Hide ## Apply similar plotting code to above cov_adjust_sim %&gt;% as.data.frame() %&gt;% pivot_longer( cols = c(&quot;lin&quot;, &quot;additive&quot;, &quot;none&quot;), names_to = &quot;method&quot;, values_to = &quot;power&quot; ) %&gt;% mutate( Method = ifelse(method == &quot;lin&quot;, &quot;Lin&quot;, method), Method = ifelse(method == &quot;additive&quot;, &quot;Additive&quot;, Method), Method = ifelse(method == &quot;none&quot;, &quot;No covariates&quot;, Method) ) %&gt;% ggplot() + geom_line( aes(delta, power, linetype = Method) ) + scale_linetype_manual(values = c(&quot;solid&quot;, &quot;longdash&quot;, &quot;dotted&quot;)) + geom_hline( yintercept = 0.8, color = &quot;grey25&quot;, alpha = 0.8 ) + scale_y_continuous( n.breaks = 6, limits = c(0, 1) ) + labs( x = expression(delta), y = &quot;Power&quot;, title = &quot;Power with Lin Adjustment&quot;, linetype = &quot;Method:&quot; ) + ggridges::theme_ridges( font_size = 10, center_axis_labels = TRUE ) + theme( legend.position = &quot;bottom&quot; ) ** Apply similar plotting code to above label var p_stand &quot;Additive&quot; label var p_none &quot;No covariates&quot; label var p_lin &quot;Lin&quot; twoway /// (line p_stand delta, lcolor(black) lpattern(solid)) /// (line p_lin delta, lcolor(black) lpattern(dash)) /// (line p_none delta, lcolor(black) lpattern(dot)), /// legend(pos(6) rows(1)) /// xtitle(&quot;Effect size&quot;) ytitle(&quot;Power&quot;) /// title(&quot;Power with Lin adjustment&quot;) yline(0.8) We chose parameters to limit processing time, so the results are a bit noisy (you could reduce this by adding replications for each effect size). Nonetheless, for this data generating process, it&#x2019;s clear that the Lin estimator provides substantial improvements in power over the unadjusted difference in means, while standard covariate adjustment provides smaller improvements in power. This is, of course, true by design. In real projects, the differences between these covariate adjustment strategies may be negligible. But it is useful to remember that Lin (2013) adjustment is more likely to be valuable with imbalanced treatment assignment and substantial effect heterogeneity. See our discussion in the Analysis Choices chapter. 6.4.3 Incorporating DeclareDesign into OES Power Tools In R, we can also use DeclareDesign within this Replicate, Estimate, Evaluate framework. This involves using DeclareDesign to draw estimates, and then feeding the results into the OES evaluate_power() function. We compare the DeclareDesign approach to the OES Replicate and Estimate steps below. First, we simulate a simple design with the OES tools introduced above: eval &lt;- replicate_design( R = 1000, N = 100, Y = rnorm(N), Z = rbinom(N, 1, 0.5) ) %&gt;% estimate( form = Y ~ Z, vars = &quot;Z&quot; ) %&gt;% evaluate_power( delta = seq(0, 0.6, len = 10) ) Then, we do the same with DeclareDesign, declaring a population, potential outcomes, assignments, a target quantity of interest, and an estimator: design &lt;- declare_population( N = 100, U = rnorm(N), potential_outcomes(Y ~ U) ) + declare_assignment(Z = simple_ra(N, prob = 0.5)) + declare_inquiry(ATE = mean(Y_Z_1 - Y_Z_0)) + declare_measurement(Y = reveal_outcomes(Y ~ Z)) + declare_estimator( Y ~ Z, inquiry = &quot;ATE&quot;, .method = lm_robust ) We then use draws from this design within the OES tools: dd_eval &lt;- replicate( n = 1000, expr = draw_estimates(design) %&gt;% list ) %&gt;% bind_rows() %&gt;% evaluate_power( delta = seq(0, 0.6, len = 10) ) We show the similarity between the two approaches to generating the simulated data in the figure below: bind_rows( eval %&gt;% mutate(method = &quot;OES Power Tools&quot;), dd_eval %&gt;% mutate(method = &quot;DeclareDesign&quot;) ) %&gt;% ggplot() + geom_line( aes(delta, power, linetype = method) ) + scale_linetype_manual(values = c(&quot;solid&quot;, &quot;longdash&quot;)) + labs( x = expression(delta), y = &quot;Power&quot;, linetype = NULL ) + scale_y_continuous( n.breaks = 6 ) + geom_hline( yintercept = 0.8, col = &quot;grey25&quot;, size = 1, alpha = 0.8 ) + ggridges::theme_ridges( center_axis_labels = TRUE, font_size = 10 ) + theme( legend.position = &quot;bottom&quot; ) References "],
["code-example-index.html", "Chapter 7 Code example index", " Chapter 7 Code example index Below, we list places where you can find coded examples of common tasks our evaluations often require. But note that there examples of many other more minor tasks throughout chapters 3-6 that we can&#x2019;t exhaustively list here. All examples referenced provide both R and Stata versions. Calculating design-justified standard errors: (3.1.1) Using simulation (comparing designs with fake data) and derived expressions (real data) Calculating design-justified confidence intervals: (3.1.2) Using simulation (randomization inference CIs) or standard methods (i.e., &#x3B8;&#xB1;1.96&#xD7;SE\\theta \\pm 1.96 \\times SE&#x3B8;&#xB1;1.96&#xD7;SE) Random assignment (4.1 - 4.2) Advantages of urn draw randomization; randomization with 2+ groups (4.3) Factorial assignment (4.4) Blocked assignment (4.5) Clustered assignment Balance testing (4.8.4) Separate comparisons for each of many covariates (4.8.4) Omnibus tests (asymptotic inference or randomization inference) Estimation (5.1.1) Estimating average treatment effects, standard errors, and performing randomization inference with two-arm trials (continuous and binary outcomes) (5.2.1) In multiple arm trials Multiple testing adjustment (5.2.1 - 5.2.2) Methods for multiple testing with multiple treatment arms and/or multiple outcomes, including examples of randomization inference simulations Covariate adjustment (5.3.2 - 5.3.3) Lin (2013) adjustment and Rosenbaum (2002) adjustment as alternatives to standard linear, additive adjustment for covariates in a regression Adjusting estimation to account for our randomization design (5.5.2) Different methods of adjusting for blocked random assignment (5.6) Adjusting for clustered random assignment Design simulation (e.g., estimating bias and/or precision) (5.3 - 5.6) Comparing estimation strategies using DeclareDesign in R or a parallel approach in Stata Power analysis (5.3 - 5.6) Using DeclareDesign in R or a parallel approach in Stata (6.1) Analytical power calculations (6.2 and 6.4) Simulating test statistics under a true null and using them to calculate power (6.4.2) Simulating data with effect sizes built in, and comparing power estimates across effect sizes Comparing &#x201C;nested&#x201D; models (e.g., testing differences between regression coefficients) (4.8.4 and 5.2.2) Wald test Randomization inference (3.1.2, 4.8.4, 5.1.1, and 5.2.1) Applications in various settings listed above "],
["methods-topic-index.html", "Chapter 8 Methods topic index", " Chapter 8 Methods topic index Below, we list places where you can find discussions of common methodological issues we often have to think about when designing evaluations. There are many more minor issues we cover that we can&#x2019;t exhaustively cover. But this should but a useful reference: Design-based inference: (3.1.1) The logic for why we prefer HC2 standard errors, and why we prefer design-justified inference methods in general (5.1.1) The &#x201C;sharp null&#x201D; (employed when using randomization inference simulations to estimate a p-value), and how it differs from the standard null hypothesis researchers generally employ Random assignment decisions: (4.1) Why we prefer urn-draw based (i.e., complete) random assignment over coin-flip based (simple) assignment (4.3) Issues to think about when analyzing factorial experiments (4.4.1) The benefits of blocking, along with a review of some occasional disadvantages (in 4.4.4) (4.7) The logic of as-if random assignment, with examples of past OES projects relying on this Balance checks: (4.8.1) Some statistical issues raised by performing significance tests to check balance for each of many different covariates (4.8.2) The advantages of &#x201C;omnibus&#x201D; balance tests, particularly those that rely on randomization inference (4.8.5) How we tend to think through evidence of &#x201C;failed&#x201D; random assignments OLS linear regression as a useful tool for treatment effect estimation (2.3) Quick review of our thinking (5.1.1) More detailed illustration, for both continuous (5.1.1.1) and binary (5.1.1.2) outcomes Multiple testing (5.2 - 5.2.2) When might we want to use different approaches to handle the problem of multiple testing? (5.2.3) When do we think adjusting for multiple tests is most needed in the first place? Estimation choices (5.3 - 5.3.2) What is &#x201C;Lin adjustment&#x201D; or &#x201C;Lin estimation,&#x201D; and when does it have advantages over the more standard approach of linear, additive adjustment for covariates? (5.5.2) How do different methods of adjusting for blocked random assignment compare to each other? In particular: unbiased vs precision-weighted methods. (5.6) How does adjusting standard errors for clustering generally impact the precision of our tests? Do we ever need to worry about bias due to clustering (5.6.1)? Preliminary power calculation (6.3) When are simple analytic tools sufficient, and when do we need to set aside time for more intensive simulations? "],
["appendix.html", "Chapter 9 Appendix", " Chapter 9 Appendix Here, we organize internal links to further resources for OES team members. These resources are not accessible to the public (they require a log-in). Example data processing templates: R Stata "],
["references.html", "References", " References "]
]
