```{r setupdat13,  echo=FALSE, include=FALSE}
if(!exists("dat1")){ dat1 <- read.csv(here::here('dat1.csv')) }
```

# Randomization and Design

After working together with our agency partners to translate insights from the
social and behavioral sciences into policy recommendations following our
[process](https://oes.gsa.gov/methods/), our combined OES and agency teams
assess these new ideas by observing differences and changes in real world
outcomes (usually measured using existing administrative data). Nearly always,
we design a randomized control trial (an RCT) to ensure that the differences
and changes we observe arise from the policy intervention and not from some
other pre-existing difference or change. Here we show examples of the ways that
we create the random numbers that form the core of our different types of RCTs
that we use to build evidence about the effectiveness of the new policy.


## Coin flipping randomization versus Urn-drawing randomization

Many discussions of RCTs begin by talking about the intervention being assigned
to units (people,  schools, offices, districts) "by the flip of a coin". We
rarely use this method directly even though it is a useful way to introduce the
idea that RCTs guarantee fair access to the new policy.

The following code contrasts coin-flipping style randomization (where the
number of units in each condition is not guaranteed) with drawing-from-an-urn
style, or complete, randomization (where the number of units in each condition
is guaranteed). We try to avoid the coin-flipping style of randomization
because of this lack of control over the number of units in each condition ---
coin-flipping based experiments are still valid and tell us about the
underlying counterfactuals, but they can have less statistical power.

Notice that the simple randomization implemented in the code below results in more observations in the treatment group (group `1`) than in the control group (group `0`).  The complete randomization will always assign 5 units to the treatment, 5 to the control.


```{r completera}
## Start with a small experiment with only 10 units
n <- 10
## Set a seed for the pseudo-random number generator so that we always get the same results
set.seed(12345)
## Coin flipping does not guarantee half and half treated and control
## This next bit of code shows the base R version of coin flipping randomization
## trt_coinflip <- rbinom(10, size = 1, prob = .5)
trt_coinflip <- simple_ra(n)
## Drawing from an urn or shuffling cards, guarantees half treated and control
##trt_urn <- sample(rep(c(1, 0), n / 2))
trt_urn <- complete_ra(n)
table(trt_coinflip)
table(trt_urn)
```

## Urn-Drawing or Complete Randomization into 2 or more groups

We tend to use the `randomizr` R package [@R-randomizr] for simple designs
rather than the base R `sample` function because thee `randomizr` does some
quality control checks. Notice that we implement a check on our code below with
the `stopifnot` command: the code will stop and issue a warning if we didn't
actually assign 1/4 of the observations to the treatment condition. Here, we
assign the units first to 2 arms with  equal probability (`Z2armEqual`) and
then, to show how the code works, to 2 arms where one arm has only 1/4 the
probability of receiving treatment (imagining  a design with an expensive
intervention) (`Z2armUnequalA` and `Z2armUnequalB`), and then to a design with
4 arms, each with equal probability. (We often use $Z$ to refer to the variable
recording our intervention arms.)

```{r completera2}
N <- nrow(dat1)
## Two equal arms
dat1$Z2armEqual <- complete_ra(N)
## Two unequal arms: .25 chance of treatment
set.seed(12345)
dat1$Z2armUnequalA <- complete_ra(N,prob=.25)
stopifnot(sum(dat1$Z2armUnequalA)==N/4)
dat1$Z2armUnequalB <- complete_ra(N,m=N/4)
## Four equal arms
dat1$Z4arms <- complete_ra(N,m_each=rep(N/4,4))

table(Z2armEqual=dat1$Z2armEqual)
table(Z2armUnequalA=dat1$Z2armUnequalA)
table(Z2armUnequalB=dat1$Z2armUnequalB)
table(Z4arms=dat1$Z4arms)
```

## Factorial Designs

One can often test the effects of more than one intervention without losing
much statistical power by randomly assigning more than one treatment
independently of each other. The simplest design that we use for this purpose
is the $2 \times 2$ factorial design. For example, in the next table we see that
we have assigned `r N/2` observations to each arm of both of two interventions.
Since the randomization of `treatment1` is independent of `treatment2`, we can
assess the effects of each treatment separately and pay little power
penalty unless one of the treatments dramatically increases the variance of
the outcome compared to the experiment with only one treatment assigned.

```{r factdesign2x2}
## Two equal arms, second cross treatment
dat1$Z2armEqual2 <- complete_ra(N)
table(treatment1=dat1$Z2armEqual,treatment2=dat1$Z2armEqual2)
```

Although factorial designs allow us to test more than one intervention at the
same time, they tend to provide little statistical power for testing
hypotheses about the **interaction** of the two treatments. If we want to
learn about how two different interventions work together, then the sample
size requirements will be much larger than if we want to learn about only each
treatment separately.^[But see @small2011structured for a way to increase the
power of tests for such questions. This approach is not yet part of our
standard practice.]


## Block Random Assignment

Statistical power depends not only on the size of the experiment and the strength of the treatment effect, but also on the amount of noise, or
non-treatment-related variability, in outcomes. Block-randomized designs can help reduce the noise in outcomes, while simultaneously minimizing estimation error -- the amount that our particular experiment's estimate differs from the truth.  

In a block-randomized, or
stratified, experiment, we randomly assign units to the policy
interventions *within* groups. 

Suppose we are evaluating whether dedicated navigators can increase the percentage of students who live in public housing who complete federal financial aid applications (FAFSA).  Our experiment will send navigators to two of four eligible buildings, two of which are large and two of which are small. Though we can never know the outcome in all buildings both with and without navigators (the "fundamental problem of causal inference"), if we could, we might have the data below.

\begin{center}
\begin{tabular}{ccccc}
&&& FAFSA \% & FAFSA \% \\
Building & Size & Navigators? & if No Navigators & if Navigators   \\ \hline
1 & Large & $\_$ & 20 & 60 \\
2 & Large & $\_$ & 30 & 70 \\
3 & Small & $\_$ & 20 & 30 \\
4 & Small & $\_$ & 30 & 40 \\ \hline
&& Means: & 25 & 50 \\
\end{tabular}
\end{center}

In this case, the true average treatment effect for this sample is the average under treatment minus the average under control: $\text{ATE} = 50-25=25$ percent more applications if navigators are deployed everywhere.

If we randomly allocate two buildings to treatment and two to control, we might treat the first two buildings and observe

\begin{center}
\begin{tabular}{ccccc}
&&& FAFSA \% & FAFSA \% \\
Building & Size & Navigators? & if No Navigators & if Navigators   \\ \hline
1 & Large & Yes &  & 60 \\
2 & Large & Yes &  & 70 \\
3 & Small & No & 20 &  \\
4 & Small & No & 30 &  \\ \hline
&& Means: & 25 & 65 \\
\end{tabular}
\end{center}

yielding an estimate of the treatment effect of $65-25 = 40$ percent more applications -- an estimate larger than the true value of 25.

Or, we might observe

\begin{center}
\begin{tabular}{ccccc}
&&& FAFSA \% & FAFSA \% \\
Building & Size & Navigators? & if No Navigators & if Navigators   \\ \hline
1 & Large & $\_$ &  & 60 \\
2 & Large & $\_$ & 30 &  \\
3 & Small & $\_$ &  & 30 \\
4 & Small & $\_$ & 30 &  \\ \hline
&& Means: & 30 & 45 \\
\end{tabular}
\end{center}

yielding an estimate of the treatment effect of $45-30 = 15$ percent fewer applications -- an estimate smaller than the true value of 25.

All the possible equiprobable assignments, and their estimated treatment effects are below.  

\begin{center}
\begin{tabular}{cc}
Assignments & Est TE \\ \hline
YYNN & 40  \\ 
NYNY & 35 \\
YNNY & 25 \\
NYYN & 25 \\
YNYN & 15 \\
NNYY & 10
\end{tabular}
\end{center}

These possible estimates have mean equal to the true value of 25, showing that the difference in means is an unbiased estimator.  However, some of these estimates are far from the truth, and they have a lot of variability.  

To design our experiment to best estimate the true value, and to do so with more statistical power, we can *block* the experiment.  Here, this means restricting our randomization to those three possible assignments that balance the large and small buildings across the treatment groups.

\begin{center}
\begin{tabular}{cc}
Assignments & Est TE \\ \hline
NYNY & 35 \\
YNNY & 25 \\
NYYN & 25 \\
YNYN & 15 \\
\end{tabular}
\end{center}

With the blocked design, we will get an estimate no more than 10 percentage points from the truth.  Further, our estimates will have less variability (an SD of `r round(sd(c(35, 25, 25, 15)), 2)` rather than 
`r round(sd(c(35, 25, 25, 15, 40, 10)), 2)`), improving the power of our design.

For a more realistic example, suppose our experiment
has two hospitals. We might randomly assign people to treatment and control *within* each hospital.  We might assign half of the people in hospital "A"
to treatment and half to control and likewise in hospital "B".^[We use "block"
rather than "strata" throughout this document to refer to groups of
experimental units that are fixed before randomization, where group membership
cannot be changed by the experiment itself.]  Below, we have `r nrow(dat1) / 2` units in hospital "A" and `r nrow(dat1) / 2` in hospital "B".

```{r blockra1}
dat1$blockID <- gl(n = 2, k = N/2, labels = c("A", "B"))
with(dat1,table(blockID=dat1$blockID))
```

We assign half of the units in each hospital to each treatment condition:

```{r blockra15}
dat1$Z2armBlocked <- block_ra(blocks=dat1$blockID)
with(dat1,table(blockID,Z2armBlocked))
```

If, say, there were fewer people eligible for treatment in hospital "A" --- or
perhaps the intervention was more expensive in that block --- we might assign
treatment with different probability within each block.  The code below shows half of the hospital "A" units assigned to treatment, but only a quarter of those from hospital "B".  Again, we check that
this code worked by including a test. This approach is an informal version of
one of the best practices in writing code in general, called "unit testing".
See the [EGAP Guide to Workflow for more
examples](https://rawgit.com/egap/methods-guides/master/workflow/workflow.html#to-minimize-error-build-testing-into-the-code).

```{r blockra2}
dat1$Z2armBlockedUneqProb <- block_ra(blocks=dat1$blockID,block_prob = c(.5,.25))
with(dat1,table(blockID,Z2armBlockedUneqProb))
stopifnot(sum(dat1$Z2armBlockedUneqProb==1&dat1$blockID=="B")==ceiling(sum(dat1$blockID=="B")/4)|
sum(dat1$Z2armBlockedUneqProb==1&dat1$blockID=="B")==floor(sum(dat1$blockID=="B")/4))
```

Our team tries to implement block-randomized assignment whenever possible in
order to increase the statistical power of our experiments. We also often find
it useful in cases where different administrative units are implementing the
treatment or where we expect different groups of people to have different
reactions to the treatment.

### Using only a few covariates to create blocks

If we have background information on a few covariates, we can great blocks by
hand using the kind of process demonstrated here:


```{r cutcovs}
## For example, make three groups from the cov2 variable
dat1$cov2cat <- with(dat1,cut(cov2,breaks=3))
table(dat1$cov2cat,exclude=c())
with(dat1,tapply(cov2,cov2cat,summary))
## And we can make blocks that are the same on two covariates
dat1$cov1bin <- as.numeric( dat1$cov1 > median(dat1$cov1) )
dat1$blockV2 <- droplevels(with(dat1,interaction(cov1bin,cov2cat)))
table(dat1$blockV2,exclude=c())
## And then assign within these blocks
set.seed(12345)
dat1$ZblockV2 <- block_ra(blocks = dat1$blockV2)
with(dat1,table(blockV2,ZblockV2,exclude=c()))
```

### Multivariate blocking using many covariates

If we have many background variables, we can increase precision by thinking
about the problem of blocking as a problem of matching or creating sets of
units which are as similar as possible in terms of the collection of those
covariates [@moore2012multivariate;@moore2016bT063]. Here we show two approaches.

Creating pairs:

```{r}
## using the blockTools package
mvblocks <- block(dat1,id.vars="id",block.vars=c("cov1","cov2"),algorithm="optimal")
dat1$blocksV3 <- createBlockIDs(mvblocks,data=dat1,id.var = "id")
dat1$ZblockV3 <- block_ra(blocks = dat1$blocksV3)
## just show the first ten pairs
with(dat1,table(blocksV3,ZblockV3,exclude=c()))[1:10,]
```

Creating larger blocks:

```{r}
## using the quickblock package
distmat <- distances(dat1, dist_variables = c("cov1", "cov2"),id_variable =
		     "id", normalize="mahalanobiz")
distmat[1:5,1:5]
quantile(as.vector(distmat),seq(0,1,.1))
## The caliper argument is supposed to prevent the inclusion of ill-matched
## points.
mvbigblock <- quickblock(distmat, size_constraint = 6L) #,caliper=2.5)
## Look for missing points
table(mvbigblock,exclude=c())
dat1$blocksV4 <- mvbigblock
dat1$ZblockV4 <- block_ra(blocks = dat1$blocksV4)
with(dat1,table(blocksV4,ZblockV3,exclude=c()))[1:10,]
```

Here we produce some description of the differences within block: the proportion
of people in category "1" on the binary covariate (notice that the sets are
homogeneous on this covariate) and the difference between the largest and
smallest value on the continuous covariate.

```{r blockingeval}
blockingDescEval <- dat1 %>% group_by(blocksV4) %>% summarize(cov2diff = max(abs(cov2))
							  - min(abs(cov2)),
							  cov1 = mean(cov1))
blockingDescEval
```

## Cluster random assignment


We often implement a new policy intervention at the level of some group
of people --- like a doctor's practice, or a building, or some other
administrative unit. Notice that, even though we have `r nrow(dat1)` units in our example
data, imagine that they are grouped into 10 buildings, and the policy
intervention is at the building level. Below, we assign `r nrow(dat1)/2` of those units
to treatment and `r nrow(dat1)/2` to control.  Everyone in each building has the same treatment assignment.

```{r clusterRA}
ndat1 <- nrow(dat1)
## Make an indicator of cluster membership
dat1$buildingID <- rep(1:(ndat1/10),length=ndat1)
set.seed(12345)
dat1$Zcluster <- cluster_ra(cluster=dat1$buildingID)
with(dat1,table(Zcluster,buildingID))
```

Cluster randomized designs raise new questions about estimation and testing and
thus statistical power.  We describe our approaches to analysis and power
analysis of cluster randomized designs [in the section  on  the analysis of Cluster Randomized Trials](#clusterrandanalysis).


## Other designs

Our team has also designed stepped-wedge style designs, saturation designs
aimed to discover whether the effects of the experimental intervention are
communicated across people (via some spillover or network mechanism), and
designs where we try to isolate certain experimental units (like buildings)
from each other so that we can focus our learning about the effects of the
intervention rather than on the effects of communication of the intervention
across people. In later versions of this document we will include simple
descriptions and code for those other, less common designs.

## Randomization assessment

If we have covariates, we can assess the performance of the randomization
procedure by testing the hypothesis that the treatment-vs-control differences,
or differences across treatment arms, in covariates are consistent with our
claimed mechanism of randomization. In the absence of covariates, we assess
whether the number of units assigned to each arm (conditional on other design
features, such as blocking or stratification) are consistent with the claimed
random assignment.

Here is an example with a binary treatment and a continuous outcome and 10
covariates. In this case we use the $d^2$ omnibus balance test function `xBalance()` in
the package `RItools` [see @hansen_covariate_2008 ; @bowers_ritools_2016].

The "overall" $p$-value below shows us that we have little evidence against the
idea that treatment ($Z$) was assigned at random --- at least in terms of the
relationships between the two covariates and the treatment assignment. The
test statistics here are mean differences. This overall or omnibus test is the
key here --- if we had many covariates, it would easy to discover one or a few
covariates with means that differ detectably from zero just by chance. That
is, in a well-operating experiment, we would expect some baseline imbalances
(as measured using randomization based tests) -- roughly 5 in 100. Thus the value
of the omnibus test is that we will not be misled by chance false positives.

```{r}
randAssessV1 <- balanceTest(Z~cov1+cov2,data=dat1)
randAssessV1$overall[,]
```

Here is an example of randomization assessment with block-randomized assignment:

```{r}

randAssessV3 <- balanceTest(ZblockV3~cov1+cov2+strata(blocksV3),data=dat1)
randAssessV3$overall[,]
```

One can also assess the randomization given both block and cluster random
assignment using a formula like so:
`Z ~ cov1 + cov2 + strata(blockID) + cluster(clusterID)`.

This approach, using the `RItools` package, works well for experiments that
are not overly small. In a very small experiment, say, an experiment with 5
clusters assigned to treatment and 5 clusters assigned to control, we would do
the same test, but would not use the $\chi^2$ distribution.  Instead, we would do a
permutation-based test.

We do not use $F$-tests or Likelihood Ratio tests to assess randomization. See
@hansen_covariate_2008 for some evidence that a test based on
randomization-inference (like the $d^2$ test developed in that article)
maintains false positive rates better than the sampling- or likelihood-justified $F$ and likelihood ratio tests.

###  What to do with "failed" randomization  assessments?

A $p$-value less than .05 on a test of  randomization mechanism ought to
triggers extra scrutiny of how the randomization was conducted and how the data
were recorded by the agency. For example, we might contact our agency partner
to learn  more deetails  about  how the random  numbers  themselves were
generated, or we  may ask for  the  SQL or  SAS  code or other code  that might
have been  used  to randomize.  In most cases, we will learn  that
randomization worked well but  that our understanding of the  design  and  the
data were incorrect. In some  cases, we will learn  that the  randomization
occurred  as we had initially understood. In such  cases, we tend to assume
that rejecting the  null in our randomization  assessment is a false positive
from  our  testing procedure  (we assume we would see about 5 such errors in
every 100 experiments).  If the rejection of  the null appears  to be driven by
one or more particularly  substantively relevant covariates,  say, the variable
age looks very imbalanced between treated  and control groups in  a health
study,  then  we  will present  both the unadjusted results but  also adjust
for that covariate via stratification and/or covariance  adjustment  as  we
describe later  in  this document. A  chance rejection of the null that the
experiment was randomized as it should not cause us to the use the adjusted
estimate as our  primary causal effect  --- after all, it will be biased
whereas the unadjusted estimate will not be biased undere  chance  departures
from the null. But, large differences between the  two  estimates  can inform
the  qualitative task  of  substantive  interpretation of  the  study: looking
at  different effects by age groups, for  example,  might  tell  us  something
about the  particular  context  of the study  and, in  turn, help  us think
about what we have learned.

### How to minimize large chance departures of randomization?

Our approach to  block-randomization helps  us avoid these problems. We can also restrict our randomization in ways that are  more flexible than requiring  blocks, but  which, in  turn should minimize chance. We tend not to  use  re-randomization, among methods of  restricted  randomization,  only because we often can use block-randomization and thus can minimize the complexity of later analysis. However, since we pursue a randomization-based approach to the analysis of  experiments,  we can easily  (in concept) estimate causal  effects and test hypotheses about causal effects as well after such modes  of randomization.


```{r savedat1, echo=FALSE}
write.csv(dat1,file="dat1_with_designs.csv")
```

